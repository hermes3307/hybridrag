import os
import json
import numpy as np
from tqdm import tqdm
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
import re
import emoji

# 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
def load_case_studies(directory="case_studies"):
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        directory (str): ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        list: êµ¬ì¡°í™”ëœ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    case_studies = []
    
    if not os.path.exists(directory):
        print(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}")
        return case_studies
    
    files = [f for f in os.listdir(directory) if f.endswith('.txt')]
    print(f"ì´ {len(files)}ê°œì˜ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    for filename in tqdm(files, desc="íŒŒì¼ ë¡œë”© ì¤‘"):
        file_path = os.path.join(directory, filename)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # íŒŒì¼ëª…ì—ì„œ cateì™€ idx ì¶”ì¶œ
            cate_match = re.search(r'cate_(\d+)', filename)
            idx_match = re.search(r'idx_(\d+)', filename)
            
            cate = int(cate_match.group(1)) if cate_match else None
            idx = int(idx_match.group(1)) if idx_match else None
            
            # ë‚´ìš© íŒŒì‹±
            sections = {}
            
            # ì œëª© ì¶”ì¶œ
            title_match = re.search(r'ì œëª©:\s*(.*?)(?:\n\n|$)', content)
            title = title_match.group(1).strip() if title_match else "ì œëª© ì—†ìŒ"
            sections['title'] = title
            
            # ê° ì„¹ì…˜ ì¶”ì¶œ
            for section in ['Who', 'Problem', 'Solution', 'Results']:
                pattern = rf'{section}:\s*(.*?)(?:\n\n(?:[A-Za-z]+:|$)|$)'
                section_match = re.search(pattern, content, re.DOTALL)
                sections[section.lower()] = section_match.group(1).strip() if section_match else ""
            
            # ì‚°ì—… ë¶„ì•¼ ì¶”ì • (ì¹´í…Œê³ ë¦¬ ê¸°ë°˜)
            industry_map = {
                3: "í†µì‹ ",
                4: "ê¸ˆìœµ",
                5: "ì œì¡°",
                6: "ê³µê³µ",
                7: "ì„œë¹„ìŠ¤",
                8: "ê¸°íƒ€"
            }
            industry = industry_map.get(cate, "ê¸°íƒ€")
            
            # ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” êµ¬ì¡°í™”
            case_study = {
                'id': f"cate_{cate}_idx_{idx}",
                'filename': filename,
                'title': title,
                'who': sections['who'],
                'problem': sections['problem'],
                'solution': sections['solution'],
                'results': sections['results'],
                'full_text': content,
                'cate': cate,
                'idx': idx,
                'industry': industry,
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                'emoji': get_industry_emoji(industry)
            }
            
            case_studies.append(case_study)
            
        except Exception as e:
            print(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path}, ì˜¤ë¥˜: {str(e)}")
    
    print(f"{len(case_studies)}ê°œì˜ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return case_studies

def get_industry_emoji(industry):
    """ì‚°ì—… ë¶„ì•¼ì— ë§ëŠ” ì´ëª¨ì§€ ë°˜í™˜"""
    emoji_map = {
        "í†µì‹ ": "ğŸ“±",
        "ê¸ˆìœµ": "ğŸ’°",
        "ì œì¡°": "ğŸ­",
        "ê³µê³µ": "ğŸ›ï¸",
        "ì„œë¹„ìŠ¤": "ğŸ›ï¸",
        "ê¸°íƒ€": "ğŸ“Š"
    }
    return emoji_map.get(industry, "ğŸ“„")

# 2. ë²¡í„° ì„ë² ë”© ìƒì„±
class CaseStudyVectorizer:
    def __init__(self, model_name="paraphrase-multilingual-MiniLM-L12-v2"):
        """
        ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ë²¡í„°í™”í•˜ëŠ” í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  Sentence Transformer ëª¨ë¸ ì´ë¦„
        """
        print(f"ëª¨ë¸ {model_name} ë¡œë”© ì¤‘...")
        self.model = SentenceTransformer(model_name)
        self.vector_size = self.model.get_sentence_embedding_dimension()
        print(f"ë²¡í„° ì°¨ì›: {self.vector_size}")
    
    def vectorize_case_studies(self, case_studies):
        """
        ëª¨ë“  ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ì˜ ì„¹ì…˜ë³„ ë²¡í„° ì„ë² ë”© ìƒì„±
        
        Args:
            case_studies (list): ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë¦¬ìŠ¤íŠ¸
            
        Returns:
            dict: ì„¹ì…˜ë³„ ë²¡í„° ì„ë² ë”©ê³¼ ì „ì²´ ë²¡í„° ì„ë² ë”©
        """
        print("ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë²¡í„°í™” ì¤‘...")
        
        # ê° ì„¹ì…˜ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        sections = ['title', 'who', 'problem', 'solution', 'results']
        section_texts = {section: [] for section in sections}
        
        # ì „ì²´ í…ìŠ¤íŠ¸
        full_texts = []
        
        for case in case_studies:
            for section in sections:
                section_texts[section].append(case[section])
            
            # ì „ì²´ í…ìŠ¤íŠ¸ëŠ” ëª¨ë“  ì„¹ì…˜ì„ ê²°í•©
            combined_text = " ".join([case[section] for section in sections])
            full_texts.append(combined_text)
        
        # ê° ì„¹ì…˜ë³„ ë²¡í„°í™”
        vectors = {}
        for section in sections:
            print(f"{section} ì„¹ì…˜ ë²¡í„°í™” ì¤‘...")
            vectors[section] = self.model.encode(section_texts[section], show_progress_bar=True)
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ë²¡í„°í™”
        print("ì „ì²´ í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤‘...")
        vectors['full_text'] = self.model.encode(full_texts, show_progress_bar=True)
        
        return vectors

# 3. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥
class CaseStudyVectorDB:
    def __init__(self, vector_size):
        """
        ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            vector_size (int): ë²¡í„°ì˜ ì°¨ì› í¬ê¸°
        """
        self.vector_size = vector_size
        self.indices = {}
        self.case_studies = []
    
    def build_indices(self, case_studies, vectors):
        """
        ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ì™€ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
        
        Args:
            case_studies (list): ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë¦¬ìŠ¤íŠ¸
            vectors (dict): ì„¹ì…˜ë³„ ë²¡í„° ì„ë² ë”©
        """
        print("FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self.case_studies = case_studies
        
        # ê° ì„¹ì…˜ë³„ ì¸ë±ìŠ¤ ìƒì„±
        for section, section_vectors in vectors.items():
            print(f"{section} ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            index = faiss.IndexFlatL2(self.vector_size)  # L2 ê±°ë¦¬(ìœ í´ë¦¬ë“œ ê±°ë¦¬) ì‚¬ìš©
            if len(section_vectors) > 0:
                index.add(np.array(section_vectors).astype('float32'))
                self.indices[section] = index
    
    def save(self, directory="vector_db"):
        """
        ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            directory (str): ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        os.makedirs(directory, exist_ok=True)
        
        # ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(os.path.join(directory, 'case_studies.json'), 'w', encoding='utf-8') as f:
            json.dump(self.case_studies, f, ensure_ascii=False, indent=2)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        for section, index in self.indices.items():
            index_path = os.path.join(directory, f"{section}_index.faiss")
            faiss.write_index(index, index_path)
        
        print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ {directory}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    @classmethod
    def load(cls, directory="vector_db", vector_size=384):
        """
        ì €ì¥ëœ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
        
        Args:
            directory (str): ë¡œë“œí•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            vector_size (int): ë²¡í„° ì°¨ì› í¬ê¸°
            
        Returns:
            CaseStudyVectorDB: ë¡œë“œëœ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê°ì²´
        """
        if not os.path.exists(directory):
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}")
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        instance = cls(vector_size)
        
        # ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(os.path.join(directory, 'case_studies.json'), 'r', encoding='utf-8') as f:
            instance.case_studies = json.load(f)
        
        # ê°€ëŠ¥í•œ ëª¨ë“  ì„¹ì…˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        for filename in os.listdir(directory):
            if filename.endswith('_index.faiss'):
                section = filename.replace('_index.faiss', '')
                index_path = os.path.join(directory, filename)
                instance.indices[section] = faiss.read_index(index_path)
        
        return instance
    
    def search(self, query, section='full_text', k=5, vectorizer=None):
        """
        ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ê²€ìƒ‰
        
        Args:
            query (str): ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            section (str): ê²€ìƒ‰í•  ì„¹ì…˜ ('title', 'who', 'problem', 'solution', 'results', 'full_text')
            k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            vectorizer (CaseStudyVectorizer): ì¿¼ë¦¬ ë²¡í„°í™”ì— ì‚¬ìš©í•  ë²¡í„°ë¼ì´ì €
            
        Returns:
            list: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if section not in self.indices:
            raise ValueError(f"ì„¹ì…˜ '{section}'ì— ëŒ€í•œ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if vectorizer is None:
            raise ValueError("ì¿¼ë¦¬ ë²¡í„°í™”ë¥¼ ìœ„í•œ vectorizerê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¿¼ë¦¬ ë²¡í„°í™”
        query_vector = vectorizer.model.encode([query])[0].reshape(1, -1).astype('float32')
        
        # FAISSë¡œ ê²€ìƒ‰
        distances, indices = self.indices[section].search(query_vector, k)
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.case_studies):
                case = self.case_studies[idx]
                result = {
                    'rank': i + 1,
                    'distance': float(distance),
                    'similarity': 1.0 / (1.0 + float(distance)),  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    'case_study': case
                }
                results.append(result)
        
        return results

# 4. ê²€ìƒ‰ ë° í‘œì‹œ ê¸°ëŠ¥
def format_search_results(results, highlight_query=None):
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ í˜•ì‹í™”
    
    Args:
        results (list): ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        highlight_query (str): í•˜ì´ë¼ì´íŠ¸í•  ì¿¼ë¦¬ (ì„ íƒ ì‚¬í•­)
        
    Returns:
        str: í˜•ì‹í™”ëœ ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´
    """
    if not results:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    output = "# ğŸ” ê²€ìƒ‰ ê²°ê³¼\n\n"
    
    for result in results:
        case = result['case_study']
        similarity = result['similarity'] * 100
        
        industry_emoji = case.get('emoji', 'ğŸ“„')
        
        output += f"## {industry_emoji} {case['title']} (ìœ ì‚¬ë„: {similarity:.1f}%)\n\n"
        output += f"**ì‚°ì—…:** {case['industry']}\n"
        output += f"**ID:** {case['id']}\n\n"
        
        # ì„¹ì…˜ë³„ ë‚´ìš© í‘œì‹œ
        for section in ['who', 'problem', 'solution', 'results']:
            content = case[section]
            
            # ì¿¼ë¦¬ í•˜ì´ë¼ì´íŠ¸ (ì„ íƒ ì‚¬í•­)
            if highlight_query and highlight_query.strip():
                content = content.replace(highlight_query, f"**{highlight_query}**")
            
            section_emoji = {
                'who': 'ğŸ‘¥',
                'problem': 'â“',
                'solution': 'ğŸ’¡', 
                'results': 'âœ…'
            }
            
            emoji_icon = section_emoji.get(section, '')
            output += f"### {emoji_icon} {section.capitalize()}\n{content}\n\n"
        
        output += "---\n\n"
    
    return output

def interactive_search():
    """
    ëŒ€í™”í˜• ê²€ìƒ‰ ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
    """
    # ì´ˆê¸° ë””ë ‰í† ë¦¬ ì„¤ì •
    source_dir = "case_studies"
    vector_db_dir = "vector_db"
    
    # í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ
    print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")
    
    # ë²¡í„° DB ë¡œë“œ ë„ìš°ë¯¸ í•¨ìˆ˜
    def load_vector_db():
        if not os.path.exists(vector_db_dir) or not os.listdir(vector_db_dir):
            print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ({vector_db_dir})")
            print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì˜µì…˜(7)ì„ ì„ íƒí•˜ì—¬ ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.")
            return None, None
            
        try:
            # ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
            model_name = "paraphrase-multilingual-MiniLM-L12-v2"
            
            # ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”
            vectorizer = CaseStudyVectorizer(model_name)
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
            db = CaseStudyVectorDB.load(vector_db_dir, vectorizer.vector_size)
            
            # ë””ë²„ê¹…: ë¡œë“œëœ ì¸ë±ìŠ¤ í™•ì¸
            print(f"ë¡œë“œëœ ì¸ë±ìŠ¤: {list(db.indices.keys())}")
            print(f"ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ìˆ˜: {len(db.case_studies)}")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì„¹ì…˜ í™•ì¸
            if 'full_text' not in db.indices:
                print("ê²½ê³ : 'full_text' ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                # ë¹„ì–´ìˆëŠ” ì¸ë±ìŠ¤ë¼ë„ ìƒì„±í•´ì„œ ì˜¤ë¥˜ ë°©ì§€
                if not db.indices:
                    print("ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
                    return None, None
            
            print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ!")
            return db, vectorizer
        except Exception as e:
            print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()  # ìƒì„¸ ì˜¤ë¥˜ ì¶”ì 
            return None, None
    
    # ì´ˆê¸° ë¡œë“œ ì‹œë„
    db, vectorizer = load_vector_db()
    
    # ê²€ìƒ‰ ì˜µì…˜ í‘œì‹œ í•¨ìˆ˜
    def display_menu():
        print("\n========== ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ê²€ìƒ‰ ì‹œìŠ¤í…œ ==========")
        print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("1. 'ì „ì²´ í…ìŠ¤íŠ¸' - ëª¨ë“  ì„¹ì…˜ì—ì„œ ê²€ìƒ‰")
        print("2. 'ì œëª©' - ì œëª©ì—ì„œë§Œ ê²€ìƒ‰")
        print("3. 'ë¬¸ì œ' - Problem ì„¹ì…˜ì—ì„œë§Œ ê²€ìƒ‰")
        print("4. 'ì†”ë£¨ì…˜' - Solution ì„¹ì…˜ì—ì„œë§Œ ê²€ìƒ‰")
        print("5. 'ê²°ê³¼' - Results ì„¹ì…˜ì—ì„œë§Œ ê²€ìƒ‰")
        print("6. 'ë‚˜ê°€ê¸°' - í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        print("7. 'ë²¡í„°ë°ì´í„° ì¬êµ¬ì¶•' - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬êµ¬ì¶•")
        print("8. 'ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ì„¤ì •' - ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ì„¤ì •")
        print("9. 'íƒ€ê²Ÿ ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •' - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ìœ„ì¹˜ ì„¤ì •")
        print("í˜„ì¬ ì„¤ì •:")
        print(f"- ì†ŒìŠ¤ ë””ë ‰í† ë¦¬: {source_dir}")
        print(f"- ë²¡í„°ìŠ¤í† ì–´ ìœ„ì¹˜: {vector_db_dir}")
        print("=" * 50)
    
    while True:
        display_menu()
        search_option = input("ê²€ìƒ‰ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš” (1-9): ")
        
        if search_option == '6':
            print("ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        elif search_option == '7':
            print("\në²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤...")
            try:
                build_vector_db(source_dir, vector_db_dir)
                print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ!")
                # ìƒˆë¡œ êµ¬ì¶•ëœ DB ë¡œë“œ
                db, vectorizer = load_vector_db()
            except Exception as e:
                print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue
            
        elif search_option == '8':
            new_source_dir = input(f"ìƒˆ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (í˜„ì¬: {source_dir}): ")
            if os.path.exists(new_source_dir):
                source_dir = new_source_dir
                print(f"ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ê°€ '{source_dir}'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print(f"ì˜¤ë¥˜: ë””ë ‰í† ë¦¬ '{new_source_dir}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue
            
        elif search_option == '9':
            new_vector_db_dir = input(f"ìƒˆ ë²¡í„°ìŠ¤í† ì–´ ìœ„ì¹˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (í˜„ì¬: {vector_db_dir}): ")
            vector_db_dir = new_vector_db_dir
            print(f"ë²¡í„°ìŠ¤í† ì–´ ìœ„ì¹˜ê°€ '{vector_db_dir}'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ìƒˆ ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì¡´ì¬í•˜ë©´ DB ë‹¤ì‹œ ë¡œë“œ
            if os.path.exists(vector_db_dir) and os.listdir(vector_db_dir):
                print("ìƒˆ ìœ„ì¹˜ì—ì„œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
                db, vectorizer = load_vector_db()
            else:
                print("ìƒˆ ìœ„ì¹˜ì—ëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜µì…˜ 7ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¶•í•˜ì„¸ìš”.")
                db, vectorizer = None, None
            continue
        
        # ê²€ìƒ‰ ê´€ë ¨ ì˜µì…˜ (1-5)
        section_map = {
            '1': 'full_text',
            '2': 'title',
            '3': 'problem',
            '4': 'solution',
            '5': 'results'
        }
        
        if search_option not in section_map:
            print("ì˜¬ë°”ë¥¸ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš” (1-9)")
            continue
        
        # ë²¡í„° DBê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if db is None or vectorizer is None:
            print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì˜µì…˜ 7ì„ ì‚¬ìš©í•˜ì—¬ ë¨¼ì € ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.")
            continue
        
        section = section_map[search_option]
        
        # ì¿¼ë¦¬ ì…ë ¥
        query = input("ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if not query.strip():
            print("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            continue
        
        # ê²°ê³¼ ìˆ˜ ì…ë ¥
        try:
            k = int(input("ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 3): ") or "3")
        except ValueError:
            k = 3
        
        # ê²€ìƒ‰ ì‹¤í–‰
        print(f"\n'{section}' ì„¹ì…˜ì—ì„œ '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼:")
        try:
            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ëŒ€ì²´ ì²˜ë¦¬
            if section not in db.indices:
                print(f"ê²½ê³ : '{section}' ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                available_indices = list(db.indices.keys())
                if not available_indices:
                    print("ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜µì…˜ 7ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•˜ì„¸ìš”.")
                    continue
                
                print(f"ëŒ€ì‹  '{available_indices[0]}' ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                section = available_indices[0]
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ì¿¼ë¦¬ ë²¡í„° ë¡œê¹…
            query_vector = vectorizer.model.encode([query])[0]
            print(f"ì¿¼ë¦¬ ë²¡í„° ìƒì„± ì„±ê³µ: ì°¨ì› {len(query_vector)}")
            
            results = db.search(query, section=section, k=k, vectorizer=vectorizer)
            
            if not results:
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ê²°ê³¼ í‘œì‹œ
            for i, result in enumerate(results):
                case = result['case_study']
                similarity = result['similarity'] * 100
                
                industry_emoji = case.get('emoji', 'ğŸ“„')
                print(f"\n{i+1}. {industry_emoji} {case['title']} (ìœ ì‚¬ë„: {similarity:.1f}%)")
                print(f"   ì‚°ì—…: {case['industry']}")
                print(f"   ID: {case['id']}")
                
                # ìì„¸í•œ ì •ë³´ í‘œì‹œ ì—¬ë¶€
                show_details = input("   ìì„¸í•œ ì •ë³´ë¥¼ ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
                if show_details:
                    for section_name in ['who', 'problem', 'solution', 'results']:
                        section_emoji = {
                            'who': 'ğŸ‘¥',
                            'problem': 'â“',
                            'solution': 'ğŸ’¡', 
                            'results': 'âœ…'
                        }
                        emoji_icon = section_emoji.get(section_name, '')
                        print(f"\n   {emoji_icon} {section_name.capitalize()}:")
                        print(f"   {case[section_name]}")
        except Exception as e:
            print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


# ë©”ì¸ í•¨ìˆ˜: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
def build_vector_db(case_studies_dir="case_studies", vector_db_dir="vector_db"):
    """
    ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    Args:
        case_studies_dir (str): ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
        vector_db_dir (str): ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
    """
    # 1. ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë¡œë“œ
    case_studies = load_case_studies(case_studies_dir)
    
    if not case_studies:
        print("ë¡œë“œëœ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # 2. ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™” ë° ë²¡í„° ìƒì„±
    # ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ ì‚¬ìš© (í•œêµ­ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•´)
    vectorizer = CaseStudyVectorizer("paraphrase-multilingual-MiniLM-L12-v2")
    vectors = vectorizer.vectorize_case_studies(case_studies)
    
    # 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ì €ì¥
    db = CaseStudyVectorDB(vectorizer.vector_size)
    db.build_indices(case_studies, vectors)
    db.save(vector_db_dir)
    
    print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ {vector_db_dir}ì— ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ì´ {len(case_studies)}ê°œì˜ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ê°€ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return db, vectorizer


# ë©”ì¸ í•¨ìˆ˜: ì½”ë“œ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    import argparse
    
    parser = argparse.ArgumentParser(description='Altibase ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ')
    parser.add_argument('--build', action='store_true', help='ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•')
    parser.add_argument('--search', action='store_true', help='ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ ì‹¤í–‰')
    parser.add_argument('--input-dir', type=str, default='case_studies', help='ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ë””ë ‰í† ë¦¬')
    parser.add_argument('--output-dir', type=str, default='vector_db', help='ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    if args.build:
        build_vector_db(args.input_dir, args.output_dir)
    
    if args.search or not (args.build or args.search):
        interactive_search()

        

        