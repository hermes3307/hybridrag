import os
import argparse
import json
import sys
import time
from typing import List, Dict, Any, Optional, Tuple

# LLM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from llama_cpp import Llama
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# VectorQuery í´ë˜ìŠ¤ ì„í¬íŠ¸
from vectorquery import VectorQuery

class CaseStudyLLM:
    def __init__(self, model_type: str = "llama", model_path: Optional[str] = None):
        """
        ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ì—¬ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ìƒì„±ì„ ìœ„í•œ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            model_type (str): ì‚¬ìš©í•  ëª¨ë¸ ìœ í˜• ("llama" ë˜ëŠ” "gemma")
            model_path (str, optional): ëª¨ë¸ íŒŒì¼ ê²½ë¡œ. ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        """
        self.model_type = model_type.lower()
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.vector_query = VectorQuery()
        
        # ëª¨ë¸ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        if self.model_path is None:
            if self.model_type == "llama":
                self.model_path = "models/llama-3-8b-instruct.gguf"  # ê¸°ë³¸ Llama 3 ê²½ë¡œ
            elif self.model_type == "gemma":
                self.model_path = "models/gemma-7b-it"  # ê¸°ë³¸ Gemma ê²½ë¡œ
        
        print(f"LLM ëª¨ë¸ ìœ í˜•: {self.model_type}")
        print(f"ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
    
    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.model_type == "llama":
                print("Llama ëª¨ë¸ ë¡œë“œ ì¤‘...")
                
                # ëª¨ë¸ ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ì¸ ê²½ìš°, ë””ë ‰í† ë¦¬ ë‚´ .gguf íŒŒì¼ ì°¾ê¸°
                if os.path.isdir(self.model_path):
                    gguf_files = [f for f in os.listdir(self.model_path) if f.endswith('.gguf')]
                    if not gguf_files:
                        raise ValueError(f"ì§€ì •ëœ ë””ë ‰í† ë¦¬ '{self.model_path}'ì—ì„œ .gguf íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ì²« ë²ˆì§¸ .gguf íŒŒì¼ ì‚¬ìš©
                    model_file = os.path.join(self.model_path, gguf_files[0])
                    print(f"ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼: {model_file}")
                else:
                    model_file = self.model_path
                
                self.model = Llama(
                    model_path=model_file,
                    n_ctx=4096,  # ì»¨í…ìŠ¤íŠ¸ í¬ê¸°
                    n_gpu_layers=-1,  # GPU ì‚¬ìš© ìµœëŒ€í™”
                    verbose=False
                )
                print("Llama ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                
            elif self.model_type == "gemma":
                print("Gemma ëª¨ë¸ ë¡œë“œ ì¤‘...")
                
                # ëª¨ë¸ ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ì¸ì§€ í™•ì¸
                if os.path.isdir(self.model_path):
                    model_dir = self.model_path
                else:
                    # ìƒìœ„ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    model_dir = self.model_path
                
                # Gemma ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_dir,
                        device_map="auto",
                        torch_dtype=torch.float16
                    )
                    print("Gemma ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
                except Exception as e:
                    raise ValueError(f"Gemma ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                
            else:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ìœ í˜•: {self.model_type}")
            
            return True
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
    
    def load_vector_db(self, vector_db_dir: str = "vector_db"):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
        print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì¤‘: {vector_db_dir}")
        return self.vector_query.load()
    
    def query_similar_cases(self, 
                           title: str, 
                           who: str, 
                           problem: str, 
                           solution: str, 
                           results: str,
                           k: int = 5) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ê²€ìƒ‰
        
        Args:
            title, who, problem, solution, results: ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì„¹ì…˜
            k: ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            List[Dict]: ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ëª©ë¡
        """
        # ì„¹ì…˜ë³„ ê°€ì¤‘ì¹˜ (ì¤‘ìš”ë„ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
        weights = {
            'title': 0.1,
            'who': 0.1,
            'problem': 0.4,
            'solution': 0.3,
            'results': 0.1
        }
        
        # ê° ì„¹ì…˜ì— ëŒ€í•´ ì¿¼ë¦¬ ì‹¤í–‰
        results_by_section = {}
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
        full_text = f"{title}\n{who}\n{problem}\n{solution}\n{results}"
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ (ê°€ì¥ ì¤‘ìš”)
        print("ì „ì²´ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ê²€ìƒ‰ ì¤‘...")
        full_text_results = self.vector_query.query(
            full_text, 
            section='full_text', 
            k=k*2,  # ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            print_results=False
        )
        
        # ì„¹ì…˜ë³„ ì¿¼ë¦¬
        sections = {
            'problem': problem,
            'solution': solution
        }
        
        for section_name, section_text in sections.items():
            if section_text.strip():
                print(f"{section_name} ì„¹ì…˜ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ê²€ìƒ‰ ì¤‘...")
                results_by_section[section_name] = self.vector_query.query(
                    section_text, 
                    section=section_name, 
                    k=k, 
                    print_results=False
                )
        
        # ê²°ê³¼ ë³‘í•© ë° ì ìˆ˜ ê³„ì‚°
        merged_results = {}
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€
        for result in full_text_results:
            case_id = result['case_study']['id']
            merged_results[case_id] = {
                'case_study': result['case_study'],
                'score': result['similarity'] * 0.6,  # ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜
                'matched_sections': ['full_text']
            }
        
        # ì„¹ì…˜ë³„ ê²°ê³¼ ì¶”ê°€
        for section_name, section_results in results_by_section.items():
            for result in section_results:
                case_id = result['case_study']['id']
                if case_id in merged_results:
                    merged_results[case_id]['score'] += result['similarity'] * weights.get(section_name, 0.1)
                    merged_results[case_id]['matched_sections'].append(section_name)
                else:
                    merged_results[case_id] = {
                        'case_study': result['case_study'],
                        'score': result['similarity'] * weights.get(section_name, 0.1),
                        'matched_sections': [section_name]
                    }
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(
            merged_results.values(), 
            key=lambda x: x['score'], 
            reverse=True
        )
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
        return sorted_results[:k]
    
    def generate_prompt(self, 
                        user_case: Dict[str, str], 
                        similar_cases: List[Dict[str, Any]]) -> str:
        """
        LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            user_case: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””
            similar_cases: ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ëª©ë¡
            
        Returns:
            str: ìƒì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        if self.model_type == "llama":
            prompt = "<|system|>\n"
            prompt += "ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê³ ê°ì´ ì œê³µí•œ ì •ë³´ì™€ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ì°¸ê³ í•˜ì—¬ ì™„ì„±ë„ ë†’ì€ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
            prompt += "ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œì™€ ì†”ë£¨ì…˜ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê³ , ê²°ê³¼ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            prompt += "ì œê³µëœ ìœ ì‚¬ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ì˜ í˜•ì‹ê³¼ ê¹Šì´ë¥¼ ì°¸ê³ í•˜ë˜, ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³  ìƒˆë¡œìš´ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
            prompt += "</|system|>\n\n"
            
            prompt += "<|user|>\n"
            
        elif self.model_type == "gemma":
            prompt = "<start_of_turn>system\n"
            prompt += "ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê³ ê°ì´ ì œê³µí•œ ì •ë³´ì™€ ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ì°¸ê³ í•˜ì—¬ ì™„ì„±ë„ ë†’ì€ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
            prompt += "ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œì™€ ì†”ë£¨ì…˜ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ê³ , ê²°ê³¼ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            prompt += "ì œê³µëœ ìœ ì‚¬ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ì˜ í˜•ì‹ê³¼ ê¹Šì´ë¥¼ ì°¸ê³ í•˜ë˜, ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³  ìƒˆë¡œìš´ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
            prompt += "<end_of_turn>\n\n"
            
            prompt += "<start_of_turn>user\n"
        
        # ì‚¬ìš©ì ì…ë ¥ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””
        prompt += "ë‹¤ìŒì€ ë‚´ê°€ ì‘ì„± ì¤‘ì¸ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ì…ë‹ˆë‹¤:\n\n"
        prompt += f"ì œëª©: {user_case.get('title', '')}\n\n"
        prompt += f"Who: {user_case.get('who', '')}\n\n"
        prompt += f"Problem: {user_case.get('problem', '')}\n\n"
        prompt += f"Solution: {user_case.get('solution', '')}\n\n"
        prompt += f"Results: {user_case.get('results', '')}\n\n"
        
        # ìœ ì‚¬ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””
        prompt += "ë‹¤ìŒì€ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë“¤ì…ë‹ˆë‹¤:\n\n"
        
        for i, case in enumerate(similar_cases, 1):
            case_study = case['case_study']
            score = case['score'] * 100  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
            
            prompt += f"ìœ ì‚¬ ì¼€ì´ìŠ¤ {i} (ìœ ì‚¬ë„: {score:.1f}%):\n"
            prompt += f"ì œëª©: {case_study.get('title', '')}\n"
            prompt += f"Who: {case_study.get('who', '')}\n"
            prompt += f"Problem: {case_study.get('problem', '')}\n"
            prompt += f"Solution: {case_study.get('solution', '')}\n"
            prompt += f"Results: {case_study.get('results', '')}\n\n"
        
        prompt += "ìœ„ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‚´ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ ê°œì„ í•˜ê³  ì™„ì„±í•´ì£¼ì„¸ìš”. ì œëª©, Who, Problem, Solution, Results ì„¹ì…˜ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
        
        if self.model_type == "llama":
            prompt += "</|user|>\n\n<|assistant|>\n"
        elif self.model_type == "gemma":
            prompt += "<end_of_turn>\n\n<start_of_turn>model\n"
        
        return prompt
    
    def generate_response(self, prompt: str, max_tokens: int = 2048) -> str:
        """
        LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            str: ìƒì„±ëœ ì‘ë‹µ
        """
        try:
            if self.model_type == "llama":
                print("Llama ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
                response = self.model(
                    prompt,
                    max_tokens=max_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repeat_penalty=1.1
                )
                
                # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                result = response["choices"][0]["text"]
                
            elif self.model_type == "gemma":
                print("Gemma ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
                
                # ëª¨ë¸ ìƒì„±
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        temperature=0.7,
                        top_p=0.9,
                        repetition_penalty=1.1,
                        do_sample=True
                    )
                
                # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                result = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
                
                # Gemma ì‘ë‹µì—ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                end_marker = "<end_of_turn>"
                if end_marker in result:
                    result = result.split(end_marker)[0].strip()
            
            return result
            
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return f"ì˜¤ë¥˜: ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {str(e)}"

def parse_case_study_text(text: str) -> Dict[str, str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´ íŒŒì‹±
    
    Args:
        text: ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” í…ìŠ¤íŠ¸
        
    Returns:
        Dict: íŒŒì‹±ëœ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´
    """
    case_study = {
        'title': '',
        'who': '',
        'problem': '',
        'solution': '',
        'results': ''
    }
    
    # ì œëª© ì¶”ì¶œ
    title_match = None
    if "ì œëª©:" in text:
        parts = text.split("ì œëª©:", 1)
        if len(parts) > 1:
            title_text = parts[1].strip()
            if "\n" in title_text:
                title_text = title_text.split("\n", 1)[0]
            case_study['title'] = title_text
    
    # ì„¹ì…˜ ì¶”ì¶œ
    sections = {
        'Who': 'who',
        'Problem': 'problem',
        'Solution': 'solution',
        'Results': 'results'
    }
    
    for section, key in sections.items():
        marker = f"{section}:"
        if marker in text:
            parts = text.split(marker, 1)
            if len(parts) > 1:
                section_text = parts[1].strip()
                for next_section in sections.keys():
                    next_marker = f"{next_section}:"
                    if next_marker in section_text:
                        section_text = section_text.split(next_marker, 1)[0]
                
                case_study[key] = section_text.strip()
    
    return case_study

def get_user_input() -> Dict[str, str]:
    """
    ì‚¬ìš©ìë¡œë¶€í„° ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´ ì…ë ¥ ë°›ê¸°
    
    Returns:
        Dict: ì‚¬ìš©ì ì…ë ¥ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´
    """
    print("\n" + "=" * 80)
    print("ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ê° ì„¹ì…˜ ì…ë ¥ì„ ë§ˆì¹˜ë©´ 'ì™„ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("=" * 80)
    
    case_study = {}
    
    # ì œëª© ì…ë ¥
    case_study['title'] = input("\nì œëª©: ")
    
    # ì„¹ì…˜ë³„ ì…ë ¥
    sections = [
        ('who', 'Who (íšŒì‚¬/ê¸°ê´€ ì •ë³´)'),
        ('problem', 'Problem (ë¬¸ì œ ìƒí™©)'),
        ('solution', 'Solution (í•´ê²° ë°©ì•ˆ)'),
        ('results', 'Results (ê²°ê³¼)')
    ]
    
    for key, prompt in sections:
        print(f"\n{prompt} (ì—¬ëŸ¬ ì¤„ ì…ë ¥ ê°€ëŠ¥, ì…ë ¥ì„ ë§ˆì¹˜ë ¤ë©´ 'ì™„ë£Œ' ì…ë ¥)")
        lines = []
        while True:
            line = input("> ")
            if line.strip().lower() == 'ì™„ë£Œ':
                break
            lines.append(line)
        case_study[key] = "\n".join(lines)
    
    return case_study

def get_text_input() -> Dict[str, str]:
    """
    í…ìŠ¤íŠ¸ë¡œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´ ì…ë ¥ ë°›ê¸°
    
    Returns:
        Dict: íŒŒì‹±ëœ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´
    """
    print("\n" + "=" * 80)
    print("ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ì…ë ¥ì„ ë§ˆì¹˜ë©´ ë¹ˆ ì¤„ì—ì„œ Ctrl+D (Unix) ë˜ëŠ” Ctrl+Z (Windows)ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
    print("=" * 80)
    print("\nì˜ˆì‹œ í˜•ì‹:")
    print("ì œëª©: AAA ì‹œìŠ¤í…œ â€“ íšŒì‚¬ëª…")
    print("Who: íšŒì‚¬/ê¸°ê´€ ì •ë³´...")
    print("Problem: ë¬¸ì œ ìƒí™©...")
    print("Solution: í•´ê²° ë°©ì•ˆ...")
    print("Results: ê²°ê³¼...")
    print("\ní…ìŠ¤íŠ¸ ì…ë ¥:")
    
    # ì—¬ëŸ¬ ì¤„ ì…ë ¥ ë°›ê¸°
    lines = []
    try:
        while True:
            line = input()
            lines.append(line)
    except EOFError:
        # ì…ë ¥ ì¢…ë£Œ
        pass
    
    text = "\n".join(lines)
    return parse_case_study_text(text)

def list_model_files(model_dir):
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ íŒŒì¼ ëª©ë¡ í‘œì‹œ
    
    Args:
        model_dir: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    if not os.path.isdir(model_dir):
        print(f"ê²½ê³ : '{model_dir}'ëŠ” ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
        return
    
    print(f"\n'{model_dir}' ë””ë ‰í† ë¦¬ì˜ ëª¨ë¸ íŒŒì¼:")
    print("-" * 60)
    
    # Llama ëª¨ë¸ íŒŒì¼ (.gguf)
    llama_files = [f for f in os.listdir(model_dir) if f.endswith('.gguf')]
    if llama_files:
        print("Llama ëª¨ë¸ íŒŒì¼ (.gguf):")
        for i, file in enumerate(llama_files, 1):
            file_path = os.path.join(model_dir, file)
            file_size = os.path.getsize(file_path) / (1024 * 1024 * 1024)  # GB ë‹¨ìœ„
            print(f"  {i}. {file} ({file_size:.2f} GB)")
    
    # Gemma ëª¨ë¸ ë””ë ‰í† ë¦¬
    gemma_dirs = [d for d in os.listdir(model_dir) 
                 if os.path.isdir(os.path.join(model_dir, d)) and 
                 (d.startswith('gemma') or 
                  any(os.path.exists(os.path.join(model_dir, d, f)) for f in ['config.json', 'tokenizer.json']))]
    
    if gemma_dirs:
        print("\nGemma ëª¨ë¸ ë””ë ‰í† ë¦¬:")
        for i, dir_name in enumerate(gemma_dirs, 1):
            print(f"  {i}. {dir_name}")
    
    if not llama_files and not gemma_dirs:
        print("  ë°œê²¬ëœ ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
    
    print("-" * 60)

def interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰"""
    print("\n" + "=" * 80)
    print("ğŸ¤– ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ìƒì„± ì‹œìŠ¤í…œ - ëŒ€í™”í˜• ëª¨ë“œ")
    print("=" * 80)
    
    # ëª¨ë¸ ì„ íƒ
    print("\nì‚¬ìš©í•  LLM ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. Llama 3")
    print("2. Gemma")
    
    model_choice = input("\nëª¨ë¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 1): ") or "1"
    model_type = "llama" if model_choice == "1" else "gemma"
    
    # ëª¨ë¸ ê²½ë¡œ ì…ë ¥
    model_path = input(f"\n{model_type.capitalize()} ëª¨ë¸ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’ ì‚¬ìš©: ì—”í„°): ")
    if not model_path:
        model_path = None
    elif os.path.isdir(model_path):
        # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë¸ íŒŒì¼ ëª©ë¡ í‘œì‹œ
        list_model_files(model_path)
        
        if model_type == "llama":
            # Llama ëª¨ë¸ íŒŒì¼ ì„ íƒ
            llama_files = [f for f in os.listdir(model_path) if f.endswith('.gguf')]
            if llama_files:
                print("\nLlama ëª¨ë¸ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”:")
                for i, file in enumerate(llama_files, 1):
                    print(f"{i}. {file}")
                
                file_choice = input("\níŒŒì¼ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 1): ") or "1"
                try:
                    file_idx = int(file_choice) - 1
                    if 0 <= file_idx < len(llama_files):
                        model_path = os.path.join(model_path, llama_files[file_idx])
                        print(f"ì„ íƒëœ ëª¨ë¸ íŒŒì¼: {model_path}")
                    else:
                        print("ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        model_path = os.path.join(model_path, llama_files[0])
                except ValueError:
                    print("ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    model_path = os.path.join(model_path, llama_files[0])
        elif model_type == "gemma":
            # Gemma ëª¨ë¸ ë””ë ‰í† ë¦¬ ì„ íƒ
            gemma_dirs = [d for d in os.listdir(model_path) 
                         if os.path.isdir(os.path.join(model_path, d)) and 
                         (d.startswith('gemma') or 
                          any(os.path.exists(os.path.join(model_path, d, f)) for f in ['config.json', 'tokenizer.json']))]
            
            if gemma_dirs:
                print("\nGemma ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
                for i, dir_name in enumerate(gemma_dirs, 1):
                    print(f"{i}. {dir_name}")
                
                dir_choice = input("\në””ë ‰í† ë¦¬ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 1): ") or "1"
                try:
                    dir_idx = int(dir_choice) - 1
                    if 0 <= dir_idx < len(gemma_dirs):
                        model_path = os.path.join(model_path, gemma_dirs[dir_idx])
                        print(f"ì„ íƒëœ ëª¨ë¸ ë””ë ‰í† ë¦¬: {model_path}")
                    else:
                        print("ìœ íš¨í•˜ì§€ ì•Šì€ ì„ íƒì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        model_path = os.path.join(model_path, gemma_dirs[0])
                except ValueError:
                    print("ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    model_path = os.path.join(model_path, gemma_dirs[0])
    
    # LLM ê°ì²´ ìƒì„±
    llm = CaseStudyLLM(model_type, model_path)
    
    # ëª¨ë¸ ë¡œë“œ
    if not llm.load_model():
        print("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    vector_db_dir = input("\në²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: vector_db): ") or "vector_db"
    if not llm.load_vector_db(vector_db_dir):
        print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ì…ë ¥ ë°©ì‹ ì„ íƒ
    print("\nì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´ ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ëŒ€í™”í˜• ì…ë ¥ (ì„¹ì…˜ë³„ë¡œ ì°¨ë¡€ëŒ€ë¡œ ì…ë ¥)")
    print("2. í…ìŠ¤íŠ¸ ì…ë ¥ (ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì…ë ¥)")
    
    input_choice = input("\nì…ë ¥ ë°©ì‹ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 1): ") or "1"
    
    # ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´ ì…ë ¥
    if input_choice == "1":
        user_case = get_user_input()
    else:
        user_case = get_text_input()
    
    # ì…ë ¥ í™•ì¸
    print("\n" + "=" * 80)
    print("ì…ë ¥í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì •ë³´:")
    print("=" * 80)
    print(f"ì œëª©: {user_case.get('title', '')}")
    print(f"Who: {user_case.get('who', '')}")
    print(f"Problem: {user_case.get('problem', '')}")
    print(f"Solution: {user_case.get('solution', '')}")
    print(f"Results: {user_case.get('results', '')}")
    
    # ê²€ìƒ‰í•  ìœ ì‚¬ ì¼€ì´ìŠ¤ ìˆ˜ ì…ë ¥
    try:
        k = int(input("\nê²€ìƒ‰í•  ìœ ì‚¬ ì¼€ì´ìŠ¤ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 5): ") or "5")
    except ValueError:
        k = 5
    
    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰
    similar_cases = llm.query_similar_cases(
        user_case.get('title', ''),
        user_case.get('who', ''),
        user_case.get('problem', ''),
        user_case.get('solution', ''),
        user_case.get('results', ''),
        k=k
    )
    
    # ìœ ì‚¬ ì¼€ì´ìŠ¤ ì •ë³´ ì¶œë ¥
    print("\n" + "=" * 80)
    print(f"ìœ ì‚¬í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” {len(similar_cases)}ê°œ ì°¾ìŒ:")
    print("=" * 80)
    
    for i, case in enumerate(similar_cases, 1):
        case_study = case['case_study']
        score = case['score'] * 100  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
        matched_sections = case.get('matched_sections', [])
        
        print(f"\n{i}. {case_study.get('title', '')} (ìœ ì‚¬ë„: {score:.1f}%)")
        print(f"   ID: {case_study.get('id', '')}")
        print(f"   ì‚°ì—…: {case_study.get('industry', 'ê¸°íƒ€')}")
        print(f"   ë§¤ì¹­ ì„¹ì…˜: {', '.join(matched_sections)}")
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = llm.generate_prompt(user_case, similar_cases)
    
    # í”„ë¡¬í”„íŠ¸ ì¶œë ¥ ë° í¸ì§‘
    while True:
        print("\n" + "=" * 80)
        print("ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ (LLMì— ì „ë‹¬ë¨):")
        print("=" * 80)
        print(prompt)
        
        edit_choice = input("\ní”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: n): ").lower() or "n"
        
        if edit_choice == "y":
            print("\ní”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”. ì…ë ¥ì„ ë§ˆì¹˜ë©´ ë¹ˆ ì¤„ì—ì„œ Ctrl+D (Unix) ë˜ëŠ” Ctrl+Z (Windows)ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            
            # ì—¬ëŸ¬ ì¤„ ì…ë ¥ ë°›ê¸°
            lines = []
            try:
                while True:
                    line = input()
                    lines.append(line)
            except EOFError:
                # ì…ë ¥ ì¢…ë£Œ
                pass
            
            prompt = "\n".join(lines)
        else:
            break
    
    # LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±
    print("\nLLMìœ¼ë¡œ ì‘ë‹µ ìƒì„± ì¤‘... (ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    start_time = time.time()
    response = llm.generate_response(prompt)
    end_time = time.time()
    
    # ì‘ë‹µ ì¶œë ¥
    print("\n" + "=" * 80)
    print(f"ìƒì„±ëœ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” (ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ):")
    print("=" * 80)
    print(response)
    
    # ì €ì¥ ì—¬ë¶€ í™•ì¸
    save_choice = input("\nìƒì„±ëœ ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: n): ").lower() or "n"
    
    if save_choice == "y":
        file_path = input("ì €ì¥í•  íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: case_study_output.txt): ") or "case_study_output.txt"
        
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(response)
        
        print(f"ì¼€ì´ìŠ¤ ìŠ¤í„°ë””ê°€ '{file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë¡œì»¬ LLMì„ ì‚¬ìš©í•œ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ìƒì„±')
    parser.add_argument('--model', type=str, choices=['llama', 'gemma'], default='llama', help='ì‚¬ìš©í•  ëª¨ë¸ ìœ í˜• (llama ë˜ëŠ” gemma)')
    parser.add_argument('--model-path', type=str, help='ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--list-models', '-l', action='store_true', help='ì§€ì •ëœ ë””ë ‰í† ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í‘œì‹œ')
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ëª©ë¡ í‘œì‹œ ëª¨ë“œ
    if args.list_models and args.model_path:
        list_model_files(args.model_path)
        return
    
    # ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
    interactive_mode()

if __name__ == "__main__":
    main()