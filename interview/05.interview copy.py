#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LIGNEX1 RAG ê¸°ë°˜ ì¡°ì§ì í•©ë„ ì¸ì„±ë©´ì ‘ ì‹œìŠ¤í…œ
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ë©´ì ‘ í”„ë¡œê·¸ë¨
"""

import streamlit as st
import json
import sqlite3
import logging
import os
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import hashlib
import asyncio
from collections import Counter, defaultdict
import time  # ìƒë‹¨ì— ì¶”ê°€

# ë²¡í„° ê²€ìƒ‰ ê´€ë ¨
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range
from sentence_transformers import SentenceTransformer
import numpy as np
import streamlit as st
import numpy as np



try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range
    QDRANT_AVAILABLE = True
except ImportError:
    QDRANT_AVAILABLE = False
    st.warning("âš ï¸ Qdrantê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    st.warning("âš ï¸ SentenceTransformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TF-IDFë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# ìì—°ì–´ ì²˜ë¦¬
import nltk
try:
    # ìƒˆ ë²„ì „ ë¨¼ì € ì‹œë„
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    try:
        nltk.download('punkt_tab', quiet=True)
    except:
        # êµ¬ ë²„ì „ìœ¼ë¡œ fallback
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt', quiet=True)

# ì„¤ì •
st.set_page_config(
    page_title="LIGNEX1 RAG ë©´ì ‘ ì‹œìŠ¤í…œ",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('lignex1_interview.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ìŠ¤íƒ€ì¼ ì„¤ì •
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1E3A8A;
        text-align: center;
        margin-bottom: 2rem;
        padding: 1rem;
        background: linear-gradient(90deg, #3B82F6, #1E40AF);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    
    .sub-header {
        font-size: 1.5rem;
        color: #374151;
        margin-bottom: 1rem;
        padding-left: 10px;
        border-left: 4px solid #3B82F6;
    }
    
    .interview-card {
        background: #F8FAFC;
        padding: 1.5rem;
        border-radius: 10px;
        border: 1px solid #E2E8F0;
        margin: 1rem 0;
    }
    
    .context-box {
        background: #EEF2FF;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #3B82F6;
        margin: 1rem 0;
    }
    
    .score-good {
        color: #059669;
        font-weight: bold;
    }
    
    .score-average {
        color: #D97706;
        font-weight: bold;
    }
    
    .score-poor {
        color: #DC2626;
        font-weight: bold;
    }
    
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

@dataclass
class InterviewQuestion:
    """ë©´ì ‘ ì§ˆë¬¸ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    text: str
    category: str
    context_keywords: List[str]
    follow_up_questions: List[str]
    difficulty_level: str = "medium"
    expected_duration: int = 5  # ë¶„
    evaluation_criteria: List[str] = None

@dataclass
class InterviewAnswer:
    """ë©´ì ‘ ë‹µë³€ ë°ì´í„° í´ë˜ìŠ¤"""
    question_id: str
    question_text: str
    answer: str
    context_used: List[Dict]
    timestamp: str
    evaluation_score: float = 0.0
    keyword_match_score: float = 0.0
    answer_length: int = 0
    sentiment_score: float = 0.0
    coherence_score: float = 0.0

@dataclass
class InterviewSession:
    """ë©´ì ‘ ì„¸ì…˜ ë°ì´í„° í´ë˜ìŠ¤"""
    session_id: str
    candidate_name: str
    position: str
    start_time: str
    end_time: Optional[str] = None
    answers: List[InterviewAnswer] = None
    overall_score: float = 0.0
    feedback: str = ""
    recommendations: List[str] = None

class TextAnalyzer:
    """í…ìŠ¤íŠ¸ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ê°ì • ë¶„ì„ìš© í‚¤ì›Œë“œ
        self.positive_keywords = {
            'ì„±ì·¨', 'ì„±ê³µ', 'ê·¹ë³µ', 'ì—´ì •', 'ë„ì „', 'ì„±ì¥', 'ë°œì „', 'í˜‘ë ¥', 'ì†Œí†µ',
            'ì°½ì˜', 'í˜ì‹ ', 'ì±…ì„', 'ë¦¬ë”ì‹­', 'íŒ€ì›Œí¬', 'ëª©í‘œ', 'ë¹„ì „', 'ê¸ì •',
            'ìì‹ ê°', 'ëŠ¥ë ¥', 'ê²½í—˜', 'í•™ìŠµ', 'ê°œì„ ', 'íš¨ìœ¨', 'í’ˆì§ˆ', 'ìš°ìˆ˜',
            'achievement', 'success', 'overcome', 'passion', 'challenge', 'growth'
        }
        
        self.negative_keywords = {
            'ì‹¤íŒ¨', 'í¬ê¸°', 'ì–´ë ¤ì›€', 'ë¬¸ì œ', 'í•œê³„', 'ë¶€ì¡±', 'ê±±ì •', 'ìŠ¤íŠ¸ë ˆìŠ¤',
            'ê°ˆë“±', 'ì••ë°•', 'ìœ„ê¸°', 'ë¯¸í¡', 'ì‹¤ìˆ˜', 'ì§€ì—°', 'ì†ì‹¤',
            'failure', 'give up', 'difficult', 'problem', 'limitation', 'worry'
        }
        
        # LIGNEX1 ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ
        self.company_keywords = {
            'culture': ['OPEN', 'POSITIVE', 'ê°œë°©', 'ê¸ì •', 'ê¸°ì—…ë¬¸í™”', 'í•µì‹¬ê°€ì¹˜', 'ì¡°ì§ë¬¸í™”'],
            'business': ['ë°©ìœ„ì‚°ì—…', 'êµ­ë°©', 'ë¯¸ì‚¬ì¼', 'ë ˆì´ë”', 'ë¬´ê¸°ì²´ê³„', 'ì•ˆë³´', 'êµ­ê°€'],
            'technology': ['R&D', 'ì—°êµ¬ê°œë°œ', 'ê¸°ìˆ ê°œë°œ', 'í˜ì‹ ', 'ì²¨ë‹¨ê¸°ìˆ ', 'ì—”ì§€ë‹ˆì–´ë§'],
            'performance': ['ì„±ê³¼', 'ìˆ˜ì£¼', 'ë§¤ì¶œ', 'ì„±ì¥', 'ì‹¤ì ', 'ëª©í‘œë‹¬ì„±'],
            'future': ['ë¯¸ë˜', 'ë¹„ì „', 'ì „ëµ', 'ê³„íš', 'ëª©í‘œ', 'ë°œì „ë°©í–¥']
        }
    
    def analyze_sentiment(self, text: str) -> float:
        """ê°ì • ë¶„ì„ (ê¸ì •ë„ ì ìˆ˜ ë°˜í™˜)"""
        if not text:
            return 0.5
        
        text_lower = text.lower()
        positive_count = sum(1 for word in self.positive_keywords if word in text_lower)
        negative_count = sum(1 for word in self.negative_keywords if word in text_lower)
        
        total_words = len(text.split())
        if total_words == 0:
            return 0.5
        
        # ì •ê·œí™”ëœ ê°ì • ì ìˆ˜ (0.0 ~ 1.0)
        sentiment_score = 0.5 + (positive_count - negative_count) / max(total_words, 1)
        return max(0.0, min(1.0, sentiment_score))
    
    def calculate_keyword_match(self, text: str, keywords: List[str]) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        if not text or not keywords:
            return 0.0
        
        text_lower = text.lower()
        matched_keywords = sum(1 for keyword in keywords if keyword.lower() in text_lower)
        
        return min(1.0, matched_keywords / len(keywords))
    
    def calculate_coherence_score(self, text: str) -> float:
        """ë‹µë³€ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        if not text or len(text.split()) < 10:
            return 0.3
        
        sentences = nltk.sent_tokenize(text)
        if len(sentences) < 2:
            return 0.5
        
        # ë¬¸ì¥ ê°„ ì—°ê²°ì„± ë¶„ì„ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        transition_words = ['ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ì¦‰', 'ì˜ˆë¥¼ ë“¤ì–´', 'ê²°ê³¼ì ìœ¼ë¡œ']
        transition_count = sum(1 for word in transition_words if word in text)
        
        # ë¬¸ì¥ ê¸¸ì´ ì¼ê´€ì„±
        sentence_lengths = [len(s.split()) for s in sentences]
        length_variance = np.var(sentence_lengths) if sentence_lengths else 0
        
        # ì ìˆ˜ ê³„ì‚°
        coherence_score = 0.5 + (transition_count / len(sentences)) * 0.3
        coherence_score -= min(0.2, length_variance / 100)  # ê¸¸ì´ í¸ì°¨ í˜ë„í‹°
        
        return max(0.0, min(1.0, coherence_score))

class QdrantRAGService:
    """Qdrant RAG ì„œë¹„ìŠ¤ (Fallback í¬í•¨)"""
    
    def __init__(self, 
                 qdrant_host: str = "localhost",
                 qdrant_port: int = 6333,
                 collection_name: str = "lignex1_news",
                 model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        
        self.collection_name = collection_name
        self.analyzer = TextAnalyzer()
        self.model_name = model_name
        
        try:
            self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
            logger.info(f"Qdrant ì—°ê²° ì„±ê³µ: {qdrant_host}:{qdrant_port}")
        except Exception as e:
            logger.error(f"Qdrant ì—°ê²° ì‹¤íŒ¨: {e}")
            self.qdrant_client = None
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (SentenceTransformer ìš°ì„ , ì‹¤íŒ¨ì‹œ TF-IDF)
        self.embedding_model = None
        self.vector_size = 384  # ê¸°ë³¸ê°’
        
        # ë¨¼ì € SentenceTransformer ì‹œë„
        try:
            from sentence_transformers import SentenceTransformer
            self.embedding_model = SentenceTransformer(model_name)
            self.vector_size = self.embedding_model.get_sentence_embedding_dimension()
            logger.info(f"SentenceTransformer ë¡œë“œ ì™„ë£Œ: {model_name}")
            self.use_sentence_transformer = True
        except Exception as e:
            logger.warning(f"SentenceTransformer ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("TF-IDF ê¸°ë°˜ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            
            # TF-IDF ê¸°ë°˜ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
            try:
                self.embedding_model = SimpleTfidfEmbedder(vector_size=384)
                self.vector_size = self.embedding_model.get_sentence_embedding_dimension()
                logger.info(f"TF-IDF ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                self.use_sentence_transformer = False
            except Exception as e2:
                logger.error(f"TF-IDF ì„ë² ë”© ëª¨ë¸ ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
                self.embedding_model = None
    
    def is_available(self) -> bool:
        """RAG ì„œë¹„ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return self.qdrant_client is not None and self.embedding_model is not None
    
    def search_context(self, query: str, keywords: List[str] = None, limit: int = 5) -> List[Dict]:
        """ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        if not self.is_available():
            return []
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vector = self.embedding_model.encode(query)
            
            # TF-IDF ëª¨ë¸ì˜ ê²½ìš° ë” ë‚®ì€ threshold ì‚¬ìš©
            score_threshold = 0.3 if self.use_sentence_transformer else 0.1
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_result = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_vector.tolist(),
                limit=limit,
                score_threshold=score_threshold
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            contexts = []
            for hit in search_result:
                context = {
                    'id': hit.id,
                    'score': hit.score,
                    'title': hit.payload.get('title', ''),
                    'content': hit.payload.get('content', ''),
                    'source': hit.payload.get('source', ''),
                    'url': hit.payload.get('url', ''),
                    'keywords': hit.payload.get('keywords', []),
                    'published_date': hit.payload.get('published_date', ''),
                    'relevance_score': hit.score,
                    'embedding_model': hit.payload.get('embedding_model', 'unknown')
                }
                contexts.append(context)
            
            return contexts
            
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_enhanced_context(self, question: InterviewQuestion) -> List[Dict]:
        """ì§ˆë¬¸ì— ëŒ€í•œ í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ì œê³µ"""
        all_contexts = []
        
        # ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¡œ ê²€ìƒ‰
        contexts = self.search_context(question.text, limit=3)
        all_contexts.extend(contexts)
        
        # í‚¤ì›Œë“œë³„ ê²€ìƒ‰
        for keyword in question.context_keywords[:3]:
            contexts = self.search_context(keyword, limit=2)
            all_contexts.extend(contexts)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ìˆœ ì •ë ¬
        unique_contexts = {}
        for context in all_contexts:
            context_id = context['id']
            if context_id not in unique_contexts or context['score'] > unique_contexts[context_id]['score']:
                unique_contexts[context_id] = context
        
        return sorted(unique_contexts.values(), key=lambda x: x['score'], reverse=True)[:5]

class SimpleTfidfEmbedder:
    """TF-IDF ê¸°ë°˜ ê°„ë‹¨í•œ ì„ë² ë”© ì‹œìŠ¤í…œ (SentenceTransformer ëŒ€ì²´ìš©)"""
    
    def __init__(self, vector_size: int = 384):
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            
            self.vector_size = vector_size
            self.vectorizer = TfidfVectorizer(
                max_features=2000,
                ngram_range=(1, 2),
                min_df=1,
                max_df=0.9,
                stop_words=None  # í•œêµ­ì–´ ì§€ì›ì„ ìœ„í•´ None
            )
            
            # ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ SVD
            self.svd = TruncatedSVD(n_components=min(vector_size, 384))
            self.is_fitted = False
            
            # í•œêµ­ì–´/ì˜ì–´ ë¶ˆìš©ì–´
            self.stopwords = {
                'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë˜ëŠ”', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ',
                'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ',
                'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
                'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',
                'after', 'above', 'below', 'between', 'among'
            }
            
            logger.info(f"TF-IDF ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ì°¨ì›: {vector_size})")
            
        except ImportError:
            logger.error("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install scikit-learn")
            raise
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¹€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ë¶ˆìš©ì–´ ì œê±°
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in self.stopwords and len(word) > 1]
        
        return ' '.join(filtered_words)
    
    def fit(self, texts: List[str]):
        """í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ëª¨ë¸ í›ˆë ¨"""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = [self.preprocess_text(text) for text in texts]
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        processed_texts = [text for text in processed_texts if text.strip()]
        
        if not processed_texts:
            logger.warning("ì „ì²˜ë¦¬ í›„ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # TF-IDF ë²¡í„°í™”
        tfidf_matrix = self.vectorizer.fit_transform(processed_texts)
        
        # SVDë¡œ ì°¨ì› ì¶•ì†Œ
        if tfidf_matrix.shape[1] > self.vector_size:
            self.svd.fit(tfidf_matrix)
        
        self.is_fitted = True
        logger.info(f"TF-IDF ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(processed_texts)}ê°œ ë¬¸ì„œ")
    
    def encode(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if isinstance(text, str):
            texts = [text]
            single_input = True
        else:
            texts = text
            single_input = False
        
        if not self.is_fitted:
            # ê¸°ë³¸ í›ˆë ¨ ë°ì´í„°ë¡œ í›ˆë ¨
            default_texts = [
                "LIGë„¥ìŠ¤ì› ê¸°ì—…ë¬¸í™” OPEN POSITIVE",
                "ë°©ìœ„ì‚°ì—… êµ­ë°© ë¯¸ì‚¬ì¼ ë ˆì´ë”",
                "ì—°êµ¬ê°œë°œ R&D ê¸°ìˆ ê°œë°œ í˜ì‹ ",
                "ì„±ê³¼ ìˆ˜ì£¼ ë§¤ì¶œ ì„±ì¥ ì‹¤ì ",
                "ë¯¸ë˜ ë¹„ì „ ì „ëµ ê³„íš ëª©í‘œ"
            ] + texts
            self.fit(default_texts)
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = [self.preprocess_text(t) for t in texts]
        
        # TF-IDF ë³€í™˜
        tfidf_vectors = self.vectorizer.transform(processed_texts)
        
        # SVD ì°¨ì› ì¶•ì†Œ (í•„ìš”í•œ ê²½ìš°)
        if hasattr(self.svd, 'components_') and tfidf_vectors.shape[1] > self.vector_size:
            vectors = self.svd.transform(tfidf_vectors)
        else:
            vectors = tfidf_vectors.toarray()
        
        # ë²¡í„° í¬ê¸° ì¡°ì •
        if vectors.shape[1] < self.vector_size:
            # íŒ¨ë”© ì¶”ê°€
            padding = np.zeros((vectors.shape[0], self.vector_size - vectors.shape[1]))
            vectors = np.hstack([vectors, padding])
        elif vectors.shape[1] > self.vector_size:
            # ì˜ë¼ë‚´ê¸°
            vectors = vectors[:, :self.vector_size]
        
        # ì •ê·œí™”
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ ë°©ì§€
        vectors = vectors / norms
        
        return vectors[0] if single_input else vectors
    
    def get_sentence_embedding_dimension(self):
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.vector_size

class InterviewEvaluator:
    """ë©´ì ‘ í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.analyzer = TextAnalyzer()
        
        # í‰ê°€ ê¸°ì¤€ ê°€ì¤‘ì¹˜
        self.evaluation_weights = {
            'content_relevance': 0.3,      # ë‚´ìš© ê´€ë ¨ì„±
            'keyword_match': 0.2,          # í‚¤ì›Œë“œ ë§¤ì¹­
            'sentiment': 0.15,             # ê¸ì •ë„
            'coherence': 0.2,              # ì¼ê´€ì„±
            'length_appropriateness': 0.15  # ë‹µë³€ ê¸¸ì´ ì ì ˆì„±
        }
    
    def evaluate_answer(self, question: InterviewQuestion, answer: str, contexts: List[Dict]) -> Dict:
        """ë‹µë³€ í‰ê°€"""
        if not answer.strip():
            return {
                'overall_score': 0.0,
                'detailed_scores': {},
                'feedback': "ë‹µë³€ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                'strengths': [],
                'improvements': ["ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”."]
            }
        
        # ê° í‰ê°€ í•­ëª©ë³„ ì ìˆ˜ ê³„ì‚°
        scores = {}
        
        # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        scores['keyword_match'] = self.analyzer.calculate_keyword_match(answer, question.context_keywords)
        
        # 2. ê°ì • ë¶„ì„ ì ìˆ˜
        scores['sentiment'] = self.analyzer.analyze_sentiment(answer)
        
        # 3. ì¼ê´€ì„± ì ìˆ˜
        scores['coherence'] = self.analyzer.calculate_coherence_score(answer)
        
        # 4. ë‹µë³€ ê¸¸ì´ ì ì ˆì„±
        word_count = len(answer.split())
        if question.difficulty_level == "easy":
            optimal_range = (50, 150)
        elif question.difficulty_level == "hard":
            optimal_range = (150, 300)
        else:
            optimal_range = (100, 200)
        
        if optimal_range[0] <= word_count <= optimal_range[1]:
            scores['length_appropriateness'] = 1.0
        elif word_count < optimal_range[0]:
            scores['length_appropriateness'] = max(0.3, word_count / optimal_range[0])
        else:
            scores['length_appropriateness'] = max(0.5, optimal_range[1] / word_count)
        
        # 5. ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± (RAG ê¸°ë°˜)
        if contexts:
            context_keywords = []
            for ctx in contexts:
                context_keywords.extend(ctx.get('keywords', []))
            
            scores['content_relevance'] = self.analyzer.calculate_keyword_match(answer, context_keywords)
        else:
            scores['content_relevance'] = 0.5
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = sum(
            scores[key] * self.evaluation_weights[key] 
            for key in scores
        )
        
        # í”¼ë“œë°± ìƒì„±
        feedback, strengths, improvements = self._generate_feedback(scores, word_count, optimal_range)
        
        return {
            'overall_score': round(overall_score, 2),
            'detailed_scores': {k: round(v, 2) for k, v in scores.items()},
            'feedback': feedback,
            'strengths': strengths,
            'improvements': improvements
        }
    
    def _generate_feedback(self, scores: Dict, word_count: int, optimal_range: Tuple[int, int]) -> Tuple[str, List[str], List[str]]:
        """í”¼ë“œë°± ìƒì„±"""
        strengths = []
        improvements = []
        
        # ê°•ì  ë¶„ì„
        if scores['keyword_match'] >= 0.7:
            strengths.append("ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì˜ í™œìš©í•˜ì…¨ìŠµë‹ˆë‹¤.")
        
        if scores['sentiment'] >= 0.7:
            strengths.append("ê¸ì •ì ì´ê³  ì ê·¹ì ì¸ íƒœë„ê°€ ì˜ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.")
        
        if scores['coherence'] >= 0.7:
            strengths.append("ë…¼ë¦¬ì ì´ê³  ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ êµ¬ì¡°ë¥¼ ë³´ì—¬ì£¼ì…¨ìŠµë‹ˆë‹¤.")
        
        if scores['length_appropriateness'] >= 0.8:
            strengths.append("ì ì ˆí•œ ê¸¸ì´ë¡œ ë‹µë³€í•´ ì£¼ì…¨ìŠµë‹ˆë‹¤.")
        
        # ê°œì„ ì  ë¶„ì„
        if scores['keyword_match'] < 0.5:
            improvements.append("ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œì™€ ë” ë°€ì ‘í•œ ì—°ê´€ì„±ì„ ë³´ì—¬ì£¼ì„¸ìš”.")
        
        if scores['sentiment'] < 0.5:
            improvements.append("ë” ê¸ì •ì ì´ê³  ìì‹ ê° ìˆëŠ” í‘œí˜„ì„ ì‚¬ìš©í•´ ë³´ì„¸ìš”.")
        
        if scores['coherence'] < 0.5:
            improvements.append("ë‹µë³€ì˜ ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°œì„ í•˜ê³  ì—°ê²°ì–´ë¥¼ í™œìš©í•´ ë³´ì„¸ìš”.")
        
        if word_count < optimal_range[0]:
            improvements.append(f"ë‹µë³€ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”. (í˜„ì¬ {word_count}ë‹¨ì–´, ê¶Œì¥ {optimal_range[0]}-{optimal_range[1]}ë‹¨ì–´)")
        elif word_count > optimal_range[1]:
            improvements.append(f"ë‹µë³€ì„ ë” ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”. (í˜„ì¬ {word_count}ë‹¨ì–´, ê¶Œì¥ {optimal_range[0]}-{optimal_range[1]}ë‹¨ì–´)")
        
        # ì „ì²´ í”¼ë“œë°±
        overall_score = sum(scores.values()) / len(scores)
        if overall_score >= 0.8:
            feedback = "ìš°ìˆ˜í•œ ë‹µë³€ì…ë‹ˆë‹¤! ğŸ‰"
        elif overall_score >= 0.6:
            feedback = "ì–‘í˜¸í•œ ë‹µë³€ì…ë‹ˆë‹¤. ëª‡ ê°€ì§€ ê°œì„ í•˜ë©´ ë” ì¢‹ê² ìŠµë‹ˆë‹¤. ğŸ‘"
        else:
            feedback = "ë‹µë³€ì„ ë” ë°œì „ì‹œí‚¬ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ğŸ’ª"
        
        return feedback, strengths, improvements

class InterviewManager:
    """ë©´ì ‘ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.rag_service = QdrantRAGService()
        self.evaluator = InterviewEvaluator()
        self.db_path = "lignex1_interview_sessions.db"
        self._init_database()
        
        # ë©´ì ‘ ì§ˆë¬¸ë“¤
        self.questions = self._load_interview_questions()
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ë©´ì ‘ ì„¸ì…˜ í…Œì´ë¸”
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS interview_sessions (
                        session_id TEXT PRIMARY KEY,
                        candidate_name TEXT NOT NULL,
                        position TEXT NOT NULL,
                        start_time TEXT NOT NULL,
                        end_time TEXT,
                        overall_score REAL DEFAULT 0.0,
                        feedback TEXT,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # ë‹µë³€ í…Œì´ë¸”
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS interview_answers (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        session_id TEXT NOT NULL,
                        question_id TEXT NOT NULL,
                        question_text TEXT NOT NULL,
                        answer TEXT NOT NULL,
                        evaluation_score REAL DEFAULT 0.0,
                        keyword_match_score REAL DEFAULT 0.0,
                        sentiment_score REAL DEFAULT 0.0,
                        coherence_score REAL DEFAULT 0.0,
                        answer_length INTEGER DEFAULT 0,
                        timestamp TEXT NOT NULL,
                        context_used TEXT,
                        FOREIGN KEY (session_id) REFERENCES interview_sessions (session_id)
                    )
                ''')
                
                conn.commit()
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_interview_questions(self) -> List[InterviewQuestion]:
        """ë©´ì ‘ ì§ˆë¬¸ ë¡œë“œ"""
        return [
            InterviewQuestion(
                id="culture_open",
                text="LIGë„¥ìŠ¤ì›ì˜ í•µì‹¬ê°€ì¹˜ ì¤‘ 'ê°œë°©(OPEN)'ì— ëŒ€í•´ ì–´ë–»ê²Œ ì´í•´í•˜ê³  ê³„ì‹œë©°, ë³¸ì¸ì˜ ê²½í—˜ ì¤‘ ê°œë°©ì  ì‚¬ê³ ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•œ ì‚¬ë¡€ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                category="í•µì‹¬ê°€ì¹˜ ì´í•´ë„",
                context_keywords=["ê¸°ì—…ë¬¸í™”", "í•µì‹¬ê°€ì¹˜", "OPEN", "ê°œë°©", "ì¡°ì§ë¬¸í™”", "ì†Œí†µ", "í˜‘ë ¥"],
                follow_up_questions=[
                    "ê·¸ ìƒí™©ì—ì„œ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ ì˜ê²¬ì„ ì–´ë–»ê²Œ ìˆ˜ìš©í•˜ì…¨ë‚˜ìš”?",
                    "ê°œë°©ì  ì‚¬ê³ ê°€ ì–´ë ¤ì› ë˜ ìˆœê°„ì€ ì–¸ì œì˜€ê³ , ì–´ë–»ê²Œ ê·¹ë³µí•˜ì…¨ë‚˜ìš”?",
                    "íŒ€ ë‚´ ê°ˆë“± ìƒí™©ì—ì„œ ê°œë°©ì  íƒœë„ë¡œ í•´ê²°í•œ ê²½í—˜ì´ ìˆë‚˜ìš”?"
                ],
                difficulty_level="medium",
                expected_duration=5,
                evaluation_criteria=["í•µì‹¬ê°€ì¹˜ ì´í•´ë„", "ì‹¤ì œ ê²½í—˜ êµ¬ì²´ì„±", "ë¬¸ì œí•´ê²° ëŠ¥ë ¥", "ì†Œí†µ ëŠ¥ë ¥"]
            ),
            InterviewQuestion(
                id="culture_positive",
                text="LIGë„¥ìŠ¤ì›ì˜ ë˜ ë‹¤ë¥¸ í•µì‹¬ê°€ì¹˜ì¸ 'ê¸ì •(POSITIVE)'ì— ëŒ€í•œ ë³¸ì¸ì˜ í•´ì„ì„ ë“¤ë ¤ì£¼ì‹œê³ , ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ ê¸ì •ì  íƒœë„ë¥¼ ìœ ì§€í•˜ë©° ì„±ê³¼ë¥¼ ì°½ì¶œí–ˆë˜ ê²½í—˜ì„ ìƒì„¸íˆ ê³µìœ í•´ ì£¼ì„¸ìš”.",
                category="ê¸ì •ì  ì‚¬ê³ ",
                context_keywords=["POSITIVE", "ê¸ì •", "ë„ì „", "ê·¹ë³µ", "ì—´ì •", "ì„±ê³¼", "ë™ê¸°ë¶€ì—¬"],
                follow_up_questions=[
                    "ì‹¤íŒ¨ë¥¼ ì„±ê³µì˜ ë°‘ê±°ë¦„ìœ¼ë¡œ í™œìš©í•œ êµ¬ì²´ì ì¸ ì‚¬ë¡€ê°€ ìˆë‚˜ìš”?",
                    "íŒ€ì›ë“¤ì˜ ì‚¬ê¸°ê°€ ë‚®ì„ ë•Œ ì–´ë–»ê²Œ ë™ê¸°ë¶€ì—¬ í•˜ì‹œê² ë‚˜ìš”?",
                    "ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë†’ì€ ìƒí™©ì—ì„œ ê¸ì •ì„±ì„ ìœ ì§€í•˜ëŠ” ë³¸ì¸ë§Œì˜ ë°©ë²•ì€?"
                ],
                difficulty_level="medium",
                expected_duration=5,
                evaluation_criteria=["ê¸ì •ì  ì‚¬ê³ ", "ì—­ê²½ ê·¹ë³µ ëŠ¥ë ¥", "ë¦¬ë”ì‹­", "íŒ€ì›Œí¬"]
            ),
            InterviewQuestion(
                id="mission_defense",
                text="ë°©ìœ„ì‚°ì—…ì€ êµ­ê°€ ì•ˆë³´ì™€ ì§ê²°ë˜ëŠ” ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë§‰ì¤‘í•œ ì±…ì„ê°ì´ ê°œì¸ì˜ ì—…ë¬´ ìˆ˜í–‰ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë¼ê³  ìƒê°í•˜ì‹œë©°, êµ­ê°€ë¥¼ ìœ„í•œ ì¼ì´ë¼ëŠ” ì‚¬ëª…ê°ì„ ì–´ë–»ê²Œ ì‹¤ë¬´ì— ì ìš©í•˜ì‹¤ ê³„íšì¸ê°€ìš”?",
                category="ì±…ì„ê°ê³¼ ì‚¬ëª…ê°",
                context_keywords=["ë°©ìœ„ì‚°ì—…", "êµ­ë°©", "ì•ˆë³´", "ì±…ì„ê°", "ì‚¬ëª…ê°", "êµ­ê°€", "ë³´ì•ˆ"],
                follow_up_questions=[
                    "ë³´ì•ˆê³¼ ê¸°ë°€ ìœ ì§€ì— ëŒ€í•œ ë³¸ì¸ì˜ ê°ì˜¤ëŠ” ì–´ë– í•œì§€ìš”?",
                    "ì••ë°•ê°ê³¼ ì±…ì„ê°ì„ ê±´ì„¤ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì€?",
                    "ê°œì¸ì˜ ì„±ì¥ê³¼ êµ­ê°€ì  ì‚¬ëª… ì‚¬ì´ì˜ ê· í˜•ì„ ì–´ë–»ê²Œ ë§ì¶”ì‹œê² ë‚˜ìš”?"
                ],
                difficulty_level="hard",
                expected_duration=6,
                evaluation_criteria=["ì‚¬ëª…ê°", "ì±…ì„ê°", "ë³´ì•ˆ ì˜ì‹", "ì••ë°•ê° ê´€ë¦¬"]
            ),
            InterviewQuestion(
                id="growth_rd",
                text="LIGë„¥ìŠ¤ì›ì€ ì „ì²´ ì§ì›ì˜ 50%ê°€ R&D ì¸ë ¥ìœ¼ë¡œ êµ¬ì„±ëœ ê¸°ìˆ  ì¤‘ì‹¬ ì¡°ì§ì…ë‹ˆë‹¤. ê¸‰ë³€í•˜ëŠ” ê¸°ìˆ  í™˜ê²½ì—ì„œ ì§€ì†ì ì¸ í•™ìŠµê³¼ í˜ì‹ ì´ í•„ìš”í•œ ì¡°ì§ì—ì„œ ë³¸ì¸ë§Œì˜ ì„±ì¥ ì „ëµê³¼ ê¸°ì—¬ ë°©ì•ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
                category="í•™ìŠµ ì§€í–¥ì„±ê³¼ í˜ì‹ ",
                context_keywords=["R&D", "ì—°êµ¬ê°œë°œ", "ê¸°ìˆ ê°œë°œ", "í˜ì‹ ", "í•™ìŠµ", "ì„±ì¥", "ì²¨ë‹¨ê¸°ìˆ "],
                follow_up_questions=[
                    "ìƒˆë¡œìš´ ê¸°ìˆ  íŠ¸ë Œë“œë¥¼ ìŠµë“í•˜ê³  ì ìš©í•˜ëŠ” ë³¸ì¸ë§Œì˜ ë°©ë²•ë¡ ì€?",
                    "ë™ë£Œë“¤ê³¼ ì§€ì‹ì„ ê³µìœ í•˜ê³  ì‹œë„ˆì§€ë¥¼ ì°½ì¶œí•˜ëŠ” ë°©ì•ˆì€?",
                    "ê¸°ìˆ ì  ë‚œì œì— ë¶€ë”ªí˜”ì„ ë•Œì˜ ë¬¸ì œí•´ê²° ì ‘ê·¼ë²•ì€?"
                ],
                difficulty_level="hard",
                expected_duration=6,
                evaluation_criteria=["í•™ìŠµ ëŠ¥ë ¥", "ê¸°ìˆ  ì´í•´ë„", "í˜ì‹  ì‚¬ê³ ", "ì§€ì‹ ê³µìœ "]
            ),
            InterviewQuestion(
                id="vision_future",
                text="LIGë„¥ìŠ¤ì›ì—ì„œ 10ë…„ í›„ ë³¸ì¸ì˜ ëª¨ìŠµì„ êµ¬ì²´ì ìœ¼ë¡œ ê·¸ë ¤ë³´ì‹œê³ , ê°œì¸ì˜ ì„±ì¥ê³¼ íšŒì‚¬ ë°œì „ì— ê¸°ì—¬í•˜ê¸° ìœ„í•œ ë‹¨ê³„ì  ê³„íšê³¼ ë¹„ì „ì„ ìƒì„¸íˆ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                category="ë¹„ì „ê³¼ ì„±ì¥ ì˜ì§€",
                context_keywords=["ë¯¸ë˜", "ë¹„ì „", "ì„±ì¥", "ëª©í‘œ", "ê³„íš", "ë°œì „", "ì»¤ë¦¬ì–´"],
                follow_up_questions=[
                    "íšŒì‚¬ì˜ ë¯¸ë˜ ì„±ì¥ ë™ë ¥ì— ì–´ë–¤ ê¸°ì—¬ë¥¼ í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                    "ê°œì¸ ì„±ì¥ê³¼ ì¡°ì§ ë°œì „ì˜ ê· í˜•ì„ ì–´ë–»ê²Œ ë§ì¶”ì‹œê² ë‚˜ìš”?",
                    "ì˜ˆìƒë˜ëŠ” ì–´ë ¤ì›€ê³¼ ì´ë¥¼ ê·¹ë³µí•˜ëŠ” ì „ëµì€?"
                ],
                difficulty_level="medium",
                expected_duration=5,
                evaluation_criteria=["ë¹„ì „ ì„¤ì •", "ëª©í‘œ êµ¬ì²´ì„±", "ì‹¤í–‰ ê³„íš", "ì„±ì¥ ì˜ì§€"]
            ),
            InterviewQuestion(
                id="teamwork_collaboration",
                text="ë‹¤ì–‘í•œ ë°°ê²½ì„ ê°€ì§„ íŒ€ì›ë“¤ê³¼ í˜‘ì—…í•˜ì—¬ í° ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆë˜ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì‹œê³ , LIGë„¥ìŠ¤ì›ì—ì„œë„ íš¨ê³¼ì ì¸ í˜‘ì—…ì„ ìœ„í•´ ì–´ë–¤ ì—­í• ì„ í•˜ì‹¤ ê²ƒì¸ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                category="íŒ€ì›Œí¬ì™€ í˜‘ì—…",
                context_keywords=["íŒ€ì›Œí¬", "í˜‘ì—…", "ì†Œí†µ", "ë¦¬ë”ì‹­", "ê°ˆë“±í•´ê²°", "ì„±ê³¼", "í”„ë¡œì íŠ¸"],
                follow_up_questions=[
                    "íŒ€ ë‚´ ê°ˆë“± ìƒí™©ì„ í•´ê²°í•œ ê²½í—˜ì´ ìˆë‹¤ë©´?",
                    "ë³¸ì¸ì˜ í˜‘ì—… ìŠ¤íƒ€ì¼ê³¼ ê°•ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "ë‹¤ì–‘í•œ ë¶€ì„œì™€ì˜ í˜‘ì—…ì—ì„œ ì¤‘ìš”í•œ ìš”ì†ŒëŠ”?"
                ],
                difficulty_level="medium",
                expected_duration=5,
                evaluation_criteria=["í˜‘ì—… ëŠ¥ë ¥", "ì†Œí†µ ëŠ¥ë ¥", "ê°ˆë“± í•´ê²°", "ë¦¬ë”ì‹­"]
            ),
            InterviewQuestion(
                id="problem_solving",
                text="ë³µì¡í•˜ê³  ì–´ë ¤ìš´ ë¬¸ì œ ìƒí™©ì—ì„œ ì°½ì˜ì ì´ê³  ì²´ê³„ì ì¸ ì ‘ê·¼ìœ¼ë¡œ í•´ê²°ì±…ì„ ì°¾ì•„ë‚¸ ê²½í—˜ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ì‹œê³ , LIGë„¥ìŠ¤ì›ì—ì„œ ë§ˆì£¼í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ì /ì—…ë¬´ì  ë„ì „ì— ì–´ë–»ê²Œ ëŒ€ì‘í•˜ì‹¤ ê²ƒì¸ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.",
                category="ë¬¸ì œí•´ê²° ëŠ¥ë ¥",
                context_keywords=["ë¬¸ì œí•´ê²°", "ì°½ì˜ì„±", "ë¶„ì„", "ì²´ê³„ì ", "ë…¼ë¦¬ì ", "í˜ì‹ ", "ë„ì „"],
                follow_up_questions=[
                    "ë¬¸ì œ ë¶„ì„ê³¼ í•´ê²° ê³¼ì •ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ìš”ì†ŒëŠ”?",
                    "ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í•´ê²°ë˜ì§€ ì•ŠëŠ” ë¬¸ì œì— ì–´ë–»ê²Œ ì ‘ê·¼í•˜ì‹œë‚˜ìš”?",
                    "ë¬¸ì œí•´ê²° ê³¼ì •ì—ì„œ ë‹¤ë¥¸ ì‚¬ëŒë“¤ê³¼ ì–´ë–»ê²Œ í˜‘ë ¥í•˜ì‹œë‚˜ìš”?"
                ],
                difficulty_level="hard",
                expected_duration=6,
                evaluation_criteria=["ë…¼ë¦¬ì  ì‚¬ê³ ", "ì°½ì˜ì„±", "ì²´ê³„ì  ì ‘ê·¼", "ì‹¤í–‰ë ¥"]
            ),
            InterviewQuestion(
                id="communication_leadership",
                text="ë‹¤ë¥¸ ì‚¬ëŒë“¤ì„ ì„¤ë“í•˜ê³  ì´ëŒì–´ì•¼ í–ˆë˜ ì–´ë ¤ìš´ ìƒí™©ì—ì„œ, ì–´ë–¤ ì†Œí†µ ì „ëµê³¼ ë¦¬ë”ì‹­ì„ ë°œíœ˜í•˜ì—¬ ì„±ê³µì ì¸ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚¸ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ê³µìœ í•´ ì£¼ì„¸ìš”.",
                category="ì†Œí†µê³¼ ë¦¬ë”ì‹­",
                context_keywords=["ì†Œí†µ", "ë¦¬ë”ì‹­", "ì„¤ë“", "ì˜í–¥ë ¥", "ë™ê¸°ë¶€ì—¬", "ë³€í™”ê´€ë¦¬", "ì„±ê³¼"],
                follow_up_questions=[
                    "ë°˜ëŒ€ ì˜ê²¬ì´ ë§ì€ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ í•©ì˜ë¥¼ ì´ëŒì–´ë‚´ì‹œë‚˜ìš”?",
                    "ë³¸ì¸ì˜ ë¦¬ë”ì‹­ ìŠ¤íƒ€ì¼ê³¼ ì² í•™ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "íŒ€ì›ë“¤ì˜ ë‹¤ì–‘í•œ ì˜ê²¬ì„ ì¡°ìœ¨í•˜ëŠ” ë°©ë²•ì€?"
                ],
                difficulty_level="medium",
                expected_duration=5,
                evaluation_criteria=["ì†Œí†µ ëŠ¥ë ¥", "ë¦¬ë”ì‹­", "ì„¤ë“ë ¥", "ê°ˆë“± ì¡°ì •"]
            )
        ]
        
    def create_session(self, candidate_name: str, position: str) -> str:
        """ë©´ì ‘ ì„¸ì…˜ ìƒì„±"""
        try:
            session_id = hashlib.md5(f"{candidate_name}_{position}_{datetime.now().isoformat()}".encode()).hexdigest()
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT INTO interview_sessions 
                    (session_id, candidate_name, position, start_time)
                    VALUES (?, ?, ?, ?)
                ''', (session_id, candidate_name, position, datetime.now().isoformat()))
                
                conn.commit()
                logger.info(f"ë©´ì ‘ ì„¸ì…˜ ìƒì„±: {session_id}")
                return session_id
                
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            st.error(f"ì„¸ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None

        
    def save_answer(self, session_id: str, question: InterviewQuestion, answer: str, contexts: List[Dict]) -> Dict:
        """ë‹µë³€ ì €ì¥ ë° í‰ê°€"""
        if not answer or not answer.strip():
            return {'error': 'ë‹µë³€ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.'}
        
        # ì¶”ê°€ ë¡œê·¸ë¡œ ë””ë²„ê¹…
        logger.info(f"ë‹µë³€ ì €ì¥ ì‹œë„: session_id={session_id}, answer_length={len(answer)}")
        
        # ë‹µë³€ í‰ê°€
        evaluation = self.evaluator.evaluate_answer(question, answer, contexts)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT INTO interview_answers 
                    (session_id, question_id, question_text, answer, evaluation_score,
                     keyword_match_score, sentiment_score, coherence_score, answer_length,
                     timestamp, context_used)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    session_id,
                    question.id,
                    question.text,
                    answer,
                    evaluation['overall_score'],
                    evaluation['detailed_scores'].get('keyword_match', 0),
                    evaluation['detailed_scores'].get('sentiment', 0),
                    evaluation['detailed_scores'].get('coherence', 0),
                    len(answer.split()),
                    datetime.now().isoformat(),
                    json.dumps(contexts, ensure_ascii=False)
                ))
                
                conn.commit()
                logger.info(f"ë‹µë³€ ì €ì¥ ì™„ë£Œ: {session_id} - {question.id}")
                
        except Exception as e:
            logger.error(f"ë‹µë³€ ì €ì¥ ì‹¤íŒ¨: {e}")
            return {'error': f'ì €ì¥ ì‹¤íŒ¨: {str(e)}'}
        
        return evaluation
    
    def complete_session(self, session_id: str) -> Dict:
        """ë©´ì ‘ ì„¸ì…˜ ì™„ë£Œ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ì „ì²´ ì ìˆ˜ ê³„ì‚°
                cursor = conn.execute('''
                    SELECT AVG(evaluation_score), COUNT(*)
                    FROM interview_answers
                    WHERE session_id = ?
                ''', (session_id,))
                
                result = cursor.fetchone()
                overall_score = result[0] if result[0] else 0.0
                answer_count = result[1]
                
                # ì„¸ì…˜ ì—…ë°ì´íŠ¸
                conn.execute('''
                    UPDATE interview_sessions 
                    SET end_time = ?, overall_score = ?
                    WHERE session_id = ?
                ''', (datetime.now().isoformat(), overall_score, session_id))
                
                conn.commit()
                
                # ìƒì„¸ ë¶„ì„ ë°ì´í„° ì¡°íšŒ
                cursor = conn.execute('''
                    SELECT question_id, evaluation_score, keyword_match_score, 
                           sentiment_score, coherence_score, answer_length
                    FROM interview_answers
                    WHERE session_id = ?
                    ORDER BY timestamp
                ''', (session_id,))
                
                answers_data = cursor.fetchall()
                
                return {
                    'session_id': session_id,
                    'overall_score': round(overall_score, 2),
                    'answer_count': answer_count,
                    'answers_data': answers_data,
                    'completed_at': datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def get_session_analytics(self, session_id: str) -> Dict:
        """ì„¸ì…˜ ë¶„ì„ ë°ì´í„°"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ê¸°ë³¸ ì •ë³´
                cursor = conn.execute('''
                    SELECT candidate_name, position, start_time, end_time, overall_score
                    FROM interview_sessions
                    WHERE session_id = ?
                ''', (session_id,))
                
                session_info = cursor.fetchone()
                if not session_info:
                    return {'error': 'ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
                
                # ë‹µë³€ë³„ ìƒì„¸ ë°ì´í„°
                cursor = conn.execute('''
                    SELECT question_id, question_text, answer, evaluation_score,
                           keyword_match_score, sentiment_score, coherence_score,
                           answer_length, timestamp, context_used
                    FROM interview_answers
                    WHERE session_id = ?
                    ORDER BY timestamp
                ''', (session_id,))
                
                answers = cursor.fetchall()
                
                # í†µê³„ ê³„ì‚°
                if answers:
                    scores = [a[3] for a in answers]  # evaluation_score
                    avg_score = np.mean(scores)
                    score_std = np.std(scores)
                    
                    sentiment_scores = [a[5] for a in answers]
                    avg_sentiment = np.mean(sentiment_scores)
                    
                    coherence_scores = [a[6] for a in answers]
                    avg_coherence = np.mean(coherence_scores)
                    
                    answer_lengths = [a[7] for a in answers]
                    avg_length = np.mean(answer_lengths)
                else:
                    avg_score = score_std = avg_sentiment = avg_coherence = avg_length = 0
                
                return {
                    'session_info': {
                        'candidate_name': session_info[0],
                        'position': session_info[1],
                        'start_time': session_info[2],
                        'end_time': session_info[3],
                        'overall_score': session_info[4]
                    },
                    'answers': [
                        {
                            'question_id': a[0],
                            'question_text': a[1],
                            'answer': a[2],
                            'evaluation_score': a[3],
                            'keyword_match_score': a[4],
                            'sentiment_score': a[5],
                            'coherence_score': a[6],
                            'answer_length': a[7],
                            'timestamp': a[8],
                            'context_used': json.loads(a[9]) if a[9] else []
                        } for a in answers
                    ],
                    'statistics': {
                        'average_score': round(avg_score, 2),
                        'score_deviation': round(score_std, 2),
                        'average_sentiment': round(avg_sentiment, 2),
                        'average_coherence': round(avg_coherence, 2),
                        'average_answer_length': round(avg_length, 1),
                        'total_answers': len(answers)
                    }
                }
                
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def get_all_sessions(self) -> List[Dict]:
        """ëª¨ë“  ì„¸ì…˜ ëª©ë¡"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('''
                    SELECT session_id, candidate_name, position, 
                        start_time, end_time, overall_score
                    FROM interview_sessions
                    ORDER BY start_time DESC
                ''')
                
                sessions = cursor.fetchall()
                
                # ì§„í–‰ì¤‘ì¸ ì„¸ì…˜ì˜ ë‹µë³€ ìˆ˜ë„ ì¡°íšŒ
                result = []
                for s in sessions:
                    session_data = {
                        'session_id': s[0],
                        'candidate_name': s[1],
                        'position': s[2],
                        'start_time': s[3],
                        'end_time': s[4],
                        'overall_score': s[5] or 0.0,
                        'status': 'completed' if s[4] else 'in_progress'
                    }
                    
                    # ì§„í–‰ì¤‘ì¸ ì„¸ì…˜ì˜ ë‹µë³€ ìˆ˜ í™•ì¸
                    if not s[4]:  # end_timeì´ ì—†ìœ¼ë©´ ì§„í–‰ì¤‘
                        cursor2 = conn.execute('''
                            SELECT COUNT(*) FROM interview_answers
                            WHERE session_id = ?
                        ''', (s[0],))
                        answer_count = cursor2.fetchone()[0]
                        session_data['answer_count'] = answer_count
                    
                    result.append(session_data)
                
                return result
                
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

def show_interview_records_page():
    """ë©´ì ‘ ê¸°ë¡ í˜ì´ì§€"""
    st.markdown('<h2 class="sub-header">ğŸ“‹ ë©´ì ‘ ê¸°ë¡ ê´€ë¦¬</h2>', unsafe_allow_html=True)
    
    # ì „ì²´ ì„¸ì…˜ ì¡°íšŒ
    sessions = st.session_state.interview_manager.get_all_sessions()
    
    if not sessions:
        st.info("ğŸ“‹ ì•„ì§ ì§„í–‰ëœ ë©´ì ‘ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•„í„°ë§ ì˜µì…˜
    col1, col2, col3 = st.columns(3)
    
    with col1:
        status_filter = st.selectbox(
            "ìƒíƒœ í•„í„°",
            ["ì „ì²´", "ì§„í–‰ì¤‘", "ì™„ë£Œ"]
        )
    
    with col2:
        # ì§ë¬´ë³„ í•„í„°
        positions = list(set([s['position'] for s in sessions]))
        position_filter = st.selectbox(
            "ì§ë¬´ í•„í„°",
            ["ì „ì²´"] + positions
        )
    
    with col3:
        # ë‚ ì§œ ë²”ìœ„ í•„í„°
        date_filter = st.selectbox(
            "ê¸°ê°„ í•„í„°",
            ["ì „ì²´", "ì˜¤ëŠ˜", "ìµœê·¼ 7ì¼", "ìµœê·¼ 30ì¼"]
        )
    
    # í•„í„°ë§ ì ìš©
    filtered_sessions = sessions.copy()
    
    if status_filter != "ì „ì²´":
        status_map = {"ì§„í–‰ì¤‘": "in_progress", "ì™„ë£Œ": "completed"}
        filtered_sessions = [s for s in filtered_sessions if s['status'] == status_map[status_filter]]
    
    if position_filter != "ì „ì²´":
        filtered_sessions = [s for s in filtered_sessions if s['position'] == position_filter]
    
    if date_filter != "ì „ì²´":
        from datetime import datetime, timedelta
        now = datetime.now()
        
        if date_filter == "ì˜¤ëŠ˜":
            cutoff = now.replace(hour=0, minute=0, second=0, microsecond=0)
        elif date_filter == "ìµœê·¼ 7ì¼":
            cutoff = now - timedelta(days=7)
        elif date_filter == "ìµœê·¼ 30ì¼":
            cutoff = now - timedelta(days=30)
        
        filtered_sessions = [
            s for s in filtered_sessions 
            if datetime.fromisoformat(s['start_time']) >= cutoff
        ]
    
    # í†µê³„ ìš”ì•½
    st.markdown("### ğŸ“Š ìš”ì•½ í†µê³„")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ì „ì²´ ë©´ì ‘", len(sessions))
    
    with col2:
        completed = len([s for s in sessions if s['status'] == 'completed'])
        st.metric("ì™„ë£Œëœ ë©´ì ‘", completed)
    
    with col3:
        in_progress = len([s for s in sessions if s['status'] == 'in_progress'])
        st.metric("ì§„í–‰ì¤‘ì¸ ë©´ì ‘", in_progress)
    
    with col4:
        if completed > 0:
            avg_score = np.mean([s['overall_score'] for s in sessions if s['overall_score'] > 0])
            st.metric("í‰ê·  ì ìˆ˜", f"{avg_score:.2f}")
        else:
            st.metric("í‰ê·  ì ìˆ˜", "N/A")
    
    # ì„¸ì…˜ ëª©ë¡ í‘œì‹œ
    st.markdown("### ğŸ“‹ ë©´ì ‘ ëª©ë¡")
    
    if not filtered_sessions:
        st.info("ì„ íƒí•œ ì¡°ê±´ì— ë§ëŠ” ë©´ì ‘ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df_sessions = pd.DataFrame(filtered_sessions)
    df_sessions['start_date'] = pd.to_datetime(df_sessions['start_time']).dt.strftime('%Y-%m-%d %H:%M')
    df_sessions['status_kr'] = df_sessions['status'].map({'completed': 'ì™„ë£Œ', 'in_progress': 'ì§„í–‰ì¤‘'})

    # ì„¸ì…˜ ìƒì„¸ ì •ë³´ì—ë„ í•œêµ­ì–´ ìƒíƒœ ì¶”ê°€
    for session in filtered_sessions:
        session['status_kr'] = 'ì™„ë£Œ' if session['status'] == 'completed' else 'ì§„í–‰ì¤‘'
    
    # í‘œ í‘œì‹œ
    display_df = df_sessions[['candidate_name', 'position', 'start_date', 'status_kr', 'overall_score']].copy()
    display_df.columns = ['ë©´ì ‘ì', 'ì§€ì›ì§ë¬´', 'ì‹œì‘ì‹œê°„', 'ìƒíƒœ', 'ì ìˆ˜']
    display_df['ì ìˆ˜'] = display_df['ì ìˆ˜'].apply(lambda x: f"{x:.2f}" if x > 0 else "N/A")
    
    st.dataframe(display_df, use_container_width=True)
    
    # ê°œë³„ ì„¸ì…˜ ìƒì„¸ ë³´ê¸°
    st.markdown("### ğŸ” ìƒì„¸ ë³´ê¸°")
    
    session_options = {
        f"{s['candidate_name']} - {s['position']} ({s['start_time'][:10]})": s['session_id'] 
        for s in filtered_sessions
    }
    
    if session_options:
        selected_session_label = st.selectbox(
            "ìƒì„¸íˆ ë³¼ ë©´ì ‘ ì„ íƒ",
            ["ì„ íƒí•˜ì„¸ìš”..."] + list(session_options.keys())
        )
        
        if selected_session_label != "ì„ íƒí•˜ì„¸ìš”...":
            selected_session_id = session_options[selected_session_label]
            
            # ì„¸ì…˜ ìƒì„¸ ì •ë³´
            session_detail = next(s for s in filtered_sessions if s['session_id'] == selected_session_id)
            
            with st.expander("ğŸ“‹ ì„¸ì…˜ ì •ë³´", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**ì„¸ì…˜ ID**: {session_detail['session_id'][:16]}...")
                    st.write(f"**ë©´ì ‘ì**: {session_detail['candidate_name']}")
                    st.write(f"**ì§€ì› ì§ë¬´**: {session_detail['position']}")
                
                with col2:
                    st.write(f"**ì‹œì‘ ì‹œê°„**: {session_detail['start_time'][:19]}")
                    if session_detail['end_time']:
                        st.write(f"**ì¢…ë£Œ ì‹œê°„**: {session_detail['end_time'][:19]}")
                    st.write(f"**ìƒíƒœ**: {session_detail['status_kr']}")
                    if session_detail['overall_score'] > 0:
                        st.write(f"**ì¢…í•© ì ìˆ˜**: {session_detail['overall_score']:.2f}")
            
            # ë‹µë³€ ëª©ë¡ (ì™„ë£Œëœ ì„¸ì…˜ë§Œ)
            if session_detail['status'] == 'completed':
                analytics = st.session_state.interview_manager.get_session_analytics(selected_session_id)
                
                if 'error' not in analytics:
                    answers = analytics['answers']
                    
                    st.markdown("#### ğŸ“ ë‹µë³€ ëª©ë¡")
                    
                    for i, answer in enumerate(answers, 1):
                        with st.expander(f"ì§ˆë¬¸ {i} (ì ìˆ˜: {answer['evaluation_score']:.2f})", expanded=False):
                            st.markdown(f"**ì§ˆë¬¸**: {answer['question_text']}")
                            st.markdown(f"**ë‹µë³€**: {answer['answer'][:300]}...")
                            st.markdown(f"**ë‹µë³€ ê¸¸ì´**: {answer['answer_length']}ë‹¨ì–´")
                            st.markdown(f"**ì‘ì„± ì‹œê°„**: {answer['timestamp'][:19]}")

def show_system_status_page():
    """ì‹œìŠ¤í…œ ìƒíƒœ í˜ì´ì§€"""
    st.markdown('<h2 class="sub-header">ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ</h2>', unsafe_allow_html=True)
    
    # RAG ì‹œìŠ¤í…œ ìƒíƒœ
    st.markdown("### ğŸ¤– RAG ì‹œìŠ¤í…œ ìƒíƒœ")
    
    rag_service = st.session_state.interview_manager.rag_service
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ”— ì—°ê²° ìƒíƒœ")
        
        if rag_service.is_available():
            st.success("âœ… RAG ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™")
            
            # Qdrant ì •ë³´
            try:
                collections = rag_service.qdrant_client.get_collections().collections
                collection_exists = any(c.name == rag_service.collection_name for c in collections)
                
                if collection_exists:
                    st.success(f"âœ… ì»¬ë ‰ì…˜ '{rag_service.collection_name}' ì¡´ì¬")
                    
                    # ì»¬ë ‰ì…˜ ìƒì„¸ ì •ë³´
                    try:
                        # ì»¬ë ‰ì…˜ ê¸°ë³¸ ì •ë³´ë§Œ ì¡°íšŒ (ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ íšŒí”¼)
                        collection_info = rag_service.qdrant_client.get_collection(rag_service.collection_name)
                        
                        # ì•ˆì „í•˜ê²Œ ì •ë³´ ì¶”ì¶œ
                        if hasattr(collection_info, 'vectors_count'):
                            vector_count = collection_info.vectors_count
                        else:
                            vector_count = "ì•Œ ìˆ˜ ì—†ìŒ"
                        
                        st.info(f"ğŸ“Š ì»¬ë ‰ì…˜: {rag_service.collection_name}")
                        st.info(f"ğŸ“Š ë²¡í„° ë°ì´í„°: {vector_count}")
                        st.success("âœ… ì»¬ë ‰ì…˜ ì •ìƒ ì‘ë™")
                        
                    except Exception as detail_error:
                        # ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ í‘œì‹œ
                        st.success(f"âœ… ì»¬ë ‰ì…˜ '{rag_service.collection_name}' ì¡´ì¬")
                        st.info("ğŸ“Š ìƒì„¸ ì •ë³´ ì¡°íšŒ ë¶ˆê°€ (ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ)")
                        logger.warning(f"Collection detail query failed: {detail_error}")


                else:
                    st.error(f"âŒ ì»¬ë ‰ì…˜ '{rag_service.collection_name}' ì—†ìŒ")
                    
            except Exception as e:
                st.error(f"âŒ Qdrant ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        else:
            st.error("âŒ RAG ì‹œìŠ¤í…œ ì—°ê²° ì‹¤íŒ¨")
            st.warning("Qdrant ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
    
    with col2:
        st.markdown("#### ğŸ§  ì„ë² ë”© ëª¨ë¸")
        
        if rag_service.embedding_model:
            st.success("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œë¨")
            st.info(f"ğŸ“ ë²¡í„° ì°¨ì›: {rag_service.vector_size}")
            
            # í…ŒìŠ¤íŠ¸ ì„ë² ë”©
            if st.button("ğŸ§ª ì„ë² ë”© í…ŒìŠ¤íŠ¸"):
                test_text = "LIGë„¥ìŠ¤ì› ê¸°ì—…ë¬¸í™”"
                try:
                    with st.spinner("ì„ë² ë”© ìƒì„± ì¤‘..."):
                        embedding = rag_service.embedding_model.encode(test_text)
                    st.success(f"âœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ (ë²¡í„° ê¸¸ì´: {len(embedding)})")
                except Exception as e:
                    st.error(f"âŒ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        else:
            st.error("âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ
    st.markdown("---")
    st.markdown("### ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ")
    
    try:
        with sqlite3.connect(st.session_state.interview_manager.db_path) as conn:
            # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor = conn.execute("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name IN ('interview_sessions', 'interview_answers')
            """)
            tables = [row[0] for row in cursor.fetchall()]
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### ğŸ“‹ í…Œì´ë¸” ìƒíƒœ")
                if 'interview_sessions' in tables:
                    st.success("âœ… interview_sessions í…Œì´ë¸” ì¡´ì¬")
                else:
                    st.error("âŒ interview_sessions í…Œì´ë¸” ì—†ìŒ")
                
                if 'interview_answers' in tables:
                    st.success("âœ… interview_answers í…Œì´ë¸” ì¡´ì¬")
                else:
                    st.error("âŒ interview_answers í…Œì´ë¸” ì—†ìŒ")
            
            with col2:
                st.markdown("#### ğŸ“Š ë°ì´í„° í†µê³„")
                
                # ì„¸ì…˜ ìˆ˜
                cursor = conn.execute("SELECT COUNT(*) FROM interview_sessions")
                session_count = cursor.fetchone()[0]
                st.metric("ì´ ë©´ì ‘ ì„¸ì…˜", session_count)
                
                # ë‹µë³€ ìˆ˜
                cursor = conn.execute("SELECT COUNT(*) FROM interview_answers")
                answer_count = cursor.fetchone()[0]
                st.metric("ì´ ë‹µë³€ ìˆ˜", answer_count)
                
                # ì™„ë£Œëœ ì„¸ì…˜ ìˆ˜
                cursor = conn.execute("SELECT COUNT(*) FROM interview_sessions WHERE end_time IS NOT NULL")
                completed_count = cursor.fetchone()[0]
                st.metric("ì™„ë£Œëœ ë©´ì ‘", completed_count)
                
    except Exception as e:
        st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    st.markdown("---")
    st.markdown("### âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ”§ RAG ì„¤ì •")
        st.code(f"""
Qdrant Host: {rag_service.qdrant_client.host if rag_service.qdrant_client else 'N/A'}
Qdrant Port: {rag_service.qdrant_client.port if rag_service.qdrant_client else 'N/A'}
Collection: {rag_service.collection_name}
Vector Size: {rag_service.vector_size if rag_service.embedding_model else 'N/A'}
        """)
    
    with col2:
        st.markdown("#### ğŸ“ íŒŒì¼ ê²½ë¡œ")
        st.code(f"""
Database: {st.session_state.interview_manager.db_path}
Log File: lignex1_interview.log
        """)
    
    # ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    st.markdown("---")
    st.markdown("### ğŸ§ª ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    if st.button("ğŸ” ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸", use_container_width=True):
        with st.spinner("ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘..."):
            test_results = []
            
            # RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
            if rag_service.is_available():
                try:
                    contexts = rag_service.search_context("LIGë„¥ìŠ¤ì›", limit=1)
                    if contexts:
                        test_results.append("âœ… RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                    else:
                        test_results.append("âš ï¸ RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë°ì´í„° ë¶€ì¡±)")
                except Exception as e:
                    test_results.append(f"âŒ RAG ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            else:
                test_results.append("âŒ RAG ì‹œìŠ¤í…œ ì—°ê²° ì•ˆë¨")
            
            # ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸
            try:
                test_session_id = st.session_state.interview_manager.create_session("í…ŒìŠ¤íŠ¸", "í…ŒìŠ¤íŠ¸ì§ë¬´")
                if test_session_id:
                    test_results.append("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                    
                    # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ì‚­ì œ
                    with sqlite3.connect(st.session_state.interview_manager.db_path) as conn:
                        conn.execute("DELETE FROM interview_sessions WHERE session_id = ?", (test_session_id,))
                        conn.commit()
                else:
                    test_results.append("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            except Exception as e:
                test_results.append(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            
            # í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
            try:
                test_question = st.session_state.interview_manager.questions[0]
                evaluation = st.session_state.interview_manager.evaluator.evaluate_answer(
                    test_question, "í…ŒìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤.", []
                )
                if evaluation and 'overall_score' in evaluation:
                    test_results.append("âœ… í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                else:
                    test_results.append("âŒ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            except Exception as e:
                test_results.append(f"âŒ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ í‘œì‹œ
        st.markdown("#### ğŸ” í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        for result in test_results:
            st.write(result)
    
    # ë¡œê·¸ íŒŒì¼ ì¡°íšŒ
    st.markdown("---")
    st.markdown("### ğŸ“‹ ìµœê·¼ ë¡œê·¸")
    
    log_file = "lignex1_interview.log"
    if os.path.exists(log_file):
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                log_lines = f.readlines()
            
            # ìµœê·¼ 20ì¤„ë§Œ í‘œì‹œ
            recent_logs = log_lines[-20:] if len(log_lines) > 20 else log_lines
            
            st.text_area(
                "ë¡œê·¸ ë‚´ìš©",
                value=''.join(recent_logs),
                height=200,
                disabled=True
            )
            
            if st.button("ğŸ“¥ ì „ì²´ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ"):
                with open(log_file, 'rb') as f:
                    st.download_button(
                        label="ë¡œê·¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=f.read(),
                        file_name=f"lignex1_interview_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
                        mime="text/plain"
                    )
        except Exception as e:
            st.error(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    else:
        st.info("ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


def show_new_interview_page():
    """ìƒˆ ë©´ì ‘ ì‹œì‘ í˜ì´ì§€"""
    st.markdown('<h2 class="sub-header">ğŸ†• ìƒˆ ë©´ì ‘ ì‹œì‘</h2>', unsafe_allow_html=True)

    # Ensure interview_manager is initialized
    if 'interview_manager' not in st.session_state:
        st.session_state.interview_manager = InterviewManager()

    # Form for new interview
    with st.form("new_interview_form"):
        st.markdown("### ğŸ‘¤ ë©´ì ‘ì ì •ë³´")

        candidate_name = st.text_input(
            "ë©´ì ‘ì ì´ë¦„ *",
            placeholder="ì˜ˆ: í™ê¸¸ë™"
        )

        position = st.selectbox(
            "ì§€ì› ì§ë¬´ *",
            [
                "ì—°êµ¬ê°œë°œ(R&D)",
                "ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´",
                "í•˜ë“œì›¨ì–´ ì—”ì§€ë‹ˆì–´",
                "ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´",
                "í’ˆì§ˆê´€ë¦¬",
                "ê¸°ìˆ ì˜ì—…",
                "ì‚¬ì—…ê´€ë¦¬",
                "ê¸°íƒ€"
            ]
        )

        st.markdown("### ğŸ“‹ ë©´ì ‘ ì„¤ì •")

        interview_type = st.selectbox(
            "ë©´ì ‘ ìœ í˜•",
            [
                "ì¡°ì§ì í•©ë„ ì¸ì„±ë©´ì ‘ (ê¸°ë³¸)",
                "í•µì‹¬ê°€ì¹˜ ì¤‘ì‹¬ ë©´ì ‘",
                "ë¦¬ë”ì‹­ ì—­ëŸ‰ ë©´ì ‘",
                "ì»¤ìŠ¤í…€ ë©´ì ‘"
            ]
        )

        question_count = st.slider(
            "ì§ˆë¬¸ ìˆ˜",
            min_value=3,
            max_value=8,
            value=5,
            help="ë©´ì ‘ ì‹œê°„ì— ë§ì¶° 3-8ê°œ ì§ˆë¬¸ ì„ íƒ"
        )

        submitted = st.form_submit_button(
            "ğŸš€ ë©´ì ‘ ì‹œì‘",
            use_container_width=True
        )

        if submitted:
            if not candidate_name or not position:
                st.error("âš ï¸ ëª¨ë“  í•„ìˆ˜ í•­ëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # Create new session
                session_id = st.session_state.interview_manager.create_session(
                    candidate_name, position
                )

                if session_id:
                    st.session_state.current_session_id = session_id
                    st.session_state.current_question_index = 0
                    st.session_state.interview_answers = []
                    st.session_state.selected_questions = st.session_state.interview_manager.questions[:question_count]
                    st.session_state.interview_started = True

                    st.success(f"âœ… ë©´ì ‘ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤! (ì„¸ì…˜ ID: {session_id[:8]}...)")
                    st.info("ğŸ“ ì•„ë˜ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë©´ì ‘ì„ ì‹œì‘í•˜ì„¸ìš”.")
                else:
                    st.error("âŒ ë©´ì ‘ ì„¸ì…˜ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# í¼ ë°–ì—ì„œ ë©´ì ‘ ì§„í–‰ ë²„íŠ¼ í‘œì‹œ
    if st.session_state.get('interview_started', False):
        if st.button("ğŸš€ ë©´ì ‘ ì§„í–‰í•˜ê¸°", use_container_width=True, key="goto_interview"):
            st.session_state.auto_navigate_to_interview = True
            st.session_state.interview_started = False  # ë²„íŠ¼ ìˆ¨ê¸°ê¸°
            st.rerun()

    # Interview statistics section
    st.markdown("---")
    st.markdown("### ğŸ“Š ë©´ì ‘ í†µê³„")
    col1, col2 = st.columns(2)  # Define columns here

    with col1:
        sessions = st.session_state.interview_manager.get_all_sessions()
        total_sessions = len(sessions)
        completed_sessions = len([s for s in sessions if s['status'] == 'completed'])

        st.metric("ì „ì²´ ë©´ì ‘", total_sessions)
        st.metric("ì™„ë£Œëœ ë©´ì ‘", completed_sessions)

    with col2:
        if completed_sessions > 0:
            avg_score = np.mean([s['overall_score'] for s in sessions if s['overall_score'] > 0])
            st.metric("í‰ê·  ì ìˆ˜", f"{avg_score:.1f}ì ")
        else:
            st.metric("í‰ê·  ì ìˆ˜", "N/A")

        st.markdown("---")
        st.markdown("### ğŸ’¡ ë©´ì ‘ ê°€ì´ë“œ")

        with st.expander("ë©´ì ‘ ì§„í–‰ ë°©ë²•"):
            st.markdown("""
            1. **ë©´ì ‘ì ì •ë³´ ì…ë ¥**: ì´ë¦„ê³¼ ì§€ì› ì§ë¬´ë¥¼ ì •í™•íˆ ì…ë ¥
            2. **ì§ˆë¬¸ ìˆ˜ ì„ íƒ**: ë©´ì ‘ ì‹œê°„ì— ë§ì¶° 3-8ê°œ ì§ˆë¬¸ ì„ íƒ
            3. **ë©´ì ‘ ì§„í–‰**: ê° ì§ˆë¬¸ì— ëŒ€í•´ ì¶©ë¶„íˆ ìƒê°í•œ í›„ ë‹µë³€
            4. **ì‹¤ì‹œê°„ í”¼ë“œë°±**: AIê°€ ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ ì¦‰ì‹œ í”¼ë“œë°± ì œê³µ
            5. **ì¢…í•© ë¶„ì„**: ë©´ì ‘ ì™„ë£Œ í›„ ìƒì„¸í•œ ë¶„ì„ ë¦¬í¬íŠ¸ ì œê³µ
            """)

        with st.expander("í‰ê°€ ê¸°ì¤€"):
            st.markdown("""
            - **ë‚´ìš© ê´€ë ¨ì„±** (30%): ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ì—°ê´€ì„±
            - **í‚¤ì›Œë“œ ë§¤ì¹­** (20%): í•µì‹¬ í‚¤ì›Œë“œ í™œìš©ë„
            - **ì¼ê´€ì„±** (20%): ë…¼ë¦¬ì  ë‹µë³€ êµ¬ì¡°
            - **ê¸ì •ë„** (15%): ì ê·¹ì ì´ê³  ê¸ì •ì ì¸ íƒœë„
            - **ë‹µë³€ ê¸¸ì´** (15%): ì ì ˆí•œ ë‹µë³€ ë¶„ëŸ‰
            """)

def show_interview_progress_page():
    """ë©´ì ‘ ì§„í–‰ í˜ì´ì§€"""
    st.markdown('<h2 class="sub-header">ğŸ“ ë©´ì ‘ ì§„í–‰</h2>', unsafe_allow_html=True)
    
    # ì„¸ì…˜ ìƒíƒœ í™•ì¸
    if not st.session_state.current_session_id:
        st.warning("âš ï¸ ì§„í–‰ ì¤‘ì¸ ë©´ì ‘ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ 'ìƒˆ ë©´ì ‘ ì‹œì‘' í˜ì´ì§€ì—ì„œ ë©´ì ‘ì„ ì‹œì‘í•˜ì„¸ìš”.")
        return
    
    # í˜„ì¬ ë©´ì ‘ ì •ë³´ í‘œì‹œ
    current_session = st.session_state.current_session_id
    current_index = st.session_state.current_question_index
    questions = st.session_state.selected_questions
    
    if current_index >= len(questions):
        st.success("ğŸ‰ ëª¨ë“  ì§ˆë¬¸ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")
        
        if st.button("ğŸ“Š ë©´ì ‘ ì™„ë£Œ ë° ê²°ê³¼ í™•ì¸", use_container_width=True):
            completion_result = st.session_state.interview_manager.complete_session(current_session)
            
            if 'error' not in completion_result:
                st.success("âœ… ë©´ì ‘ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.balloons()
                
                # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.current_session_id = None
                st.session_state.current_question_index = 0
                st.session_state.interview_answers = []
                st.session_state.selected_questions = []
                
                st.info("ğŸ“‹ 'ê²°ê³¼ ë¶„ì„' í˜ì´ì§€ì—ì„œ ìƒì„¸í•œ ë¶„ì„ì„ í™•ì¸í•˜ì„¸ìš”.")
            else:
                st.error(f"âŒ ë©´ì ‘ ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨: {completion_result['error']}")
        
        return
    
    # í˜„ì¬ ì§ˆë¬¸
    current_question = questions[current_index]
    
    # ì§„í–‰ë¥  í‘œì‹œ
    progress = (current_index) / len(questions)
    st.progress(progress, text=f"ì§„í–‰ë¥ : {current_index}/{len(questions)} ({progress*100:.1f}%)")
    
    # ì§ˆë¬¸ í‘œì‹œ
    st.markdown(f"### ì§ˆë¬¸ {current_index + 1}/{len(questions)}")
    st.markdown(f'<div class="interview-card">{current_question.text}</div>', unsafe_allow_html=True)
    
    # ì§ˆë¬¸ ì •ë³´
    col1, col2, col3 = st.columns(3)
    with col1:
        st.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {current_question.category}")
    with col2:
        st.info(f"â±ï¸ ì˜ˆìƒ ì‹œê°„: {current_question.expected_duration}ë¶„")
    with col3:
        st.info(f"ğŸ“Š ë‚œì´ë„: {current_question.difficulty_level}")
    
    # RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    if st.session_state.interview_manager.rag_service.is_available():
        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ì„ ì„¸ì…˜ ìƒíƒœì— ìºì‹œ
        context_key = f"contexts_{current_session}_{current_index}"
        
        if context_key not in st.session_state:
            with st.spinner("ğŸ” ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘..."):
                st.session_state[context_key] = st.session_state.interview_manager.rag_service.get_enhanced_context(current_question)
        
        contexts = st.session_state[context_key]
        
        if contexts:
            with st.expander("ğŸ’¡ ì°¸ê³  ì •ë³´ (AIê°€ ì°¾ì€ ê´€ë ¨ ìë£Œ)", expanded=False):
                for i, ctx in enumerate(contexts, 1):
                    st.markdown(f"**{i}. {ctx['title']}** (ê´€ë ¨ë„: {ctx['score']:.3f})")
                    st.markdown(f"{ctx['content'][:200]}...")
                    if ctx['url']:
                        st.markdown(f"[ì›ë¬¸ ë³´ê¸°]({ctx['url']})")
                    st.markdown("---")
    else:
        contexts = []
        st.warning("âš ï¸ RAG ì‹œìŠ¤í…œì´ ì—°ê²°ë˜ì§€ ì•Šì•„ ì°¸ê³  ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë‹µë³€ ì…ë ¥
    st.markdown("### ğŸ“ ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”")
    
    # ë‹µë³€ ì…ë ¥ í‚¤ë¥¼ í˜„ì¬ ì§ˆë¬¸ì— ê³ ìœ í•˜ê²Œ ë§Œë“¤ê¸°
    answer_key = f"answer_{current_session}_{current_index}"
    
    answer = st.text_area(
        "ë‹µë³€",
        height=200,
        placeholder="êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•´ ì£¼ì„¸ìš”...",
        key=answer_key
    )
    
    # ë‹µë³€ ê¸¸ì´ ì •ë³´
    if answer:
        word_count = len(answer.split())
        st.caption(f"ğŸ’¬ í˜„ì¬ ë‹µë³€ ê¸¸ì´: {word_count}ë‹¨ì–´")
        
        if current_question.difficulty_level == "easy":
            optimal_range = "50-150ë‹¨ì–´"
        elif current_question.difficulty_level == "hard":
            optimal_range = "150-300ë‹¨ì–´"
        else:
            optimal_range = "100-200ë‹¨ì–´"
        
        st.caption(f"ğŸ“ ê¶Œì¥ ê¸¸ì´: {optimal_range}")
    
    # ë‹µë³€ ìœ íš¨ì„± ê²€ì‚¬
    answer_valid = answer and answer.strip() and len(answer.strip()) > 10
    
    # ë‹µë³€ ì œì¶œ ë²„íŠ¼ë“¤
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        if st.button("ğŸ“¤ ë‹µë³€ ì œì¶œ", disabled=not answer_valid, use_container_width=True, key=f"submit_{current_index}"):
            if not answer or not answer.strip():
                st.error("ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                return
            
            # ë¡œë”© ìƒíƒœ í‘œì‹œ
            with st.spinner("ğŸ“Š ë‹µë³€ ë¶„ì„ ì¤‘..."):
                # ë‹µë³€ ì €ì¥ ë° í‰ê°€
                evaluation = st.session_state.interview_manager.save_answer(
                    current_session, current_question, answer, contexts
                )
            
            if 'error' not in evaluation:
                # í‰ê°€ ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥ (ë‹¤ìŒ í™”ë©´ì—ì„œ í‘œì‹œí•˜ê¸° ìœ„í•´)
                st.session_state.last_evaluation = evaluation
                
                # ë‹µë³€ ê¸°ë¡ ì €ì¥
                st.session_state.interview_answers.append({
                    'question': current_question,
                    'answer': answer,
                    'evaluation': evaluation,
                    'contexts': contexts
                })
                
                # ë‹¤ìŒ ì§ˆë¬¸ì„ ìœ„í•´ ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì‚­ì œ
                next_context_key = f"contexts_{current_session}_{current_index + 1}"
                if next_context_key in st.session_state:
                    del st.session_state[next_context_key]
                
                # ì§ˆë¬¸ ì¸ë±ìŠ¤ ì¦ê°€
                st.session_state.current_question_index += 1
                
                # ë‹µë³€ ì„±ê³µ í”Œë˜ê·¸ ì„¤ì •
                st.session_state.answer_submitted = True
                
                # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                st.rerun()
            else:
                st.error(f"âŒ ë‹µë³€ ì €ì¥ ì‹¤íŒ¨: {evaluation['error']}")
    
    with col2:
        if st.button("â­ï¸ ê±´ë„ˆë›°ê¸°", use_container_width=True, key=f"skip_{current_index}"):
            # ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ì‚­ì œ
            next_context_key = f"contexts_{current_session}_{current_index + 1}"
            if next_context_key in st.session_state:
                del st.session_state[next_context_key]
            
            st.session_state.current_question_index += 1
            st.rerun()
    
    with col3:
        if st.button("âŒ ë©´ì ‘ ì¤‘ë‹¨", use_container_width=True, key=f"abort_{current_index}"):
            if st.session_state.get('confirm_abort', False):
                # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.current_session_id = None
                st.session_state.current_question_index = 0
                st.session_state.interview_answers = []
                st.session_state.selected_questions = []
                st.session_state.confirm_abort = False
                
                st.warning("ë©´ì ‘ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
            else:
                st.session_state.confirm_abort = True
                st.warning("âš ï¸ ì •ë§ë¡œ ë©´ì ‘ì„ ì¤‘ë‹¨í•˜ì‹œê² ìŠµë‹ˆê¹Œ? ë‹¤ì‹œ í•œ ë²ˆ í´ë¦­í•˜ë©´ ì¤‘ë‹¨ë©ë‹ˆë‹¤.")
    
    # ë‹µë³€ ì œì¶œ ì§í›„ í”¼ë“œë°± í‘œì‹œ
    if st.session_state.get('answer_submitted', False) and st.session_state.get('last_evaluation'):
        st.session_state.answer_submitted = False  # í”Œë˜ê·¸ ë¦¬ì…‹
        
        evaluation = st.session_state.last_evaluation
        
        # ì„±ê³µ ë©”ì‹œì§€
        st.success("âœ… ë‹µë³€ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ì ìˆ˜ í‘œì‹œ
        score = evaluation['overall_score']
        if score >= 0.8:
            score_class = "score-good"
            score_emoji = "ğŸ‰"
        elif score >= 0.6:
            score_class = "score-average"
            score_emoji = "ğŸ‘"
        else:
            score_class = "score-poor"
            score_emoji = "ğŸ’ª"
        
        st.markdown(f'<div class="{score_class}">ì¢…í•© ì ìˆ˜: {score:.2f}/1.00 {score_emoji}</div>', unsafe_allow_html=True)
        
        # í”¼ë“œë°± í‘œì‹œ
        st.markdown("#### ğŸ“ ì¦‰ì‹œ í”¼ë“œë°±")
        st.info(evaluation['feedback'])
        
        # ê°•ì ê³¼ ê°œì„ ì 
        col1, col2 = st.columns(2)
        
        with col1:
            if evaluation['strengths']:
                st.markdown("**âœ… ê°•ì **")
                for strength in evaluation['strengths']:
                    st.markdown(f"â€¢ {strength}")
        
        with col2:
            if evaluation['improvements']:
                st.markdown("**ğŸ’¡ ê°œì„ ì **")
                for improvement in evaluation['improvements']:
                    st.markdown(f"â€¢ {improvement}")
        
        # ìƒì„¸ ì ìˆ˜ ì°¨íŠ¸
        with st.expander("ğŸ“Š ìƒì„¸ ì ìˆ˜ ë³´ê¸°"):
            detailed_scores = evaluation['detailed_scores']
            
            score_data = {
                'í‰ê°€ í•­ëª©': ['í‚¤ì›Œë“œ ë§¤ì¹­', 'ê°ì • ë¶„ì„', 'ì¼ê´€ì„±', 'ê¸¸ì´ ì ì ˆì„±', 'ë‚´ìš© ê´€ë ¨ì„±'],
                'ì ìˆ˜': [
                    detailed_scores.get('keyword_match', 0),
                    detailed_scores.get('sentiment', 0),
                    detailed_scores.get('coherence', 0),
                    detailed_scores.get('length_appropriateness', 0),
                    detailed_scores.get('content_relevance', 0)
                ]
            }
            
            df_scores = pd.DataFrame(score_data)
            
            fig = px.bar(
                df_scores, 
                x='í‰ê°€ í•­ëª©', 
                y='ì ìˆ˜',
                color='ì ìˆ˜',
                color_continuous_scale='RdYlGn',
                range_color=[0, 1]
            )
            fig.update_layout(height=300)
            st.plotly_chart(fig, use_container_width=True)
        
        # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™ ë²„íŠ¼
        if current_index + 1 < len(questions):
            if st.button("â¡ï¸ ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ", use_container_width=True, key="next_question"):
                st.session_state.last_evaluation = None  # í‰ê°€ ê²°ê³¼ ì´ˆê¸°í™”
                st.rerun()
        
        # í‰ê°€ ê²°ê³¼ ì´ˆê¸°í™” (3ì´ˆ í›„ ìë™)
        if 'last_evaluation' in st.session_state:
            time.sleep(2)  # 2ì´ˆ ëŒ€ê¸°
            st.session_state.last_evaluation = None
            st.rerun()

def show_result_analysis_page():
    """ê²°ê³¼ ë¶„ì„ í˜ì´ì§€"""
    st.markdown('<h2 class="sub-header">ğŸ“Š ê²°ê³¼ ë¶„ì„</h2>', unsafe_allow_html=True)
    
    # ì„¸ì…˜ ì„ íƒ
    sessions = st.session_state.interview_manager.get_all_sessions()
    
    if not sessions:
        st.info("ğŸ“‹ ë¶„ì„í•  ë©´ì ‘ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì™„ë£Œëœ ì„¸ì…˜ë§Œ í•„í„°ë§
    completed_sessions = [s for s in sessions if s['status'] == 'completed']
    
    if not completed_sessions:
        st.info("ğŸ“‹ ì™„ë£Œëœ ë©´ì ‘ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„¸ì…˜ ì„ íƒ
    session_options = {
        f"{s['candidate_name']} - {s['position']} ({s['start_time'][:10]})": s['session_id'] 
        for s in completed_sessions
    }
    
    selected_session_label = st.selectbox(
        "ë¶„ì„í•  ë©´ì ‘ ì„ íƒ",
        list(session_options.keys())
    )
    
    selected_session_id = session_options[selected_session_label]
    
    # ë¶„ì„ ë°ì´í„° ì¡°íšŒ
    analytics = st.session_state.interview_manager.get_session_analytics(selected_session_id)
    
    if 'error' in analytics:
        st.error(f"âŒ ë¶„ì„ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {analytics['error']}")
        return
    
    session_info = analytics['session_info']
    answers = analytics['answers']
    statistics = analytics['statistics']
    
    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
    st.markdown("### ğŸ“‹ ë©´ì ‘ ê¸°ë³¸ ì •ë³´")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ë©´ì ‘ì", session_info['candidate_name'])
        st.metric("ì§€ì› ì§ë¬´", session_info['position'])
    
    with col2:
        st.metric("ì¢…í•© ì ìˆ˜", f"{session_info['overall_score']:.2f}")
        st.metric("ë‹µë³€ ìˆ˜", statistics['total_answers'])
    
    with col3:
        if session_info['end_time']:
            start_time = datetime.fromisoformat(session_info['start_time'])
            end_time = datetime.fromisoformat(session_info['end_time'])
            duration = end_time - start_time
            st.metric("ë©´ì ‘ ì‹œê°„", f"{duration.seconds // 60}ë¶„")
        st.metric("í‰ê·  ë‹µë³€ ê¸¸ì´", f"{statistics['average_answer_length']:.0f}ë‹¨ì–´")
    
    # ì ìˆ˜ ë¶„ì„ ì°¨íŠ¸
    st.markdown("### ğŸ“ˆ ì ìˆ˜ ë¶„ì„")
    
    if answers:
        # ì§ˆë¬¸ë³„ ì ìˆ˜ ì°¨íŠ¸
        score_data = {
            'question': [f"Q{i+1}" for i in range(len(answers))],
            'overall_score': [a['evaluation_score'] for a in answers],
            'keyword_match': [a['keyword_match_score'] for a in answers],
            'sentiment': [a['sentiment_score'] for a in answers],
            'coherence': [a['coherence_score'] for a in answers]
        }
        
        df_scores = pd.DataFrame(score_data)
        
        # ì¢…í•© ì ìˆ˜ ì°¨íŠ¸
        fig1 = px.line(
            df_scores, 
            x='question', 
            y='overall_score',
            title='ì§ˆë¬¸ë³„ ì¢…í•© ì ìˆ˜',
            markers=True,
            range_y=[0, 1]
        )
        fig1.update_layout(height=400)
        st.plotly_chart(fig1, use_container_width=True)
        
        # ìƒì„¸ ì ìˆ˜ ë¹„êµ ì°¨íŠ¸
        fig2 = go.Figure()
        
        fig2.add_trace(go.Scatter(
            x=df_scores['question'],
            y=df_scores['keyword_match'],
            mode='lines+markers',
            name='í‚¤ì›Œë“œ ë§¤ì¹­',
            line=dict(color='blue')
        ))
        
        fig2.add_trace(go.Scatter(
            x=df_scores['question'],
            y=df_scores['sentiment'],
            mode='lines+markers',
            name='ê°ì • ë¶„ì„',
            line=dict(color='green')
        ))
        
        fig2.add_trace(go.Scatter(
            x=df_scores['question'],
            y=df_scores['coherence'],
            mode='lines+markers',
            name='ì¼ê´€ì„±',
            line=dict(color='red')
        ))
        
        fig2.update_layout(
            title='ì§ˆë¬¸ë³„ ì„¸ë¶€ ì ìˆ˜ ë¹„êµ',
            xaxis_title='ì§ˆë¬¸',
            yaxis_title='ì ìˆ˜',
            yaxis=dict(range=[0, 1]),
            height=400
        )
        
        st.plotly_chart(fig2, use_container_width=True)
    
    # í†µê³„ ìš”ì•½
    st.markdown("### ğŸ“Š í†µê³„ ìš”ì•½")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ì ìˆ˜ í†µê³„")
        st.metric("í‰ê·  ì ìˆ˜", f"{statistics['average_score']:.2f}")
        st.metric("ì ìˆ˜ í¸ì°¨", f"{statistics['score_deviation']:.2f}")
        st.metric("í‰ê·  ê°ì • ì ìˆ˜", f"{statistics['average_sentiment']:.2f}")
    
    with col2:
        st.markdown("#### ë‹µë³€ í†µê³„")
        st.metric("í‰ê·  ì¼ê´€ì„±", f"{statistics['average_coherence']:.2f}")
        st.metric("í‰ê·  ë‹µë³€ ê¸¸ì´", f"{statistics['average_answer_length']:.1f}ë‹¨ì–´")
        st.metric("ì´ ë‹µë³€ ìˆ˜", statistics['total_answers'])
    
    # ê°œë³„ ë‹µë³€ ìƒì„¸ ë¶„ì„
    st.markdown("### ğŸ“ ë‹µë³€ ìƒì„¸ ë¶„ì„")
    
    for i, answer in enumerate(answers, 1):
        with st.expander(f"ì§ˆë¬¸ {i}: {answer['question_text'][:50]}... (ì ìˆ˜: {answer['evaluation_score']:.2f})"):
            
            # ì§ˆë¬¸ê³¼ ë‹µë³€
            st.markdown("**ì§ˆë¬¸:**")
            st.write(answer['question_text'])
            
            st.markdown("**ë‹µë³€:**")
            st.write(answer['answer'])
            
            # ì ìˆ˜ ìƒì„¸
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ì¢…í•© ì ìˆ˜", f"{answer['evaluation_score']:.2f}")
                st.metric("í‚¤ì›Œë“œ ë§¤ì¹­", f"{answer['keyword_match_score']:.2f}")
            
            with col2:
                st.metric("ê°ì • ë¶„ì„", f"{answer['sentiment_score']:.2f}")
                st.metric("ì¼ê´€ì„±", f"{answer['coherence_score']:.2f}")
            
            with col3:
                st.metric("ë‹µë³€ ê¸¸ì´", f"{answer['answer_length']}ë‹¨ì–´")
                st.metric("ì‘ì„± ì‹œê°„", answer['timestamp'][:16])
            
            # ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸
            if answer['context_used']:
                st.markdown("**ì°¸ê³ í•œ ìë£Œ:**")
                for ctx in answer['context_used']:
                    st.write(f"- {ctx.get('title', 'Unknown')} (ê´€ë ¨ë„: {ctx.get('score', 0):.3f})")
    
    # ì¢…í•© í‰ê°€ ë° ê¶Œì¥ì‚¬í•­
    st.markdown("### ğŸ’¡ ì¢…í•© í‰ê°€ ë° ê¶Œì¥ì‚¬í•­")
    
    overall_score = statistics['average_score']
    
    if overall_score >= 0.8:
        st.success("ğŸ‰ **ìš°ìˆ˜í•œ ë©´ì ‘ ì„±ê³¼ì…ë‹ˆë‹¤!**")
        st.write("ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸ì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, ì²´ê³„ì ì´ê³  ë…¼ë¦¬ì ì¸ ë‹µë³€ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.")
    elif overall_score >= 0.6:
        st.info("ğŸ‘ **ì–‘í˜¸í•œ ë©´ì ‘ ì„±ê³¼ì…ë‹ˆë‹¤.**")
        st.write("ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ë‹µë³€ì„ í–ˆì§€ë§Œ, ëª‡ ê°€ì§€ ì˜ì—­ì—ì„œ ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
    else:
        st.warning("ğŸ’ª **ê°œì„ ì´ í•„ìš”í•œ ë©´ì ‘ ì„±ê³¼ì…ë‹ˆë‹¤.**")
        st.write("ë©´ì ‘ ê¸°ìˆ ê³¼ ë‹µë³€ êµ¬ì„±ì— ë” ë§ì€ ì—°ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ê°œì„  ê¶Œì¥ì‚¬í•­
    st.markdown("#### ğŸ“ˆ ê°œì„  ê¶Œì¥ì‚¬í•­")
    
    recommendations = []
    
    if statistics['average_sentiment'] < 0.6:
        recommendations.append("ë” ê¸ì •ì ì´ê³  ì ê·¹ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
    
    if statistics['average_coherence'] < 0.6:
        recommendations.append("ë‹µë³€ì˜ ë…¼ë¦¬ì  êµ¬ì¡°ë¥¼ ê°œì„ í•˜ê³  ì—°ê²°ì–´ë¥¼ ë” í™œìš©í•´ë³´ì„¸ìš”.")
    
    if statistics['average_answer_length'] < 80:
        recommendations.append("ë‹µë³€ì„ ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±í•´ë³´ì„¸ìš”.")
    elif statistics['average_answer_length'] > 200:
        recommendations.append("ë‹µë³€ì„ ë” ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ì •ë¦¬í•´ë³´ì„¸ìš”.")
    
    if statistics['score_deviation'] > 0.3:
        recommendations.append("ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ ì¼ê´€ëœ ìˆ˜ì¤€ì˜ ë‹µë³€ì„ ìœ ì§€í•´ë³´ì„¸ìš”.")
    
    if recommendations:
        for rec in recommendations:
            st.write(f"â€¢ {rec}")
    else:
        st.write("â€¢ ì „ë°˜ì ìœ¼ë¡œ ê· í˜•ì¡íŒ ì¢‹ì€ ë©´ì ‘ ì„±ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.")

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    try:
        if 'interview_manager' not in st.session_state:
            st.session_state.interview_manager = InterviewManager()
        
        if 'current_session_id' not in st.session_state:
            st.session_state.current_session_id = None
                
        if 'current_question_index' not in st.session_state:
            st.session_state.current_question_index = 0
        
        if 'interview_answers' not in st.session_state:
            st.session_state.interview_answers = []
        
        if 'selected_questions' not in st.session_state:
            st.session_state.selected_questions = []
    except Exception as e:
        st.error(f"ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error(f"Session state initialization error: {e}")

def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    # Header
    st.markdown('<h1 class="main-header">ğŸ›¡ï¸ LIGNEX1 RAG ê¸°ë°˜ ì¡°ì§ì í•©ë„ ì¸ì„±ë©´ì ‘ ì‹œìŠ¤í…œ</h1>', unsafe_allow_html=True)

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”ë¥¼ ê°€ì¥ ë¨¼ì € ì‹¤í–‰
    initialize_session_state()
    
    # Sidebar
    with st.sidebar:
        st.markdown('<h2 class="sub-header">ğŸ“‹ ë©”ë‰´</h2>', unsafe_allow_html=True)
        
        page = st.selectbox(
            "í˜ì´ì§€ ì„ íƒ",
            ["ìƒˆ ë©´ì ‘ ì‹œì‘", "ë©´ì ‘ ì§„í–‰", "ê²°ê³¼ ë¶„ì„", "ë©´ì ‘ ê¸°ë¡", "ì‹œìŠ¤í…œ ìƒíƒœ"]
        )

        # RAG system status
        st.markdown("---")
        st.markdown("### ğŸ”§ RAG ì‹œìŠ¤í…œ ìƒíƒœ")
        rag_available = st.session_state.interview_manager.rag_service.is_available()

        if rag_available:
            st.success("âœ… RAG ì‹œìŠ¤í…œ ì—°ê²°ë¨")
            try:
                # ë¨¼ì € ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                collections = st.session_state.interview_manager.rag_service.qdrant_client.get_collections().collections
                collection_name = st.session_state.interview_manager.rag_service.collection_name
                collection_exists = any(c.name == collection_name for c in collections)
                
                if collection_exists:
                    try:
                        # ì»¬ë ‰ì…˜ ê¸°ë³¸ ì •ë³´ë§Œ ì¡°íšŒ (ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ íšŒí”¼)
                        collection_info = st.session_state.interview_manager.rag_service.qdrant_client.get_collection(collection_name)
                        
                        # ì•ˆì „í•˜ê²Œ ì •ë³´ ì¶”ì¶œ
                        if hasattr(collection_info, 'vectors_count'):
                            vector_count = collection_info.vectors_count
                        else:
                            # ëŒ€ì²´ ë°©ë²•: ì ê²€ API ì‚¬ìš©
                            vector_count = "ì•Œ ìˆ˜ ì—†ìŒ"
                        
                        st.info(f"ğŸ“Š ì»¬ë ‰ì…˜: {collection_name}")
                        st.info(f"ğŸ“Š ë²¡í„° ë°ì´í„°: {vector_count}")
                        st.success("âœ… ì»¬ë ‰ì…˜ ì •ìƒ ì‘ë™")
                        
                    except Exception as detail_error:
                        # ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ í‘œì‹œ
                        st.success(f"âœ… ì»¬ë ‰ì…˜ '{collection_name}' ì¡´ì¬")
                        st.info("ğŸ“Š ìƒì„¸ ì •ë³´ ì¡°íšŒ ë¶ˆê°€ (ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ)")
                        logger.warning(f"Collection detail query failed: {detail_error}")
                else:
                    st.warning(f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    st.info("ğŸ’¡ RAG ë°ì´í„°ë¥¼ ë¨¼ì € ì¸ë±ì‹±í•´ì£¼ì„¸ìš”")

            except Exception as e:
                st.error(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                logger.error(f"Collection info query failed: {e}")
        else:
            st.error("âŒ RAG ì‹œìŠ¤í…œ ì—°ê²° ì•ˆë¨")
            st.info("ğŸ’¡ Qdrantê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
        

    # ìë™ ë„¤ë¹„ê²Œì´ì…˜ ì²˜ë¦¬
    if st.session_state.get('auto_navigate_to_interview', False):
        st.session_state.auto_navigate_to_interview = False
        page = "ë©´ì ‘ ì§„í–‰"

    # Page routing
    if page == "ìƒˆ ë©´ì ‘ ì‹œì‘":
        show_new_interview_page()
    elif page == "ë©´ì ‘ ì§„í–‰":
        show_interview_progress_page()

    elif page == "ê²°ê³¼ ë¶„ì„":
        show_result_analysis_page()
    elif page == "ë©´ì ‘ ê¸°ë¡":
        show_interview_records_page()
    elif page == "ì‹œìŠ¤í…œ ìƒíƒœ":
        show_system_status_page()

    # Footer
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; padding: 1rem;'>
            ğŸ›¡ï¸ LIGNEX1 RAG ê¸°ë°˜ ë©´ì ‘ ì‹œìŠ¤í…œ v1.0<br>
            Powered by Streamlit, Qdrant, and Sentence Transformers
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        logger.error(f"Application error: {e}")
        if st.button("ğŸ”„ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ì‹œì‘"):
            st.rerun()