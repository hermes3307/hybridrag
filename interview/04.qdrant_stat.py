#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ë° ê²€ìƒ‰ ë„êµ¬
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ì •ë³´ë¥¼ ì¡°íšŒí•˜ê³  ê°„ë‹¨í•œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import json
import logging
import argparse
from datetime import datetime
from typing import List, Dict, Optional
import sys

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import re
from collections import Counter

# Qdrant í´ë¼ì´ì–¸íŠ¸
try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import Distance, VectorParams, PointStruct
    from qdrant_client.http import models
except ImportError:
    print("âŒ Qdrant í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install qdrant-client")
    sys.exit(1)

# ì„ë² ë”© ëª¨ë¸ (fallback í¬í•¨)
class SimpleTfidfEmbedder:
    """TF-IDF ê¸°ë°˜ ê°„ë‹¨í•œ ì„ë² ë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, vector_size: int = 384):
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            
            self.vector_size = vector_size
            self.vectorizer = TfidfVectorizer(
                max_features=2000,
                ngram_range=(1, 2),
                min_df=1,
                max_df=0.9,
                stop_words=None
            )
            
            self.svd = TruncatedSVD(n_components=min(vector_size, 384))
            self.is_fitted = False
            
            # ë¶ˆìš©ì–´
            self.stopwords = {
                'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë˜ëŠ”', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ',
                'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ',
                'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
                'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',
                'after', 'above', 'below', 'between', 'among'
            }
            
        except ImportError:
            print("âŒ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì„¤ì¹˜: pip install scikit-learn")
            raise
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text.strip())
        
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in self.stopwords and len(word) > 1]
        
        return ' '.join(filtered_words)
    
    def fit(self, texts: List[str]):
        """ëª¨ë¸ í›ˆë ¨"""
        processed_texts = [self.preprocess_text(text) for text in texts]
        processed_texts = [text for text in processed_texts if text.strip()]
        
        if not processed_texts:
            return
        
        tfidf_matrix = self.vectorizer.fit_transform(processed_texts)
        
        if tfidf_matrix.shape[1] > self.vector_size:
            self.svd.fit(tfidf_matrix)
        
        self.is_fitted = True
    
    def encode(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if isinstance(text, str):
            texts = [text]
            single_input = True
        else:
            texts = text
            single_input = False
        
        if not self.is_fitted:
            default_texts = [
                "LIGë„¥ìŠ¤ì› ê¸°ì—…ë¬¸í™” OPEN POSITIVE",
                "ë°©ìœ„ì‚°ì—… êµ­ë°© ë¯¸ì‚¬ì¼ ë ˆì´ë”",
                "ì—°êµ¬ê°œë°œ R&D ê¸°ìˆ ê°œë°œ í˜ì‹ "
            ] + texts
            self.fit(default_texts)
        
        processed_texts = [self.preprocess_text(t) for t in texts]
        tfidf_vectors = self.vectorizer.transform(processed_texts)
        
        if hasattr(self.svd, 'components_') and tfidf_vectors.shape[1] > self.vector_size:
            vectors = self.svd.transform(tfidf_vectors)
        else:
            vectors = tfidf_vectors.toarray()
        
        if vectors.shape[1] < self.vector_size:
            padding = np.zeros((vectors.shape[0], self.vector_size - vectors.shape[1]))
            vectors = np.hstack([vectors, padding])
        elif vectors.shape[1] > self.vector_size:
            vectors = vectors[:, :self.vector_size]
        
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1
        vectors = vectors / norms
        
        return vectors[0] if single_input else vectors

class QdrantStats:
    """Qdrant í†µê³„ ë° ê²€ìƒ‰ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 qdrant_host: str = "localhost",
                 qdrant_port: int = 6333,
                 collection_name: str = "lignex1_news"):
        
        self.collection_name = collection_name
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
            print(f"âœ… Qdrant ì—°ê²° ì„±ê³µ: {qdrant_host}:{qdrant_port}")
        except Exception as e:
            print(f"âŒ Qdrant ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ê²€ìƒ‰ìš©)
        self.embedding_model = None
        self.use_sentence_transformer = False
        
        try:
            from sentence_transformers import SentenceTransformer
            self.embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            self.use_sentence_transformer = True
            print("âœ… SentenceTransformer ë¡œë“œ ì™„ë£Œ")
        except Exception:
            print("âš ï¸  SentenceTransformer ë¡œë“œ ì‹¤íŒ¨, TF-IDF ì‚¬ìš©")
            self.embedding_model = SimpleTfidfEmbedder()
            self.use_sentence_transformer = False
    
    def list_collections(self) -> List[str]:
        """ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ"""
        try:
            collections = self.qdrant_client.get_collections().collections
            return [c.name for c in collections]
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_collection_info(self, collection_name: str = None) -> Dict:
        """ì»¬ë ‰ì…˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        if not collection_name:
            collection_name = self.collection_name
        
        try:
            info = self.qdrant_client.get_collection(collection_name)
            
            return {
                'name': collection_name,
                'vectors_count': info.vectors_count,
                'indexed_vectors_count': info.indexed_vectors_count,
                'points_count': info.points_count,
                'status': str(info.status),
                'optimizer_status': str(info.optimizer_status),
                'disk_data_size': info.disk_data_size,
                'ram_data_size': info.ram_data_size,
                'config': {
                    'distance': str(info.config.params.vectors.distance) if hasattr(info.config.params, 'vectors') else 'unknown',
                    'vector_size': info.config.params.vectors.size if hasattr(info.config.params, 'vectors') else 'unknown'
                }
            }
            
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def sample_points(self, collection_name: str = None, limit: int = 5) -> List[Dict]:
        """ì»¬ë ‰ì…˜ì˜ ìƒ˜í”Œ í¬ì¸íŠ¸ ì¡°íšŒ"""
        if not collection_name:
            collection_name = self.collection_name
        
        try:
            # ìŠ¤í¬ë¡¤ì„ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            scroll_result = self.qdrant_client.scroll(
                collection_name=collection_name,
                limit=limit,
                with_payload=True,
                with_vectors=False
            )
            
            points = []
            for point in scroll_result[0]:  # scroll_resultëŠ” (points, next_page_offset) íŠœí”Œ
                point_data = {
                    'id': point.id,
                    'payload': point.payload
                }
                points.append(point_data)
            
            return points
            
        except Exception as e:
            print(f"âŒ ìƒ˜í”Œ í¬ì¸íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_payload_stats(self, collection_name: str = None, sample_size: int = 100) -> Dict:
        """í˜ì´ë¡œë“œ í†µê³„ ì •ë³´"""
        if not collection_name:
            collection_name = self.collection_name
        
        try:
            scroll_result = self.qdrant_client.scroll(
                collection_name=collection_name,
                limit=sample_size,
                with_payload=True,
                with_vectors=False
            )
            
            field_stats = {}
            sources = Counter()
            api_providers = Counter()
            embedding_models = Counter()
            
            for point in scroll_result[0]:
                payload = point.payload
                
                # í•„ë“œ í†µê³„
                for key, value in payload.items():
                    if key not in field_stats:
                        field_stats[key] = {'count': 0, 'types': set(), 'sample_values': []}
                    
                    field_stats[key]['count'] += 1
                    field_stats[key]['types'].add(type(value).__name__)
                    
                    if len(field_stats[key]['sample_values']) < 3:
                        field_stats[key]['sample_values'].append(str(value)[:50])
                
                # íŠ¹ë³„ í•„ë“œ í†µê³„
                if 'source' in payload:
                    sources[payload['source']] += 1
                if 'api_provider' in payload:
                    api_providers[payload['api_provider']] += 1
                if 'embedding_model' in payload:
                    embedding_models[payload['embedding_model']] += 1
            
            # íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            for field in field_stats:
                field_stats[field]['types'] = list(field_stats[field]['types'])
            
            return {
                'sample_size': len(scroll_result[0]),
                'fields': field_stats,
                'top_sources': dict(sources.most_common(10)),
                'api_providers': dict(api_providers.most_common()),
                'embedding_models': dict(embedding_models.most_common())
            }
            
        except Exception as e:
            print(f"âŒ í˜ì´ë¡œë“œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def search_similar(self, query: str, collection_name: str = None, limit: int = 5, score_threshold: float = 0.1) -> List[Dict]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        if not collection_name:
            collection_name = self.collection_name
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            if self.use_sentence_transformer:
                query_vector = self.embedding_model.encode(query)
            else:
                query_vector = self.embedding_model.encode(query)
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_result = self.qdrant_client.search(
                collection_name=collection_name,
                query_vector=query_vector.tolist(),
                limit=limit,
                score_threshold=score_threshold
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for hit in search_result:
                result = {
                    'id': hit.id,
                    'score': hit.score,
                    'payload': hit.payload
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_filter(self, filter_conditions: Dict, collection_name: str = None, limit: int = 10) -> List[Dict]:
        """í•„í„° ê¸°ë°˜ ê²€ìƒ‰"""
        if not collection_name:
            collection_name = self.collection_name
        
        try:
            scroll_result = self.qdrant_client.scroll(
                collection_name=collection_name,
                scroll_filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key=key,
                            match=models.MatchValue(value=value)
                        ) for key, value in filter_conditions.items()
                    ]
                ),
                limit=limit,
                with_payload=True,
                with_vectors=False
            )
            
            results = []
            for point in scroll_result[0]:
                result = {
                    'id': point.id,
                    'payload': point.payload
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ í•„í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

def print_collection_info(stats: QdrantStats, collection_name: str):
    """ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥"""
    info = stats.get_collection_info(collection_name)
    
    if not info:
        print(f"âŒ ì»¬ë ‰ì…˜ '{collection_name}' ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“Š ì»¬ë ‰ì…˜ ì •ë³´: {collection_name}")
    print("=" * 50)
    print(f"ë²¡í„° ìˆ˜: {info['vectors_count']:,}")
    print(f"ì¸ë±ìŠ¤ëœ ë²¡í„° ìˆ˜: {info['indexed_vectors_count']:,}")
    print(f"í¬ì¸íŠ¸ ìˆ˜: {info['points_count']:,}")
    print(f"ìƒíƒœ: {info['status']}")
    print(f"ìµœì í™” ìƒíƒœ: {info['optimizer_status']}")
    print(f"ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {info['disk_data_size']:,} bytes ({info['disk_data_size']/(1024*1024):.1f} MB)")
    print(f"RAM ì‚¬ìš©ëŸ‰: {info['ram_data_size']:,} bytes ({info['ram_data_size']/(1024*1024):.1f} MB)")
    print(f"ê±°ë¦¬ í•¨ìˆ˜: {info['config']['distance']}")
    print(f"ë²¡í„° ì°¨ì›: {info['config']['vector_size']}")

def print_payload_stats(stats: QdrantStats, collection_name: str):
    """í˜ì´ë¡œë“œ í†µê³„ ì¶œë ¥"""
    payload_stats = stats.get_payload_stats(collection_name)
    
    if not payload_stats:
        print(f"âŒ ì»¬ë ‰ì…˜ '{collection_name}' í˜ì´ë¡œë“œ í†µê³„ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“‹ í˜ì´ë¡œë“œ í†µê³„ (ìƒ˜í”Œ í¬ê¸°: {payload_stats['sample_size']})")
    print("=" * 50)
    
    # í•„ë“œ ì •ë³´
    print("\nğŸ” í•„ë“œ ì •ë³´:")
    for field, stats_data in payload_stats['fields'].items():
        print(f"  â€¢ {field}:")
        print(f"    - ê°œìˆ˜: {stats_data['count']}")
        print(f"    - íƒ€ì…: {', '.join(stats_data['types'])}")
        if stats_data['sample_values']:
            print(f"    - ìƒ˜í”Œ: {stats_data['sample_values'][:2]}")
    
    # ì†ŒìŠ¤ í†µê³„
    if payload_stats['top_sources']:
        print(f"\nğŸ“° ì£¼ìš” ì†ŒìŠ¤ (ìƒìœ„ {len(payload_stats['top_sources'])}ê°œ):")
        for source, count in payload_stats['top_sources'].items():
            print(f"  â€¢ {source}: {count}ê°œ")
    
    # API ì œê³µì í†µê³„
    if payload_stats['api_providers']:
        print(f"\nğŸ”Œ API ì œê³µì:")
        for provider, count in payload_stats['api_providers'].items():
            print(f"  â€¢ {provider}: {count}ê°œ")
    
    # ì„ë² ë”© ëª¨ë¸ í†µê³„
    if payload_stats['embedding_models']:
        print(f"\nğŸ¤– ì„ë² ë”© ëª¨ë¸:")
        for model, count in payload_stats['embedding_models'].items():
            print(f"  â€¢ {model}: {count}ê°œ")

def print_sample_points(stats: QdrantStats, collection_name: str, limit: int = 3):
    """ìƒ˜í”Œ í¬ì¸íŠ¸ ì¶œë ¥"""
    points = stats.sample_points(collection_name, limit)
    
    if not points:
        print(f"âŒ ì»¬ë ‰ì…˜ '{collection_name}' ìƒ˜í”Œì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“„ ìƒ˜í”Œ ë°ì´í„° (ìƒìœ„ {len(points)}ê°œ):")
    print("=" * 50)
    
    for i, point in enumerate(points, 1):
        print(f"\n[{i}] ID: {point['id']}")
        payload = point['payload']
        
        # ì£¼ìš” í•„ë“œë§Œ ì¶œë ¥
        title = payload.get('title', 'No Title')
        content = payload.get('content', 'No Content')
        source = payload.get('source', 'Unknown')
        
        print(f"ì œëª©: {title}")
        print(f"ë‚´ìš©: {content[:100]}...")
        print(f"ì¶œì²˜: {source}")
        
        if 'api_provider' in payload:
            print(f"ì œê³µì: {payload['api_provider']}")
        if 'published_date' in payload:
            print(f"ë°œí–‰ì¼: {payload['published_date']}")

def interactive_search(stats: QdrantStats, collection_name: str):
    """ëŒ€í™”í˜• ê²€ìƒ‰"""
    print(f"\nğŸ” ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ (ì»¬ë ‰ì…˜: {collection_name})")
    print("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”. ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ì„ ì…ë ¥í•˜ì„¸ìš”.")
    print("-" * 50)
    
    while True:
        try:
            query = input("\nê²€ìƒ‰ì–´: ").strip()
            
            if not query:
                continue
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # ê²€ìƒ‰ ì‹¤í–‰
            results = stats.search_similar(query, collection_name, limit=5, score_threshold=0.05)
            
            if results:
                print(f"\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê±´):")
                for i, result in enumerate(results, 1):
                    payload = result['payload']
                    title = payload.get('title', 'No Title')
                    content = payload.get('content', 'No Content')[:100]
                    source = payload.get('source', 'Unknown')
                    
                    print(f"\n[{i}] {title} (ìœ ì‚¬ë„: {result['score']:.3f})")
                    print(f"    ì¶œì²˜: {source}")
                    print(f"    ë‚´ìš©: {content}...")
            else:
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ê²€ìƒ‰ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸ ë° ê²€ìƒ‰ ë„êµ¬')
    parser.add_argument('--host', default='localhost', help='Qdrant í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: localhost)')
    parser.add_argument('--port', type=int, default=6333, help='Qdrant í¬íŠ¸ (ê¸°ë³¸ê°’: 6333)')
    parser.add_argument('--collection', default='lignex1_news', help='ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸ê°’: lignex1_news)')
    parser.add_argument('--list-collections', action='store_true', help='ëª¨ë“  ì»¬ë ‰ì…˜ ëª©ë¡ ì¶œë ¥')
    parser.add_argument('--info', action='store_true', help='ì»¬ë ‰ì…˜ ìƒì„¸ ì •ë³´ ì¶œë ¥')
    parser.add_argument('--stats', action='store_true', help='í˜ì´ë¡œë“œ í†µê³„ ì¶œë ¥')
    parser.add_argument('--sample', type=int, help='ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥ (ê°œìˆ˜ ì§€ì •)')
    parser.add_argument('--search', type=str, help='ê²€ìƒ‰ì–´ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰')
    parser.add_argument('--interactive', action='store_true', help='ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ')
    parser.add_argument('--filter', type=str, help='í•„í„° ê²€ìƒ‰ (JSON í˜•ì‹)')
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ê°’: ëª¨ë“  ì •ë³´ ì¶œë ¥
    if not any([args.list_collections, args.info, args.stats, args.sample, args.search, args.interactive, args.filter]):
        args.info = True
        args.stats = True
        args.sample = 3
    
    try:
        # QdrantStats ì´ˆê¸°í™”
        stats = QdrantStats(
            qdrant_host=args.host,
            qdrant_port=args.port,
            collection_name=args.collection
        )
        
        # ì»¬ë ‰ì…˜ ëª©ë¡ ì¶œë ¥
        if args.list_collections:
            collections = stats.list_collections()
            print(f"\nğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ ({len(collections)}ê°œ):")
            for i, name in enumerate(collections, 1):
                print(f"  {i}. {name}")
        
        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        if args.info:
            print_collection_info(stats, args.collection)
        
        # í˜ì´ë¡œë“œ í†µê³„ ì¶œë ¥
        if args.stats:
            print_payload_stats(stats, args.collection)
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if args.sample:
            print_sample_points(stats, args.collection, args.sample)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        if args.search:
            print(f"\nğŸ” ê²€ìƒ‰ì–´: '{args.search}'")
            results = stats.search_similar(args.search, args.collection, limit=5)
            
            if results:
                print(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê±´):")
                for i, result in enumerate(results, 1):
                    payload = result['payload']
                    title = payload.get('title', 'No Title')
                    source = payload.get('source', 'Unknown')
                    
                    print(f"\n[{i}] {title} (ìœ ì‚¬ë„: {result['score']:.3f})")
                    print(f"    ì¶œì²˜: {source}")
                    print(f"    ë‚´ìš©: {payload.get('content', '')[:150]}...")
            else:
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # í•„í„° ê²€ìƒ‰ ì‹¤í–‰
        if args.filter:
            try:
                filter_conditions = json.loads(args.filter)
                print(f"\nğŸ” í•„í„° ê²€ìƒ‰: {filter_conditions}")
                results = stats.search_by_filter(filter_conditions, args.collection)
                
                if results:
                    print(f"ğŸ“„ í•„í„° ê²°ê³¼ ({len(results)}ê±´):")
                    for i, result in enumerate(results, 1):
                        payload = result['payload']
                        title = payload.get('title', 'No Title')
                        source = payload.get('source', 'Unknown')
                        
                        print(f"\n[{i}] {title}")
                        print(f"    ì¶œì²˜: {source}")
                else:
                    print("í•„í„° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            except json.JSONDecodeError:
                print("âŒ í•„í„° ì¡°ê±´ì€ ìœ íš¨í•œ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                print("ì˜ˆì‹œ: --filter '{\"source\": \"ë„¤ì´ë²„\", \"api_provider\": \"naver\"}'")
        
        # ëŒ€í™”í˜• ê²€ìƒ‰ ëª¨ë“œ
        if args.interactive:
            interactive_search(stats, args.collection)
    
    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()