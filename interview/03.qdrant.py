#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LIGNEX1 ë²¡í„°í™” ë° Qdrant ì €ì¥ ì‹œìŠ¤í…œ (Fallback ë²„ì „)
ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ Qdrantì— ì €ì¥í•©ë‹ˆë‹¤.
SentenceTransformer ë¡œë“œ ì‹¤íŒ¨ì‹œ TF-IDF ê¸°ë°˜ ë²¡í„°í™”ë¡œ ëŒ€ì²´
"""

import os
import json
import sqlite3
import logging
from datetime import datetime
from typing import List, Dict, Optional
import argparse
from dataclasses import dataclass
import hashlib

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np
import re
from collections import Counter

# Qdrant í´ë¼ì´ì–¸íŠ¸
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from qdrant_client.http import models

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('lignex1_vectorize.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class NewsDocument:
    """ë‰´ìŠ¤ ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    title: str
    content: str
    source: str
    url: str
    published_date: str
    keywords: List[str]
    api_provider: str
    api_type: str
    collected_at: str

class SimpleTfidfEmbedder:
    """TF-IDF ê¸°ë°˜ ê°„ë‹¨í•œ ì„ë² ë”© ì‹œìŠ¤í…œ (SentenceTransformer ëŒ€ì²´ìš©)"""
    
    def __init__(self, vector_size: int = 384):
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.decomposition import TruncatedSVD
            
            self.vector_size = vector_size
            self.vectorizer = TfidfVectorizer(
                max_features=2000,
                ngram_range=(1, 2),
                min_df=1,
                max_df=0.9,
                stop_words=None  # í•œêµ­ì–´ ì§€ì›ì„ ìœ„í•´ None
            )
            
            # ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ SVD
            self.svd = TruncatedSVD(n_components=min(vector_size, 384))
            self.is_fitted = False
            
            # í•œêµ­ì–´/ì˜ì–´ ë¶ˆìš©ì–´
            self.stopwords = {
                'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë˜ëŠ”', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ',
                'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ',
                'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
                'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',
                'after', 'above', 'below', 'between', 'among'
            }
            
            logger.info(f"TF-IDF ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ì°¨ì›: {vector_size})")
            
        except ImportError:
            logger.error("scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install scikit-learn")
            raise
    
    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¹€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ë¶ˆìš©ì–´ ì œê±°
        words = text.split()
        filtered_words = [word for word in words if word.lower() not in self.stopwords and len(word) > 1]
        
        return ' '.join(filtered_words)
    
    def fit(self, texts: List[str]):
        """í…ìŠ¤íŠ¸ ëª©ë¡ì— ëŒ€í•´ ëª¨ë¸ í›ˆë ¨"""
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = [self.preprocess_text(text) for text in texts]
        
        # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
        processed_texts = [text for text in processed_texts if text.strip()]
        
        if not processed_texts:
            logger.warning("ì „ì²˜ë¦¬ í›„ ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # TF-IDF ë²¡í„°í™”
        tfidf_matrix = self.vectorizer.fit_transform(processed_texts)
        
        # SVDë¡œ ì°¨ì› ì¶•ì†Œ
        if tfidf_matrix.shape[1] > self.vector_size:
            self.svd.fit(tfidf_matrix)
        
        self.is_fitted = True
        logger.info(f"TF-IDF ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {len(processed_texts)}ê°œ ë¬¸ì„œ")
    
    def encode(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if isinstance(text, str):
            texts = [text]
            single_input = True
        else:
            texts = text
            single_input = False
        
        if not self.is_fitted:
            # ê¸°ë³¸ í›ˆë ¨ ë°ì´í„°ë¡œ í›ˆë ¨
            default_texts = [
                "LIGë„¥ìŠ¤ì› ê¸°ì—…ë¬¸í™” OPEN POSITIVE",
                "ë°©ìœ„ì‚°ì—… êµ­ë°© ë¯¸ì‚¬ì¼ ë ˆì´ë”",
                "ì—°êµ¬ê°œë°œ R&D ê¸°ìˆ ê°œë°œ í˜ì‹ ",
                "ì„±ê³¼ ìˆ˜ì£¼ ë§¤ì¶œ ì„±ì¥ ì‹¤ì ",
                "ë¯¸ë˜ ë¹„ì „ ì „ëµ ê³„íš ëª©í‘œ"
            ] + texts
            self.fit(default_texts)
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = [self.preprocess_text(t) for t in texts]
        
        # TF-IDF ë³€í™˜
        tfidf_vectors = self.vectorizer.transform(processed_texts)
        
        # SVD ì°¨ì› ì¶•ì†Œ (í•„ìš”í•œ ê²½ìš°)
        if hasattr(self.svd, 'components_') and tfidf_vectors.shape[1] > self.vector_size:
            vectors = self.svd.transform(tfidf_vectors)
        else:
            vectors = tfidf_vectors.toarray()
        
        # ë²¡í„° í¬ê¸° ì¡°ì •
        if vectors.shape[1] < self.vector_size:
            # íŒ¨ë”© ì¶”ê°€
            padding = np.zeros((vectors.shape[0], self.vector_size - vectors.shape[1]))
            vectors = np.hstack([vectors, padding])
        elif vectors.shape[1] > self.vector_size:
            # ì˜ë¼ë‚´ê¸°
            vectors = vectors[:, :self.vector_size]
        
        # ì •ê·œí™”
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        norms[norms == 0] = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ ë°©ì§€
        vectors = vectors / norms
        
        return vectors[0] if single_input else vectors
    
    def get_sentence_embedding_dimension(self):
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.vector_size

class TextProcessor:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (í•œêµ­ì–´ + ì˜ì–´)
        self.stopwords = {
            'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ë˜ëŠ”', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ',
            'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ',
            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',
            'after', 'above', 'below', 'between', 'among'
        }
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¹€)
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text.strip())
        
        return text
    
    def extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        clean_text = self.clean_text(text)
        
        # ë‹¨ì–´ ë¶„ë¦¬ (ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜)
        words = clean_text.split()
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
        filtered_words = [
            word.lower() for word in words 
            if len(word) > 2 and word.lower() not in self.stopwords
        ]
        
        # ë¹ˆë„ ê³„ì‚°
        word_freq = Counter(filtered_words)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        return [word for word, freq in word_freq.most_common(top_k)]
    
    def create_searchable_content(self, title: str, description: str) -> str:
        """ê²€ìƒ‰ ê°€ëŠ¥í•œ í†µí•© ì»¨í…ì¸  ìƒì„±"""
        # ì œëª©ê³¼ ì„¤ëª…ì„ ê²°í•©
        combined_text = f"{title} {description}"
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_text = self.clean_text(combined_text)
        
        return cleaned_text

class QdrantVectorizer:
    """Qdrant ë²¡í„°í™” ë° ì €ì¥ í´ë˜ìŠ¤ (Fallback í¬í•¨)"""
    
    def __init__(self, 
                 qdrant_host: str = "localhost",
                 qdrant_port: int = 6333,
                 collection_name: str = "lignex1_news",
                 model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        
        self.collection_name = collection_name
        self.text_processor = TextProcessor()
        self.model_name = model_name
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.qdrant_client = QdrantClient(host=qdrant_host, port=qdrant_port)
            logger.info(f"Qdrant í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ: {qdrant_host}:{qdrant_port}")
        except Exception as e:
            logger.error(f"Qdrant ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (SentenceTransformer ìš°ì„ , ì‹¤íŒ¨ì‹œ TF-IDF)
        self.embedding_model = None
        self.vector_size = 384  # ê¸°ë³¸ê°’
        
        # ë¨¼ì € SentenceTransformer ì‹œë„
        try:
            from sentence_transformers import SentenceTransformer
            self.embedding_model = SentenceTransformer(model_name)
            self.vector_size = self.embedding_model.get_sentence_embedding_dimension()
            logger.info(f"SentenceTransformer ë¡œë“œ ì™„ë£Œ: {model_name} (ì°¨ì›: {self.vector_size})")
            self.use_sentence_transformer = True
        except Exception as e:
            logger.warning(f"SentenceTransformer ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("TF-IDF ê¸°ë°˜ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            
            # TF-IDF ê¸°ë°˜ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´
            try:
                self.embedding_model = SimpleTfidfEmbedder(vector_size=384)
                self.vector_size = self.embedding_model.get_sentence_embedding_dimension()
                logger.info(f"TF-IDF ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.vector_size})")
                self.use_sentence_transformer = False
            except Exception as e2:
                logger.error(f"TF-IDF ì„ë² ë”© ëª¨ë¸ ë¡œë“œë„ ì‹¤íŒ¨: {e2}")
                raise
    
    def create_collection(self, recreate: bool = False) -> bool:
        """Qdrant ì»¬ë ‰ì…˜ ìƒì„±"""
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸
            collections = self.qdrant_client.get_collections().collections
            collection_exists = any(c.name == self.collection_name for c in collections)
            
            if collection_exists and recreate:
                logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {self.collection_name}")
                self.qdrant_client.delete_collection(self.collection_name)
                collection_exists = False
            
            if not collection_exists:
                logger.info(f"ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {self.collection_name}")
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.vector_size,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name}")
            else:
                logger.info(f"ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©: {self.collection_name}")
            
            return True
            
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def vectorize_text(self, text: str) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë²¡í„°í™”"""
        try:
            # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            if not text or not text.strip():
                return np.zeros(self.vector_size)
            
            # ì„ë² ë”© ìƒì„±
            embedding = self.embedding_model.encode(text)
            
            # numpy arrayë¡œ ë³€í™˜
            if not isinstance(embedding, np.ndarray):
                embedding = np.array(embedding)
            
            return embedding
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë²¡í„°í™” ì‹¤íŒ¨: {e}")
            return np.zeros(self.vector_size)
    
    def load_news_from_db(self, db_path: str, limit: int = None) -> List[NewsDocument]:
        """SQLiteì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ"""
        news_docs = []
        
        try:
            with sqlite3.connect(db_path) as conn:
                query = '''
                    SELECT id, title, description, link, source, pub_date,
                           search_keyword, api_provider, api_type, collected_at
                    FROM articles 
                    WHERE title IS NOT NULL AND description IS NOT NULL
                    ORDER BY created_at DESC
                '''
                
                if limit:
                    query += f' LIMIT {limit}'
                
                cursor = conn.execute(query)
                rows = cursor.fetchall()
                
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {len(rows)}ê°œ ê¸°ì‚¬ ë¡œë“œ")
                
                for row in rows:
                    (id, title, description, link, source, pub_date, 
                     search_keyword, api_provider, api_type, collected_at) = row
                    
                    # ê²€ìƒ‰ ê°€ëŠ¥í•œ ì»¨í…ì¸  ìƒì„±
                    content = self.text_processor.create_searchable_content(title, description)
                    
                    # í‚¤ì›Œë“œ ì¶”ì¶œ
                    keywords = self.text_processor.extract_keywords(content)
                    if search_keyword and search_keyword not in keywords:
                        keywords.insert(0, search_keyword)
                    
                    # NewsDocument ê°ì²´ ìƒì„±
                    doc = NewsDocument(
                        id=str(id),
                        title=title,
                        content=content,
                        source=source or 'Unknown',
                        url=link or '',
                        published_date=pub_date or '',
                        keywords=keywords,
                        api_provider=api_provider or 'unknown',
                        api_type=api_type or 'unknown',
                        collected_at=collected_at or ''
                    )
                    
                    news_docs.append(doc)
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return news_docs
    
    def store_documents(self, documents: List[NewsDocument], batch_size: int = 100) -> bool:
        """ë¬¸ì„œë“¤ì„ Qdrantì— ì €ì¥"""
        try:
            total_docs = len(documents)
            logger.info(f"ì´ {total_docs}ê°œ ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥ ì‹œì‘...")
            
            # TF-IDF ëª¨ë¸ì˜ ê²½ìš° ëª¨ë“  í…ìŠ¤íŠ¸ë¡œ ë¨¼ì € í›ˆë ¨
            if not self.use_sentence_transformer:
                logger.info("TF-IDF ëª¨ë¸ í›ˆë ¨ ì¤‘...")
                all_texts = [doc.content for doc in documents if doc.content.strip()]
                self.embedding_model.fit(all_texts)
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, total_docs, batch_size):
                batch_docs = documents[i:i + batch_size]
                points = []
                
                for doc in batch_docs:
                    try:
                        # í…ìŠ¤íŠ¸ ë²¡í„°í™”
                        vector = self.vectorize_text(doc.content)
                        
                        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                        payload = {
                            "title": doc.title,
                            "content": doc.content[:1000],  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ì €ì¥
                            "source": doc.source,
                            "url": doc.url,
                            "published_date": doc.published_date,
                            "keywords": doc.keywords,
                            "api_provider": doc.api_provider,
                            "api_type": doc.api_type,
                            "collected_at": doc.collected_at,
                            "vector_created_at": datetime.now().isoformat(),
                            "embedding_model": "tfidf" if not self.use_sentence_transformer else self.model_name
                        }
                        
                        # Point ê°ì²´ ìƒì„±
                        point = PointStruct(
                            id=int(doc.id),
                            vector=vector.tolist(),
                            payload=payload
                        )
                        
                        points.append(point)
                        
                    except Exception as e:
                        logger.error(f"ë¬¸ì„œ {doc.id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                if points:
                    # ë°°ì¹˜ ì—…ë¡œë“œ
                    self.qdrant_client.upsert(
                        collection_name=self.collection_name,
                        points=points
                    )
                
                processed = min(i + batch_size, total_docs)
                logger.info(f"ì§„í–‰ë¥ : {processed}/{total_docs} ({processed/total_docs*100:.1f}%)")
            
            logger.info(f"âœ… ëª¨ë“  ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: {total_docs}ê°œ")
            return True
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def search_similar(self, query: str, limit: int = 5, score_threshold: float = 0.3) -> List[Dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (TF-IDFì˜ ê²½ìš° ë‚®ì€ threshold ì‚¬ìš©)"""
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_vector = self.vectorize_text(query)
            
            # TF-IDF ëª¨ë¸ì˜ ê²½ìš° ë” ë‚®ì€ threshold ì‚¬ìš©
            if not self.use_sentence_transformer:
                score_threshold = max(0.1, score_threshold * 0.3)
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_result = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_vector.tolist(),
                limit=limit,
                score_threshold=score_threshold
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for hit in search_result:
                result = {
                    'id': hit.id,
                    'score': hit.score,
                    'title': hit.payload.get('title', ''),
                    'content': hit.payload.get('content', ''),
                    'source': hit.payload.get('source', ''),
                    'url': hit.payload.get('url', ''),
                    'keywords': hit.payload.get('keywords', []),
                    'api_provider': hit.payload.get('api_provider', ''),
                    'published_date': hit.payload.get('published_date', ''),
                    'embedding_model': hit.payload.get('embedding_model', 'unknown')
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_collection_info(self) -> Dict:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            info = self.qdrant_client.get_collection(self.collection_name)
            
            return {
                'name': self.collection_name,
                'vectors_count': info.vectors_count,
                'indexed_vectors_count': info.indexed_vectors_count,
                'points_count': info.points_count,
                'status': info.status,
                'optimizer_status': info.optimizer_status,
                'disk_data_size': info.disk_data_size,
                'ram_data_size': info.ram_data_size,
                'embedding_model': 'tfidf' if not self.use_sentence_transformer else self.model_name
            }
            
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

class InterviewRAGSystem:
    """ë©´ì ‘ìš© RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, qdrant_vectorizer: QdrantVectorizer):
        self.vectorizer = qdrant_vectorizer
        
        # LIGë„¥ìŠ¤ì› ê´€ë ¨ ì£¼ìš” ì£¼ì œë“¤
        self.interview_topics = {
            "company_culture": ["ê¸°ì—…ë¬¸í™”", "í•µì‹¬ê°€ì¹˜", "ì¡°ì§ë¬¸í™”", "OPEN", "POSITIVE"],
            "business": ["ë°©ìœ„ì‚°ì—…", "êµ­ë°©", "ë¯¸ì‚¬ì¼", "ë ˆì´ë”", "ë¬´ê¸°ì²´ê³„"],
            "technology": ["R&D", "ì—°êµ¬ê°œë°œ", "ê¸°ìˆ ê°œë°œ", "í˜ì‹ ", "ì²¨ë‹¨ê¸°ìˆ "],
            "performance": ["ì„±ê³¼", "ìˆ˜ì£¼", "ë§¤ì¶œ", "ì„±ì¥", "ì‹¤ì "],
            "future": ["ë¯¸ë˜", "ë¹„ì „", "ì „ëµ", "ê³„íš", "ëª©í‘œ"]
        }
    
    def get_context_for_question(self, question_category: str, question_text: str) -> List[Dict]:
        """ë©´ì ‘ ì§ˆë¬¸ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì œê³µ"""
        # ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        topic_keywords = self.interview_topics.get(question_category, [])
        
        # ì§ˆë¬¸ í…ìŠ¤íŠ¸ì™€ ì£¼ì œ í‚¤ì›Œë“œë¥¼ ê²°í•©í•˜ì—¬ ê²€ìƒ‰
        search_queries = [question_text] + topic_keywords
        
        all_results = []
        for query in search_queries[:3]:  # ìƒìœ„ 3ê°œ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
            results = self.vectorizer.search_similar(query, limit=3, score_threshold=0.1)
            all_results.extend(results)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ìˆœ ì •ë ¬
        unique_results = {}
        for result in all_results:
            result_id = result['id']
            if result_id not in unique_results or result['score'] > unique_results[result_id]['score']:
                unique_results[result_id] = result
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œ ë°˜í™˜
        sorted_results = sorted(unique_results.values(), key=lambda x: x['score'], reverse=True)
        return sorted_results[:5]

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='LIGNEX1 ë²¡í„°í™” ë° Qdrant ì €ì¥ (Fallback ë²„ì „)')
    parser.add_argument('--db-path', default='lignex1_data/lignex1_articles.db', help='SQLite ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ')
    parser.add_argument('--qdrant-host', default='localhost', help='Qdrant í˜¸ìŠ¤íŠ¸')
    parser.add_argument('--qdrant-port', type=int, default=6333, help='Qdrant í¬íŠ¸')
    parser.add_argument('--collection', default='lignex1_news', help='Qdrant ì»¬ë ‰ì…˜ ì´ë¦„')
    parser.add_argument('--recreate', action='store_true', help='ì»¬ë ‰ì…˜ ì¬ìƒì„±')
    parser.add_argument('--limit', type=int, help='ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ ì œí•œ')
    parser.add_argument('--batch-size', type=int, default=50, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--search', type=str, help='í…ŒìŠ¤íŠ¸ ê²€ìƒ‰')
    parser.add_argument('--info', action='store_true', help='ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥')
    
    args = parser.parse_args()
    
    # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ í™•ì¸
    if not os.path.exists(args.db_path):
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.db_path}")
        print("ë¨¼ì € 01.extract.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤...")
        try:
            vectorizer = QdrantVectorizer(
                qdrant_host=args.qdrant_host,
                qdrant_port=args.qdrant_port,
                collection_name=args.collection
            )
            
            if vectorizer.create_collection(recreate=args.recreate):
                print("âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ!")
                
                # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰
                if args.search:
                    print(f"\nğŸ” ê²€ìƒ‰ì–´: '{args.search}'")
                    print("(í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ì–´ ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                
                # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
                if args.info:
                    info = vectorizer.get_collection_info()
                    print("\nğŸ“Š Qdrant ì»¬ë ‰ì…˜ ì •ë³´:")
                    print(json.dumps(info, indent=2, ensure_ascii=False))
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return
    
    try:
        # Qdrant ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”
        vectorizer = QdrantVectorizer(
            qdrant_host=args.qdrant_host,
            qdrant_port=args.qdrant_port,
            collection_name=args.collection
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        if not vectorizer.create_collection(recreate=args.recreate):
            return
        
        # ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        if args.info:
            info = vectorizer.get_collection_info()
            print("\nğŸ“Š Qdrant ì»¬ë ‰ì…˜ ì •ë³´:")
            print(json.dumps(info, indent=2, ensure_ascii=False))
            return
        
        # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
        if args.search:
            print(f"\nğŸ” ê²€ìƒ‰ì–´: '{args.search}'")
            results = vectorizer.search_similar(args.search, limit=5)
            
            if results:
                print(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê±´):")
                for i, result in enumerate(results, 1):
                    print(f"\n[{i}] {result['title']} (ìœ ì‚¬ë„: {result['score']:.3f})")
                    print(f"ì¶œì²˜: {result['source']} | ì œê³µ: {result['api_provider']}")
                    print(f"ì„ë² ë”©: {result.get('embedding_model', 'unknown')}")
                    print(f"ë‚´ìš©: {result['content'][:150]}...")
            else:
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
        print("ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ì¤‘...")
        documents = vectorizer.load_news_from_db(args.db_path, limit=args.limit)
        
        if not documents:
            print("âŒ ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë²¡í„°í™” ë° ì €ì¥
        embedding_type = "TF-IDF" if not vectorizer.use_sentence_transformer else "SentenceTransformer"
        print(f"ğŸ”„ {len(documents)}ê°œ ë¬¸ì„œ ë²¡í„°í™” ë° ì €ì¥ ì‹œì‘... ({embedding_type} ì‚¬ìš©)")
        success = vectorizer.store_documents(documents, batch_size=args.batch_size)
        
        if success:
            # ìµœì¢… ì •ë³´ ì¶œë ¥
            info = vectorizer.get_collection_info()
            print(f"\nâœ… ë²¡í„°í™” ì™„ë£Œ!")
            print(f"ğŸ“Š ì €ì¥ëœ ë²¡í„° ìˆ˜: {info.get('vectors_count', 0):,}")
            print(f"ğŸ’¾ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰: {info.get('disk_data_size', 0):,} bytes")
            print(f"ğŸ§  RAM ì‚¬ìš©ëŸ‰: {info.get('ram_data_size', 0):,} bytes")
            print(f"ğŸ¤– ì„ë² ë”© ëª¨ë¸: {info.get('embedding_model', 'unknown')}")
            
            # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰
            print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰...")
            test_queries = ["LIGë„¥ìŠ¤ì› ê¸°ì—…ë¬¸í™”", "ë°©ìœ„ì‚°ì—… ê¸°ìˆ ê°œë°œ", "ë¯¸ì‚¬ì¼ ì—°êµ¬ê°œë°œ"]
            
            for query in test_queries:
                results = vectorizer.search_similar(query, limit=2, score_threshold=0.1)
                print(f"\n'{query}' ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê±´")
                for result in results:
                    print(f"  - {result['title']} (ìœ ì‚¬ë„: {result['score']:.3f})")
        
        else:
            print("âŒ ë²¡í„°í™” ì‹¤íŒ¨")
    
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")