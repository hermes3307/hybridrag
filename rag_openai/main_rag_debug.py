#!/usr/bin/env python3
"""
ğŸ¤– Conversational Document Processing Assistant with RAG
Main interface for natural language document processing and intelligent Q&A
"""
import openai
import os
from dotenv import load_dotenv
import asyncio

# ğŸš€ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì¶”ê°€
load_dotenv()

import os
import sys
import time
import json
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import re
from collections import Counter

# Import our modules
from web_downloader import WebDocumentDownloader
from smart_chunker import SmartDocumentChunker  
from vector_manager import VectorStoreManager
from query_engine import ConversationalQueryEngine
from status import StatusManager

# ğŸ” RAG ë””ë²„ê±° ì¶”ê°€
try:
    from debug_rag import RAGProcessDebugger
    RAG_DEBUG_AVAILABLE = True
    print("âœ… RAG Debug module loaded successfully")
except ImportError as e:
    print(f"âš ï¸ RAG Debug module not available: {e}")
    RAG_DEBUG_AVAILABLE = False

# Download required NLTK data with better compatibility
import nltk

def download_nltk_data():
    """Download required NLTK data with fallback for different versions"""
    required_data = [
        ('tokenizers/punkt', 'punkt'),
        ('tokenizers/punkt_tab', 'punkt_tab'),  # New format
        ('corpora/stopwords', 'stopwords'),
        ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger')
    ]
    
    for data_path, download_name in required_data:
        try:
            nltk.data.find(data_path)
            print(f"âœ… {download_name} already available")
        except LookupError:
            try:
                print(f"ğŸ“¥ Downloading {download_name}...")
                nltk.download(download_name, quiet=True)
                print(f"âœ… {download_name} downloaded successfully")
            except Exception as e:
                print(f"âš ï¸ Could not download {download_name}: {e}")
                if download_name == 'punkt_tab':
                    print("   Falling back to punkt tokenizer")

# Download NLTK data at startup
download_nltk_data()

try:
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.tag import pos_tag
    print("âœ… NLTK components loaded successfully")
except ImportError as e:
    print(f"âš ï¸ NLTK import error: {e}")
    # Provide fallback implementations
    def word_tokenize(text):
        return text.lower().split()
    
    def sent_tokenize(text):
        import re
        return re.split(r'[.!?]+', text)
    
    def pos_tag(tokens):
        return [(token, 'NN') for token in tokens]  # Default to noun
    
    class MockStopwords:
        def words(self, lang):
            return {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall', 'this', 'that', 'these', 'those'}
    
    stopwords = MockStopwords()
    print("âš ï¸ Using fallback NLTK implementations")


class TopicKeywordExtractor:
    """ğŸ§  Intelligent topic and keyword extraction from user questions"""
    
    def __init__(self):
        try:
            self.stop_words = set(stopwords.words('english'))
        except Exception:
            # Fallback stopwords
            self.stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall', 'this', 'that', 'these', 'those'}
        
        # Technical domain keywords for better extraction
        self.domain_keywords = {
            'database': ['database', 'db', 'sql', 'table', 'index', 'query', 'schema', 'constraint',
                        'transaction', 'backup', 'restore', 'replication', 'partition', 'view', 'trigger'],
            'performance': ['performance', 'optimization', 'tuning', 'slow', 'fast', 'memory',
                           'cpu', 'disk', 'cache', 'bottleneck', 'latency', 'throughput', 'speed'],
            'configuration': ['config', 'setting', 'parameter', 'option', 'configure', 'setup',
                             'installation', 'deployment', 'environment', 'property'],
            'security': ['security', 'authentication', 'authorization', 'permission', 'user',
                        'role', 'privilege', 'encryption', 'ssl', 'certificate', 'password'],
            'network': ['network', 'connection', 'port', 'protocol', 'tcp', 'ip', 'socket',
                       'communication', 'client', 'server', 'connection'],
            'storage': ['storage', 'disk', 'file', 'directory', 'backup', 'archive', 'volume',
                       'tablespace', 'datafile', 'memory', 'buffer'],
            'altibase': ['altibase', 'ipcda', 'apre', 'isql', 'iloader', 'altiadmin', 'altimon',
                        'hybrid', 'inmemory', 'diskdb', 'memorydb', 'sharding']
        }
        
        # Question patterns that indicate need for detailed explanation
        self.question_patterns = {
            'what_is': [r'what\s+is\s+(.+)', r'what\s+are\s+(.+)', r'define\s+(.+)', r'explain\s+(.+)'],
            'how_to': [r'how\s+to\s+(.+)', r'how\s+can\s+i\s+(.+)', r'how\s+do\s+i\s+(.+)', r'steps\s+to\s+(.+)'],
            'why': [r'why\s+(.+)', r'what\s+causes\s+(.+)', r'reason\s+for\s+(.+)'],
            'when': [r'when\s+(.+)', r'what\s+time\s+(.+)', r'at\s+what\s+point\s+(.+)'],
            'where': [r'where\s+(.+)', r'in\s+which\s+(.+)', r'location\s+of\s+(.+)'],
            'troubleshoot': [r'error\s+(.+)', r'problem\s+(.+)', r'issue\s+(.+)', r'fix\s+(.+)', r'solve\s+(.+)'],
            'best_practices': [r'best\s+practices?\s+(.+)', r'recommended\s+(.+)', r'optimal\s+(.+)'],
            'comparison': [r'difference\s+between\s+(.+)', r'compare\s+(.+)', r'(.+)\s+vs\s+(.+)']
        }

    def extract_topics_and_keywords(self, text: str) -> Dict[str, Any]:
        """ğŸ” Extract topics, keywords, and question type from user input"""
        text_lower = text.lower().strip()
        
        # 1. Detect question type
        question_type = self._detect_question_type(text_lower)
        
        # 2. Extract keywords using NLP
        keywords = self._extract_keywords_nlp(text)
        
        # 3. Identify domain topics
        domains = self._identify_domains(text_lower, keywords)
        
        # 4. Extract technical terms
        technical_terms = self._extract_technical_terms(text_lower)
        
        # 5. Build search queries
        search_queries = self._build_search_queries(keywords, technical_terms, question_type)
        
        return {
            'original_text': text,
            'question_type': question_type,
            'keywords': keywords,
            'technical_terms': technical_terms,
            'domains': domains,
            'search_queries': search_queries,
            'is_complex_question': self._is_complex_question(text_lower, question_type)
        }

    def _detect_question_type(self, text: str) -> str:
        """Detect the type of question being asked"""
        for q_type, patterns in self.question_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return q_type
        return 'general'

    def _extract_keywords_nlp(self, text: str) -> List[str]:
        """Extract keywords using NLP techniques with fallback"""
        try:
            # Tokenize and POS tag
            tokens = word_tokenize(text.lower())
            pos_tags = pos_tag(tokens)
            
            # Extract meaningful words (nouns, adjectives, verbs)
            meaningful_words = []
            for word, pos in pos_tags:
                if (pos.startswith('NN') or pos.startswith('JJ') or pos.startswith('VB')) and \
                   word not in self.stop_words and len(word) > 2 and word.isalpha():
                    meaningful_words.append(word)
            
            # Count frequency and return top keywords
            word_freq = Counter(meaningful_words)
            return [word for word, freq in word_freq.most_common(10)]
            
        except Exception as e:
            print(f"âš ï¸ NLP extraction failed, using simple fallback: {e}")
            # Simple fallback: just split and filter
            words = text.lower().split()
            return [word for word in words if word not in self.stop_words and len(word) > 2 and word.isalpha()][:10]

    def _identify_domains(self, text: str, keywords: List[str]) -> List[str]:
        """Identify technical domains mentioned in the text"""
        domains = []
        all_words = text.split() + keywords
        
        for domain, domain_words in self.domain_keywords.items():
            if any(word in all_words for word in domain_words):
                domains.append(domain)
        
        return domains

    def _extract_technical_terms(self, text: str) -> List[str]:
        """Extract technical terms and acronyms"""
        # Find acronyms (2+ uppercase letters)
        acronyms = re.findall(r'\b[A-Z]{2,}\b', text.upper())
        
        # Find technical terms from domain keywords
        technical_terms = []
        words = text.split()
        
        for domain_words in self.domain_keywords.values():
            for word in words:
                if word.lower() in domain_words:
                    technical_terms.append(word.lower())
        
        return list(set(acronyms + technical_terms))

    def _build_search_queries(self, keywords: List[str], technical_terms: List[str], 
                             question_type: str) -> List[str]:
        """Build multiple search queries for comprehensive retrieval"""
        queries = []
        
        # Main query with all keywords
        if keywords:
            main_query = ' '.join(keywords[:5])  # Top 5 keywords
            queries.append(main_query)
        
        # Technical terms query
        if technical_terms:
            tech_query = ' '.join(technical_terms)
            queries.append(tech_query)
        
        # Question-type specific queries
        if question_type == 'how_to':
            if keywords:
                queries.append(f"tutorial {' '.join(keywords[:3])}")
                queries.append(f"guide {' '.join(keywords[:3])}")
        elif question_type == 'troubleshoot':
            if keywords:
                queries.append(f"error {' '.join(keywords[:3])}")
                queries.append(f"problem {' '.join(keywords[:3])}")
        elif question_type == 'what_is':
            if keywords:
                queries.append(f"definition {' '.join(keywords[:3])}")
        
        return list(set(queries))  # Remove duplicates

    def _is_complex_question(self, text: str, question_type: str) -> bool:
        """Determine if this is a complex question needing RAG"""
        # Questions with multiple parts
        if len(text.split('?')) > 2:
            return True
        
        # Long questions
        if len(text.split()) > 8:
            return True
        
        # Specific question types that need detailed answers
        complex_types = ['how_to', 'why', 'troubleshoot', 'best_practices', 'comparison']
        if question_type in complex_types:
            return True
        
        # Contains technical terms
        if any(domain_words for domain_words in self.domain_keywords.values() 
               if any(word in text for word in domain_words)):
            return True
        
        return False


class RAGEngine:
    """ğŸ§  Retrieval Augmented Generation Engine"""
    
    def __init__(self, vector_manager: VectorStoreManager, query_engine: ConversationalQueryEngine):
        self.vector_manager = vector_manager
        self.query_engine = query_engine
        self.topic_extractor = TopicKeywordExtractor()
        
        # RAG prompts for different question types
        self.rag_prompts = {
            'what_is': """Based on the following context from technical documentation, provide a comprehensive explanation of what {topic} is:

Context:
{context}

Question: {question}

Please provide a detailed explanation that includes:
1. Definition and overview
2. Key characteristics or features
3. Use cases or applications
4. Any important technical details mentioned in the context

Answer:""",
            
            'how_to': """Based on the following context from technical documentation, provide step-by-step guidance on how to {topic}:

Context:
{context}

Question: {question}

Please provide:
1. Clear step-by-step instructions
2. Prerequisites or requirements
3. Important notes or warnings
4. Expected outcomes

Answer:""",
            
            'troubleshoot': """Based on the following context from technical documentation, help troubleshoot the issue with {topic}:

Context:
{context}

Question: {question}

Please provide:
1. Possible causes of the problem
2. Diagnostic steps
3. Solutions or workarounds
4. Prevention measures

Answer:""",
            
            'general': """Based on the following context from technical documentation, please answer the question about {topic}:

Context:
{context}

Question: {question}

Please provide a comprehensive answer based on the documentation context above.

Answer:"""
        }

        # ğŸš€ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ í™•ì¸ í¬í•¨)
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            self.openai_client = openai.OpenAI(api_key=api_key)
            print("âœ… OpenAI client initialized")
        else:
            print("âš ï¸ OPENAI_API_KEY not found in environment variables")
            self.openai_client = None
            

    async def _generate_llm_response(self, question: str, context: str, question_type: str, main_topic: str) -> str:
        """ğŸ¤– OpenAI GPTë¡œ ì‹¤ì œ ì‘ë‹µ ìƒì„±"""
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ í™•ì¸
        if not self.openai_client:
            print("âš ï¸ OpenAI client not available")
            return self._create_structured_response_fallback(question, context, question_type, main_topic)
        
        # ì§ˆë¬¸ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ ğŸ¯
        prompt_template = self.rag_prompts.get(question_type, self.rag_prompts['general'])
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„± ğŸ“
        final_prompt = prompt_template.format(
            topic=main_topic,
            context=context,
            question=question
        )
        
        try:
            # ğŸš€ OpenAI API í˜¸ì¶œ
            response = await asyncio.to_thread(
                self.openai_client.chat.completions.create,
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": """ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
                        ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
                        ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•´ ì¹œê·¼í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
                    },
                    {
                        "role": "user", 
                        "content": final_prompt
                    }
                ],
                temperature=0.3,
                max_tokens=1500,
                top_p=1.0
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"âš ï¸ OpenAI API ì˜¤ë¥˜: {e}")
            return self._create_structured_response_fallback(question, context, question_type, main_topic)

    def _create_structured_response_fallback(self, question: str, context: str, question_type: str, main_topic: str) -> str:
        """ğŸ›¡ï¸ OpenAI ì‹¤íŒ¨ì‹œ í´ë°± ì‘ë‹µ"""
        return f"""ğŸ¤– OpenAI ì—°ê²°ì— ë¬¸ì œê°€ ìˆì–´ ê°„ë‹¨í•œ ì‘ë‹µì„ ë“œë ¤ìš”:

ğŸ“„ **{main_topic}ì— ëŒ€í•œ ì •ë³´:**
{context[:500]}...

ğŸ’¡ ë” ìì„¸í•œ ë‹µë³€ì„ ìœ„í•´ OpenAI API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”!"""        

    async def generate_rag_response(self, question: str, max_context_length: int = 4000) -> str:
        """ğŸ¯ Generate RAG response for complex questions"""
        
        try:
            # Extract topics and keywords
            extraction_result = self.topic_extractor.extract_topics_and_keywords(question)
            
            if not extraction_result['is_complex_question']:
                return None  # Let normal search handle simple questions
            
            print(f"ğŸ§  RAG Analysis: {extraction_result['question_type']} question about {extraction_result['domains']}")
            print(f"ğŸ” Keywords: {extraction_result['keywords']}")
            print(f"âš™ï¸ Technical terms: {extraction_result['technical_terms']}")
            
            # Perform multiple searches for comprehensive context
            all_results = []
            
            for query in extraction_result['search_queries']:
                if query.strip():
                    try:
                        results = await self.vector_manager.search(query, k=5)
                        all_results.extend(results)
                        print(f"ğŸ“Š Found {len(results)} results for query: '{query}'")
                    except Exception as e:
                        print(f"âš ï¸ Search failed for query '{query}': {e}")
                        continue
            
            if not all_results:
                return f"ğŸ¤– I understand you're asking about {', '.join(extraction_result['keywords'])}, but I couldn't find relevant information in the indexed documents. Could you try rephrasing your question or check if the documents have been properly indexed?"
            
            # Deduplicate and rank results
            unique_results = self._deduplicate_results(all_results)
            top_results = sorted(unique_results, key=lambda x: x['score'], reverse=True)[:8]
            
            # Build context from top results
            context = self._build_context(top_results, max_context_length)
            
            # Generate RAG response
            rag_response = await self._generate_response_with_context(
                question, 
                context, 
                extraction_result,
                top_results
            )
            
            return rag_response
            
        except Exception as e:
            print(f"âš ï¸ RAG processing failed: {e}")
            return f"ğŸ¤– I encountered an issue processing your question. Let me try a simpler search approach. Error: {str(e)}"

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """Remove duplicate results based on content similarity"""
        unique_results = []
        seen_content = set()
        
        for result in results:
            content = result.get('payload', {}).get('text', '')[:200]  # First 200 chars
            content_hash = hash(content)
            
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append(result)
        
        return unique_results

    def _build_context(self, results: List[Dict], max_length: int) -> str:
        """Build context string from search results"""
        context_parts = []
        current_length = 0
        
        for i, result in enumerate(results, 1):
            payload = result.get('payload', {})
            text = payload.get('text', '').strip()
            source = payload.get('source_file', 'Unknown')
            
            if text and current_length < max_length:
                # Add source attribution
                context_part = f"[Source {i}: {source}]\n{text}\n"
                
                if current_length + len(context_part) <= max_length:
                    context_parts.append(context_part)
                    current_length += len(context_part)
                else:
                    # Add truncated version
                    remaining_space = max_length - current_length - 100  # Leave some buffer
                    if remaining_space > 100:
                        truncated_text = text[:remaining_space] + "..."
                        context_parts.append(f"[Source {i}: {source}]\n{truncated_text}\n")
                    break
        
        return "\n".join(context_parts)

    async def _generate_response_with_context(self, question: str, context: str, 
                                            extraction_result: Dict, results: List[Dict]) -> str:
        """Generate the final RAG response"""
        
        question_type = extraction_result['question_type']
        keywords = extraction_result['keywords']
        main_topic = ', '.join(keywords[:3]) if keywords else 'ì£¼ì œ'
        
        # ğŸš€ OpenAIë¡œ ì‘ë‹µ ìƒì„± ì‹œë„
        llm_response = await self._generate_llm_response(question, context, question_type, main_topic)
        
        # ğŸ“š ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
        sources_info = self._format_sources(results)
        
        final_response = f"""{llm_response}

{sources_info}

ğŸ’¡ **ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!** ğŸ˜Š"""
        
        return final_response

    def _format_sources(self, results: List[Dict]) -> str:
        """ğŸ“š ì†ŒìŠ¤ ì •ë³´ í¬ë§·íŒ…"""
        sources = set()
        for result in results[:5]:
            source = result.get('payload', {}).get('source_file', 'Unknown')
            if source != 'Unknown':
                sources.add(source)
        
        if not sources:
            return ""
        
        sources_text = "\nğŸ“š **ì°¸ê³  ë¬¸ì„œ:**\n"
        for i, source in enumerate(sources, 1):
            sources_text += f"   {i}. ğŸ“„ {source}\n"
        
        return sources_text


@dataclass
class ConversationState:
    """Tracks the current conversation state"""
    current_url: Optional[str] = None
    downloaded_files: List[str] = None
    discovered_documents: List = None
    processed_chunks: int = 0
    vector_store_ready: bool = False
    user_preferences: Dict = None
    status_manager: StatusManager = None
    
    def __post_init__(self):
        if self.downloaded_files is None:
            self.downloaded_files = []
        if self.discovered_documents is None:
            self.discovered_documents = []
        if self.user_preferences is None:
            self.user_preferences = {
                'chunk_size': 1000,
                'overlap': 200,
                'preferred_formats': ['pdf', 'doc', 'docx', 'txt'],
                'language': 'auto'
            }


class ConversationalAssistant:
    """ğŸ­ The main conversational assistant with RAG capabilities"""
    
    def __init__(self):
        print("ğŸš€ Initializing Conversational Document Assistant with RAG...")
        
        # Intent patterns for document processing commands
        self.command_patterns = {
            'download', 'scan', 'fetch', 'get documents',
            'chunk', 'process files', 'split documents',
            'index', 'build vector', 'create embeddings', 'make searchable',
            'status', 'progress', 'show files',
            'help', 'commands'
        }
        
        # Initialize components
        self.downloader = WebDocumentDownloader()

        # Chunker ì´ˆê¸°í™”
        try:
            from smart_chunker import SmartDocumentChunker
            self.chunker = SmartDocumentChunker()
        except ImportError:
            print("âš ï¸ SmartDocumentChunker not available")
            self.chunker = None
        
        # Initialize vector manager and query engine
        try:
            self.vector_manager = VectorStoreManager()
            self.query_engine = ConversationalQueryEngine(self.vector_manager)
            self.rag_engine = RAGEngine(self.vector_manager, self.query_engine)
            print("âœ… Vector search and RAG components initialized")
        except Exception as e:
            print(f"âš ï¸ Vector search and RAG not available: {e}")
            self.vector_manager = None
            self.query_engine = None
            self.rag_engine = None
        
        # ğŸ” RAG ë””ë²„ê±° ì´ˆê¸°í™”
        if RAG_DEBUG_AVAILABLE and self.vector_manager and self.query_engine:
            try:
                self.rag_debugger = RAGProcessDebugger(self.vector_manager, self.query_engine)
                print("âœ… RAG debugger initialized")
            except Exception as e:
                print(f"âš ï¸ RAG debugger initialization failed: {e}")
                self.rag_debugger = None
        else:
            self.rag_debugger = None
        
        # StatusManager ì´ˆê¸°í™”
        self.status_manager = StatusManager("processing_status.json")
        print("ğŸ“Š Status manager initialized - loading previous session data...")
        
        # Conversation state
        self.state = ConversationState()
        self.state.status_manager = self.status_manager
        
        # Conversation history
        self.conversation_history = []
        
        # ì €ì¥ëœ ìƒíƒœì—ì„œ ê¸°ë³¸ ì •ë³´ ë³µì›
        self._restore_session_state()
        
        print("âœ… Assistant ready with RAG capabilities! Let's chat! ğŸ’¬")

    def _restore_session_state(self):
        """ğŸ’¾ ì´ì „ ì„¸ì…˜ ìƒíƒœ ë³µì›"""
        try:
            current_status = self.status_manager.current_status
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ë³µì›
            download_status = current_status.get('download_status')
            if download_status and download_status.get('file_list'):
                existing_files = []
                for file_path in download_status['file_list']:
                    if os.path.exists(file_path):
                        existing_files.append(file_path)
                
                self.state.downloaded_files = existing_files
                print(f"ğŸ“¥ Restored {len(existing_files)} downloaded files from previous session")
            
            # ìµœê·¼ URL ë³µì›
            url_history = current_status.get('url_history', [])
            if url_history:
                self.state.current_url = url_history[-1]['url']
                print(f"ğŸŒ Restored last URL: {self.state.current_url}")
            
            # ì²­í¬ ìƒíƒœ ë³µì›
            chunk_status = current_status.get('chunk_status')
            if chunk_status:
                self.state.processed_chunks = chunk_status.get('total_chunks', 0)
                print(f"ğŸ§© Previous session had {self.state.processed_chunks} chunks")
            
            # ë²¡í„° ìƒíƒœ ë³µì›
            vector_status = current_status.get('vector_status')
            if vector_status:
                self.state.vector_store_ready = vector_status.get('is_ready', False)
                if self.state.vector_store_ready:
                    print(f"ğŸ—„ï¸ Vector database was ready - RAG is available!")
            
        except Exception as e:
            print(f"âš ï¸ Could not fully restore previous session: {e}")

    def _is_document_command(self, user_input: str) -> bool:
        """ğŸ” Check if user input is a document processing command"""
        user_input_lower = user_input.lower()
        
        # Check for explicit command keywords
        command_keywords = [
            'download', 'scan', 'fetch', 'get documents', 'get files',
            'chunk', 'process files', 'split documents', 'break into pieces',
            'index', 'build vector', 'create embeddings', 'make searchable',
            'status', 'progress', 'show files', 'check status',
            'help', 'commands', 'what can you do'
        ]
        
        return any(keyword in user_input_lower for keyword in command_keywords)

    def _is_debug_command(self, user_input: str) -> bool:
        """ğŸ” Check if user input is a RAG debug command"""
        debug_keywords = [
            'debug rag', 'show rag process', 'rag debug', 'debug process',
            'show process', 'explain rag', 'how rag works', 'rag analysis',
            'debug question', 'analyze question', 'rag step'
        ]
        
        user_input_lower = user_input.lower()
        return any(keyword in user_input_lower for keyword in debug_keywords)

    async def chat(self, user_input: str) -> str:
        """ğŸ’¬ Main chat interface with RAG capabilities"""
        if not user_input.strip():
            return "ğŸ¤– I'm here to help! Ask me questions about your documents or give me commands to process new ones."
        
        user_input = user_input.strip()
        
        # Store conversation
        self.conversation_history.append({
            'user': user_input,
            'timestamp': time.time()
        })

        # ğŸ” Check if this is a RAG debug command
        if self._is_debug_command(user_input):
            print("ğŸ” Processing as RAG debug command...")
            response = await self._handle_debug_command(user_input)
        
        # Check if this is a document processing command
        elif self._is_document_command(user_input):
            print("ğŸ”§ Processing as document command...")
            response = await self._handle_document_command(user_input)
        
        # Check if vector database is ready for RAG
        elif self.state.vector_store_ready and self.rag_engine:
            print("ğŸ§  Processing with RAG engine...")
            rag_response = await self.rag_engine.generate_rag_response(user_input)
            
            if rag_response:
                response = rag_response
            else:
                # Fall back to simple vector search
                print("ğŸ” Falling back to simple vector search...")
                response = await self._handle_simple_search(user_input)
        
        # Vector database not ready
        else:
            response = ("ğŸ¤– I'd love to answer your question, but I need to have documents indexed first! "
                       "Try asking me to 'download documents' or 'check status' to see what's available.")
        
        # Store response
        self.conversation_history[-1]['assistant'] = response
        
        return response

    async def _handle_debug_command(self, user_input: str) -> str:
        """ğŸ” Handle RAG debugging commands"""
        
        if not RAG_DEBUG_AVAILABLE:
            return ("ğŸ” RAG debugging feature is not available! \n\n"
                   "ğŸ’¡ To enable RAG debugging:\n"
                   "   1. Make sure debug_rag.py is in the same directory\n"
                   "   2. Install required dependencies\n"
                   "   3. Restart the application")
        
        if not self.rag_debugger:
            return ("ğŸ” RAG debugger is not initialized! \n\n"
                   "ğŸ’¡ Please make sure:\n"
                   "   â€¢ Vector database is available\n"
                   "   â€¢ Query engine is initialized\n"
                   "   â€¢ Try restarting the application")
        
        user_lower = user_input.lower()
        
        # Extract question from debug command
        question_to_debug = None
        
        # Pattern: "debug rag: question"
        if ':' in user_input:
            question_to_debug = user_input.split(':', 1)[1].strip()
        
        # Pattern: "show rag process for question"
        elif 'for' in user_lower:
            parts = user_input.split('for', 1)
            if len(parts) > 1:
                question_to_debug = parts[1].strip()
        
        # Default demo questions
        if not question_to_debug:
            demo_questions = [
                "IPCDAê°€ ë­ì•¼?",
                "How to configure database performance?",
                "Altibase troubleshooting guide"
            ]
            
            return f"""ğŸ” **RAG ë””ë²„ê¹… ë°ëª¨ë¥¼ ì‹¤í–‰í• ê²Œìš”!**

ì‚¬ìš©ë²•:
â€¢ "debug rag: IPCDAê°€ ë­ì•¼?" - íŠ¹ì • ì§ˆë¬¸ ë””ë²„ê¹…
â€¢ "show rag process for database configuration" - íŠ¹ì • ì£¼ì œ ë¶„ì„
â€¢ "rag analysis" - ë°ëª¨ ì‹¤í–‰

ğŸ¯ ë°ëª¨ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...

{await self._run_rag_debug_demo(demo_questions)}"""
        
        # Debug specific question
        try:
            print(f"ğŸ” Starting RAG debug for question: {question_to_debug}")
            debug_result = await self.rag_debugger.debug_rag_process(question_to_debug, show_details=True)
            
            if debug_result.success:
                return f"""ğŸ‰ **RAG ë””ë²„ê¹… ì™„ë£Œ!**

â“ **ë¶„ì„ëœ ì§ˆë¬¸**: {question_to_debug}
â±ï¸ **ì´ ì²˜ë¦¬ ì‹œê°„**: {debug_result.total_processing_time:.3f}ì´ˆ
âœ… **ì²˜ë¦¬ ìƒíƒœ**: ì„±ê³µ
ğŸ“Š **ì²˜ë¦¬ ë‹¨ê³„**: {len(debug_result.steps)}ë‹¨ê³„

ğŸ” **ìƒì„¸ ë¶„ì„ ê²°ê³¼ëŠ” ì½˜ì†”ì—ì„œ í™•ì¸í•˜ì„¸ìš”!**

ğŸ’¡ **íŒ**: 
â€¢ ë” ìì„¸í•œ ë¶„ì„ì„ ìœ„í•´ "debug rag: ë‹¤ë¥¸ ì§ˆë¬¸" ì‹œë„í•´ë³´ì„¸ìš”
â€¢ ì—¬ëŸ¬ ì§ˆë¬¸ ë¹„êµëŠ” consoleì—ì„œ debug_multiple_questions() ì‚¬ìš©"""
            else:
                return f"""âŒ **RAG ë””ë²„ê¹… ì‹¤íŒ¨**

â“ **ì§ˆë¬¸**: {question_to_debug}
ğŸš¨ **ì˜¤ë¥˜**: {debug_result.error_message}
â±ï¸ **ì²˜ë¦¬ ì‹œê°„**: {debug_result.total_processing_time:.3f}ì´ˆ

ğŸ’¡ **í•´ê²° ë°©ë²•**:
â€¢ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
â€¢ ë¬¸ì„œê°€ ì¸ë±ì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸
â€¢ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        
        except Exception as e:
            return f"""âŒ **RAG ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜ ë°œìƒ**

ğŸš¨ **ì˜¤ë¥˜**: {str(e)}

ğŸ’¡ **í•´ê²° ë°©ë²•**:
â€¢ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸: 'status' ëª…ë ¹ ì‹¤í–‰
â€¢ í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ í™•ì¸
â€¢ ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ì¬ì‹œì‘ ì‹œë„"""

    async def _run_rag_debug_demo(self, questions: List[str]) -> str:
        """ğŸ¯ RAG ë””ë²„ê¹… ë°ëª¨ ì‹¤í–‰"""
        
        try:
            # ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ê°„ë‹¨í•œ ë°ëª¨
            demo_question = questions[0]
            
            print(f"\nğŸ¯ RAG ë””ë²„ê¹… ë°ëª¨ - ì§ˆë¬¸: {demo_question}")
            debug_result = await self.rag_debugger.debug_rag_process(demo_question, show_details=False)
            
            demo_summary = f"""ğŸ“Š **ë°ëª¨ ê²°ê³¼ ìš”ì•½**:

â“ ì§ˆë¬¸: {demo_question}
â±ï¸ ì²˜ë¦¬ ì‹œê°„: {debug_result.total_processing_time:.3f}ì´ˆ
ğŸ“‹ ì²˜ë¦¬ ë‹¨ê³„: {len(debug_result.steps)}ë‹¨ê³„
âœ… ì„±ê³µ ì—¬ë¶€: {'ì„±ê³µ' if debug_result.success else 'ì‹¤íŒ¨'}

ğŸ” **ì£¼ìš” ì²˜ë¦¬ ë‹¨ê³„**:"""
            
            for step in debug_result.steps:
                demo_summary += f"\n   {step.step_number}. {step.step_name} ({step.processing_time:.3f}ì´ˆ)"
            
            demo_summary += f"""

ğŸ’¡ **ì½˜ì†”ì—ì„œ ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!**

ğŸ¯ **ë” ë§ì€ ë””ë²„ê¹… ì˜µì…˜**:
â€¢ "debug rag: ë‹¹ì‹ ì˜ ì§ˆë¬¸" - íŠ¹ì • ì§ˆë¬¸ ë¶„ì„
â€¢ "show rag process for ì£¼ì œ" - ì£¼ì œë³„ ë¶„ì„
â€¢ consoleì—ì„œ debug_multiple_questions() - ì—¬ëŸ¬ ì§ˆë¬¸ ë¹„êµ"""
            
            return demo_summary
        
        except Exception as e:
            return f"âŒ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    async def _handle_document_command(self, user_input: str) -> str:
        """Handle document processing commands (existing functionality)"""
        # Use simplified intent detection for commands
        user_lower = user_input.lower()
        
        if any(word in user_lower for word in ['download', 'scan', 'fetch', 'get']):
            if any(word in user_lower for word in ['all', 'everything']):
                return await self._handle_download_all()
            elif 'pdf' in user_lower:
                return await self._handle_download_pdfs()
            else:
                return await self._handle_download_from_url(user_input)
        
        elif any(word in user_lower for word in ['chunk', 'process', 'split']):
            return await self._handle_chunk_files()
        
        elif any(word in user_lower for word in ['index', 'vector', 'embed', 'searchable']):
            return await self._handle_index_files()
        
        elif any(word in user_lower for word in ['status', 'progress', 'files']):
            if 'quick' in user_lower or 'brief' in user_lower:
                return self.status_manager.get_quick_status()
            else:
                return self.status_manager.get_comprehensive_status()
        
        elif any(word in user_lower for word in ['help', 'commands', 'what can']):
            return self._get_help_message()
        
        else:
            return ("ğŸ¤– I understand you want to do something with documents, but I'm not sure what. "
                   "Try: 'download from URL', 'process files', 'index documents', or 'show status'")

    async def _handle_simple_search(self, user_input: str) -> str:
        """Handle simple vector search when RAG doesn't apply"""
        if not self.query_engine:
            return "âŒ Search functionality is not available."
        
        try:
            results = await self.query_engine.search(user_input, k=3)
            
            if not results:
                return f"ğŸ¤– I couldn't find specific information about '{user_input}'. Try rephrasing your question!"
            
            # Simple formatted response
            response = f"ğŸ” **Found information about '{user_input}':**\n\n"
            
            for i, result in enumerate(results, 1):
                payload = result.get('payload', {})
                score = result.get('score', 0)
                
                response += f"**ğŸ“‹ Result {i}** (Relevance: {score:.1%})\n"
                response += f"ğŸ“– Source: {payload.get('source_file', 'Unknown')}\n"
                
                content = payload.get('text', '')[:300]
                if len(payload.get('text', '')) > 300:
                    content += "..."
                response += f"ğŸ“„ Content: {content}\n"
                response += "â”€" * 40 + "\n\n"
            
            response += "ğŸ’¡ Need more detailed analysis? Ask a more specific question!"
            return response
            
        except Exception as e:
            return f"âŒ Search failed: {str(e)}"

    # [ì—¬ê¸°ì— ê¸°ì¡´ì˜ ë‹¤ë¥¸ ë©”ì„œë“œë“¤ì´ ê³„ì†ë©ë‹ˆë‹¤...]
    # _handle_download_from_url, _handle_download_all, ë“±ë“±...
    
    def _get_help_message(self) -> str:
        """Get help message"""
        debug_help = ""
        if RAG_DEBUG_AVAILABLE:
            debug_help = """
ğŸ” **RAG ë””ë²„ê¹… & ë¶„ì„**
   â€¢ "debug rag: IPCDAê°€ ë­ì•¼?" - íŠ¹ì • ì§ˆë¬¸ì˜ RAG í”„ë¡œì„¸ìŠ¤ ë¶„ì„
   â€¢ "show rag process for database" - ì£¼ì œë³„ RAG ë¶„ì„
   â€¢ "rag analysis" - RAG í”„ë¡œì„¸ìŠ¤ ë°ëª¨"""
        
        return f"""ğŸ¤– **I'm your intelligent document assistant!** Here's what I can do:

ğŸ“¥ **Document Management**
   â€¢ "Download PDFs from https://example.com/docs"
   â€¢ "Get all documents from that GitHub repo"
   â€¢ "Process the downloaded files"
   â€¢ "Index documents for search"

ğŸ§  **Intelligent Q&A (when documents are indexed)**
   â€¢ "What is IPCDA and how does it work?"
   â€¢ "How to configure database performance?"
   â€¢ "Troubleshoot connection timeout errors"
   â€¢ "Best practices for backup procedures"
   â€¢ "Explain the difference between memory and disk databases"
{debug_help}
ğŸ“Š **Status & Information**
   â€¢ "What's the current status?"
   â€¢ "Show quick status"
   â€¢ "How many files are indexed?"

ğŸ’¡ **Just talk naturally!** I can understand:
   - Complex technical questions
   - Step-by-step guidance requests  
   - Troubleshooting help
   - Definitions and explanations
   - Comparison questions

ğŸ¯ **Current capabilities**: """ + (
    "âœ… RAG-powered intelligent answers available!" if self.state.vector_store_ready 
    else "â³ Ready for document processing - index some documents to unlock intelligent Q&A!"
) + (
    " ğŸ” RAG debugging enabled!" if RAG_DEBUG_AVAILABLE else ""
)

    # [ê¸°ì¡´ì˜ ë‹¤ë¥¸ ë©”ì„œë“œë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€...]


async def main():
    """ğŸ­ Main interactive chat loop with RAG"""
    print("ğŸŒŸ" + "=" * 70 + "ğŸŒŸ")
    print("      ğŸ¤– Intelligent Document Assistant with RAG")
    if RAG_DEBUG_AVAILABLE:
        print("                ğŸ” RAG Debugging Enabled")
    print("ğŸŒŸ" + "=" * 70 + "ğŸŒŸ")
    print()
    print("ğŸ’¬ Hi! I'm your intelligent document processing assistant!")
    print("   I can download, process, index documents AND answer complex questions!")
    print()
    print("ğŸ¯ Try asking me:")
    print("   â€¢ Complex questions: 'What is IPCDA and how does it work?'")
    print("   â€¢ How-to questions: 'How to configure database performance?'")
    print("   â€¢ Troubleshooting: 'Fix connection timeout errors'")
    print("   â€¢ Document commands: 'Download PDFs from [URL]'")
    if RAG_DEBUG_AVAILABLE:
        print("   â€¢ RAG debugging: 'debug rag: your question here'")
    print()
    print("   Type 'quit' to exit")
    print("=" * 78)
    
    assistant = ConversationalAssistant()
    conversation_count = 0
    
    while True:
        try:
            # Get user input
            user_input = input(f"\nğŸ˜Š You: ").strip()
            
            # Check for exit
            if user_input.lower() in ['quit', 'exit', 'bye', 'goodbye']:
                print(f"\nğŸ¤– Goodbye! We had {conversation_count} great conversations today! âœ¨")
                break
            
            if not user_input:
                continue
            
            # Get response
            print(f"\nğŸ¤– Assistant: ", end="", flush=True)
            response = await assistant.chat(user_input)
            print(response)
            
            conversation_count += 1
            
        except KeyboardInterrupt:
            print(f"\n\nğŸ¤– Goodbye! We had {conversation_count} great conversations today! âœ¨")
            break
        except Exception as e:
            print(f"\nâŒ Oops! Something went wrong: {e}")
            print("ğŸ’¡ Please try again or ask for help!")

if __name__ == "__main__":
    asyncio.run(main())