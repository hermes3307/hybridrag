#!/usr/bin/env python3
"""
ğŸ” RAG Process Debugger
Detailed analysis and visualization of RAG (Retrieval Augmented Generation) process
"""

import os
import time
import json
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import re
from datetime import datetime

# Import RAG components
from vector_manager import VectorStoreManager
from query_engine import ConversationalQueryEngine


@dataclass
class RAGStep:
    """ğŸ” Individual step in RAG process"""
    step_number: int
    step_name: str
    description: str
    input_data: Any
    output_data: Any
    processing_time: float
    status: str  # success, failed, warning
    details: Dict[str, Any]
    timestamp: str


@dataclass
class RAGDebugResult:
    """ğŸ“Š Complete RAG debugging result"""
    question: str
    total_processing_time: float
    steps: List[RAGStep]
    final_result: str
    success: bool
    error_message: Optional[str] = None
    performance_metrics: Dict[str, Any] = None


class RAGProcessDebugger:
    """ğŸ” Detailed RAG Process Debugger"""
    
    def __init__(self, vector_manager: VectorStoreManager, query_engine: ConversationalQueryEngine):
        self.vector_manager = vector_manager
        self.query_engine = query_engine
        self.debug_results = []
        
        # Domain keywords for analysis
        self.domain_keywords = {
            'database': ['database', 'db', 'sql', 'table', 'index', 'query', 'schema'],
            'performance': ['performance', 'optimization', 'tuning', 'slow', 'fast', 'memory'],
            'altibase': ['altibase', 'ipcda', 'apre', 'isql', 'iloader', 'hybrid'],
            'network': ['network', 'connection', 'port', 'protocol', 'tcp', 'ip'],
            'configuration': ['config', 'setting', 'parameter', 'setup', 'installation']
        }
        
        # Question type patterns
        self.question_patterns = {
            'what_is': [r'what\s+is\s+(.+)', r'ë­ì•¼', r'ë¬´ì—‡', r'ì •ì˜'],
            'how_to': [r'how\s+to\s+(.+)', r'ì–´ë–»ê²Œ', r'ë°©ë²•'],
            'troubleshoot': [r'error|problem|issue|fix', r'ì˜¤ë¥˜', r'ë¬¸ì œ', r'í•´ê²°'],
            'comparison': [r'difference|compare|vs', r'ì°¨ì´', r'ë¹„êµ']
        }

    async def debug_rag_process(self, question: str, show_details: bool = True) -> RAGDebugResult:
        """ğŸ” RAG í”„ë¡œì„¸ìŠ¤ ì „ì²´ë¥¼ ë””ë²„ê¹…"""
        
        print(f"\nğŸ” {'='*60}")
        print(f"ğŸ§  RAG í”„ë¡œì„¸ìŠ¤ ë””ë²„ê¹… ì‹œì‘")
        print(f"â“ ì§ˆë¬¸: '{question}'")
        print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ” {'='*60}\n")
        
        start_time = time.time()
        steps = []
        
        try:
            # Step 1: ì§ˆë¬¸ ë¶„ì„
            step1_result = await self._debug_step1_question_analysis(question)
            steps.append(step1_result)
            if show_details: self._print_step_details(step1_result)
            
            # Step 2: í‚¤ì›Œë“œ ì¶”ì¶œ
            step2_result = await self._debug_step2_keyword_extraction(question, step1_result.output_data)
            steps.append(step2_result)
            if show_details: self._print_step_details(step2_result)
            
            # Step 3: ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            step3_result = await self._debug_step3_search_query_generation(step2_result.output_data)
            steps.append(step3_result)
            if show_details: self._print_step_details(step3_result)
            
            # Step 4: ë²¡í„° ê²€ìƒ‰
            step4_result = await self._debug_step4_vector_search(step3_result.output_data)
            steps.append(step4_result)
            if show_details: self._print_step_details(step4_result)
            
            # Step 5: ê²°ê³¼ í•„í„°ë§ ë° ë­í‚¹
            step5_result = await self._debug_step5_result_filtering(step4_result.output_data)
            steps.append(step5_result)
            if show_details: self._print_step_details(step5_result)
            
            # Step 6: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            step6_result = await self._debug_step6_context_building(step5_result.output_data)
            steps.append(step6_result)
            if show_details: self._print_step_details(step6_result)
            
            # Step 7: LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
            step7_result = await self._debug_step7_prompt_generation(question, step6_result.output_data, step1_result.output_data)
            steps.append(step7_result)
            if show_details: self._print_step_details(step7_result)
            
            # Step 8: ìµœì¢… ì‘ë‹µ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
            step8_result = await self._debug_step8_response_generation(step7_result.output_data)
            steps.append(step8_result)
            if show_details: self._print_step_details(step8_result)
            
            total_time = time.time() - start_time
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            performance_metrics = self._calculate_performance_metrics(steps, total_time)
            
            debug_result = RAGDebugResult(
                question=question,
                total_processing_time=total_time,
                steps=steps,
                final_result=step8_result.output_data.get('final_answer', 'No answer generated'),
                success=True,
                performance_metrics=performance_metrics
            )
            
            # ê²°ê³¼ ì €ì¥
            self.debug_results.append(debug_result)
            
            # ìµœì¢… ìš”ì•½ ì¶œë ¥
            if show_details:
                self._print_final_summary(debug_result)
            
            return debug_result
            
        except Exception as e:
            total_time = time.time() - start_time
            error_result = RAGDebugResult(
                question=question,
                total_processing_time=total_time,
                steps=steps,
                final_result="Error occurred during processing",
                success=False,
                error_message=str(e)
            )
            
            print(f"âŒ RAG í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}")
            return error_result

    async def _debug_step1_question_analysis(self, question: str) -> RAGStep:
        """Step 1: ì§ˆë¬¸ ë¶„ì„"""
        start_time = time.time()
        
        # ì§ˆë¬¸ íƒ€ì… ê°ì§€
        question_type = 'general'
        detected_pattern = None
        
        for q_type, patterns in self.question_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question, re.IGNORECASE):
                    question_type = q_type
                    detected_pattern = pattern
                    break
            if detected_pattern:
                break
        
        # ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„
        complexity_score = self._calculate_question_complexity(question)
        
        # ì–¸ì–´ ê°ì§€
        language = 'korean' if any(ord(char) > 127 for char in question) else 'english'
        
        processing_time = time.time() - start_time
        
        output_data = {
            'question_type': question_type,
            'detected_pattern': detected_pattern,
            'complexity_score': complexity_score,
            'language': language,
            'word_count': len(question.split()),
            'character_count': len(question)
        }
        
        return RAGStep(
            step_number=1,
            step_name="ì§ˆë¬¸ ë¶„ì„ (Question Analysis)",
            description="ì‚¬ìš©ì ì§ˆë¬¸ì˜ íƒ€ì…, ë³µì¡ë„, ì–¸ì–´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤",
            input_data={'question': question},
            output_data=output_data,
            processing_time=processing_time,
            status='success',
            details={
                'patterns_checked': len(self.question_patterns),
                'complexity_factors': ['word_count', 'question_marks', 'technical_terms']
            },
            timestamp=datetime.now().isoformat()
        )

    async def _debug_step2_keyword_extraction(self, question: str, question_analysis: Dict) -> RAGStep:
        """Step 2: í‚¤ì›Œë“œ ì¶”ì¶œ"""
        start_time = time.time()
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” NLTK ì‚¬ìš©)
        words = question.lower().split()
        
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'what', 'is', 'how', 'to', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'ê°€', 'ëŠ”', 'ì„', 'ë¥¼', 'ì´', 'ê·¸', 'ê·¸ê²ƒ', 'ë­ì•¼', 'ë¬´ì—‡'}
        
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
        technical_terms = []
        for word in words:
            for domain, domain_words in self.domain_keywords.items():
                if word.lower() in domain_words:
                    technical_terms.append(word.lower())
        
        # ë„ë©”ì¸ ì‹ë³„
        identified_domains = []
        for domain, domain_words in self.domain_keywords.items():
            if any(word in keywords + technical_terms for word in domain_words):
                identified_domains.append(domain)
        
        processing_time = time.time() - start_time
        
        output_data = {
            'keywords': keywords,
            'technical_terms': list(set(technical_terms)),
            'identified_domains': identified_domains,
            'keyword_count': len(keywords),
            'technical_term_count': len(set(technical_terms))
        }
        
        return RAGStep(
            step_number=2,
            step_name="í‚¤ì›Œë“œ ì¶”ì¶œ (Keyword Extraction)",
            description="ì§ˆë¬¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œì™€ ê¸°ìˆ  ìš©ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤",
            input_data={'question': question, 'analysis': question_analysis},
            output_data=output_data,
            processing_time=processing_time,
            status='success',
            details={
                'stop_words_removed': len([w for w in words if w in stop_words]),
                'domains_checked': len(self.domain_keywords)
            },
            timestamp=datetime.now().isoformat()
        )

    async def _debug_step3_search_query_generation(self, keyword_data: Dict) -> RAGStep:
        """Step 3: ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        start_time = time.time()
        
        keywords = keyword_data['keywords']
        technical_terms = keyword_data['technical_terms']
        domains = keyword_data['identified_domains']
        
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        search_queries = []
        
        # 1. ë©”ì¸ í‚¤ì›Œë“œ ì¿¼ë¦¬
        if keywords:
            main_query = ' '.join(keywords[:3])  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
            search_queries.append({
                'query': main_query,
                'type': 'main_keywords',
                'priority': 1
            })
        
        # 2. ê¸°ìˆ  ìš©ì–´ ì¿¼ë¦¬
        if technical_terms:
            tech_query = ' '.join(technical_terms)
            search_queries.append({
                'query': tech_query,
                'type': 'technical_terms',
                'priority': 2
            })
        
        # 3. ë„ë©”ì¸ íŠ¹í™” ì¿¼ë¦¬
        for domain in domains:
            domain_query = f"{domain} {' '.join(keywords[:2])}"
            search_queries.append({
                'query': domain_query,
                'type': f'domain_{domain}',
                'priority': 3
            })
        
        # ì¤‘ë³µ ì œê±°
        unique_queries = []
        seen_queries = set()
        for query_info in search_queries:
            if query_info['query'] not in seen_queries:
                unique_queries.append(query_info)
                seen_queries.add(query_info['query'])
        
        processing_time = time.time() - start_time
        
        output_data = {
            'search_queries': unique_queries,
            'query_count': len(unique_queries),
            'query_types': list(set(q['type'] for q in unique_queries))
        }
        
        return RAGStep(
            step_number=3,
            step_name="ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (Search Query Generation)",
            description="í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë²¡í„° ê²€ìƒ‰ìš© ì¿¼ë¦¬ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤",
            input_data=keyword_data,
            output_data=output_data,
            processing_time=processing_time,
            status='success',
            details={
                'original_queries': len(search_queries),
                'duplicates_removed': len(search_queries) - len(unique_queries)
            },
            timestamp=datetime.now().isoformat()
        )

    async def _debug_step4_vector_search(self, query_data: Dict) -> RAGStep:
        """Step 4: ë²¡í„° ê²€ìƒ‰"""
        start_time = time.time()
        
        search_queries = query_data['search_queries']
        all_results = []
        search_details = []
        
        # ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        for query_info in search_queries:
            query = query_info['query']
            try:
                # ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
                if self.vector_manager:
                    results = await self.vector_manager.search(query, k=5)
                    all_results.extend(results)
                    
                    search_details.append({
                        'query': query,
                        'type': query_info['type'],
                        'results_count': len(results),
                        'top_score': results[0]['score'] if results else 0,
                        'status': 'success'
                    })
                else:
                    # ë²¡í„° ë§¤ë‹ˆì €ê°€ ì—†ëŠ” ê²½ìš° ì‹œë®¬ë ˆì´ì…˜
                    simulated_results = self._simulate_search_results(query, 3)
                    all_results.extend(simulated_results)
                    
                    search_details.append({
                        'query': query,
                        'type': query_info['type'],
                        'results_count': len(simulated_results),
                        'top_score': simulated_results[0]['score'] if simulated_results else 0,
                        'status': 'simulated'
                    })
                    
            except Exception as e:
                search_details.append({
                    'query': query,
                    'type': query_info['type'],
                    'results_count': 0,
                    'error': str(e),
                    'status': 'failed'
                })
        
        processing_time = time.time() - start_time
        
        # ê²°ê³¼ í†µê³„
        total_results = len(all_results)
        successful_queries = len([d for d in search_details if d['status'] in ['success', 'simulated']])
        
        output_data = {
            'search_results': all_results,
            'search_details': search_details,
            'total_results': total_results,
            'successful_queries': successful_queries,
            'average_score': sum(r['score'] for r in all_results) / total_results if total_results > 0 else 0
        }
        
        return RAGStep(
            step_number=4,
            step_name="ë²¡í„° ê²€ìƒ‰ (Vector Search)",
            description="ìƒì„±ëœ ì¿¼ë¦¬ë¡œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤",
            input_data=query_data,
            output_data=output_data,
            processing_time=processing_time,
            status='success' if successful_queries > 0 else 'warning',
            details={
                'queries_executed': len(search_queries),
                'vector_db_available': self.vector_manager is not None
            },
            timestamp=datetime.now().isoformat()
        )

    async def _debug_step5_result_filtering(self, search_data: Dict) -> RAGStep:
        """Step 5: ê²°ê³¼ í•„í„°ë§ ë° ë­í‚¹"""
        start_time = time.time()
        
        all_results = search_data['search_results']
        
        # ì¤‘ë³µ ì œê±°
        unique_results = []
        seen_content = set()
        
        for result in all_results:
            content = result.get('payload', {}).get('text', '')[:200]
            content_hash = hash(content)
            
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append(result)
        
        # ì ìˆ˜ë³„ ì •ë ¬
        sorted_results = sorted(unique_results, key=lambda x: x.get('score', 0), reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ ì„ íƒ (ìµœëŒ€ 8ê°œ)
        top_results = sorted_results[:8]
        
        # í•„í„°ë§ í†µê³„
        duplicates_removed = len(all_results) - len(unique_results)
        score_threshold = 0.3  # ì„ê³„ì ìˆ˜
        high_quality_results = [r for r in top_results if r.get('score', 0) > score_threshold]
        
        processing_time = time.time() - start_time
        
        output_data = {
            'filtered_results': top_results,
            'high_quality_results': high_quality_results,
            'total_after_filtering': len(top_results),
            'duplicates_removed': duplicates_removed,
            'score_distribution': {
                'min': min(r['score'] for r in top_results) if top_results else 0,
                'max': max(r['score'] for r in top_results) if top_results else 0,
                'avg': sum(r['score'] for r in top_results) / len(top_results) if top_results else 0
            }
        }
        
        return RAGStep(
            step_number=5,
            step_name="ê²°ê³¼ í•„í„°ë§ (Result Filtering)",
            description="ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì¤‘ë³µì„ ì œê±°í•˜ê³  ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬í•©ë‹ˆë‹¤",
            input_data={'original_count': len(all_results)},
            output_data=output_data,
            processing_time=processing_time,
            status='success',
            details={
                'score_threshold': score_threshold,
                'max_results': 8
            },
            timestamp=datetime.now().isoformat()
        )

    async def _debug_step6_context_building(self, filtered_data: Dict) -> RAGStep:
        """Step 6: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        start_time = time.time()
        
        top_results = filtered_data['filtered_results']
        max_context_length = 2000  # ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
        
        context_parts = []
        current_length = 0
        sources_used = []
        
        for i, result in enumerate(top_results, 1):
            payload = result.get('payload', {})
            text = payload.get('text', '').strip()
            source = payload.get('source_file', 'Unknown')
            
            if text and current_length < max_context_length:
                # ì†ŒìŠ¤ ì •ë³´ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ ì¶”ê°€
                context_part = f"[Source {i}: {source}]\n{text}\n"
                
                if current_length + len(context_part) <= max_context_length:
                    context_parts.append(context_part)
                    current_length += len(context_part)
                    sources_used.append(source)
                else:
                    # ì˜ë¼ì„œ ì¶”ê°€
                    remaining_space = max_context_length - current_length - 100
                    if remaining_space > 100:
                        truncated_text = text[:remaining_space] + "..."
                        context_parts.append(f"[Source {i}: {source}]\n{truncated_text}\n")
                        sources_used.append(source)
                    break
        
        final_context = "\n".join(context_parts)
        
        processing_time = time.time() - start_time
        
        output_data = {
            'final_context': final_context,
            'context_length': len(final_context),
            'sources_used': sources_used,
            'sources_count': len(set(sources_used)),
            'truncated': len(final_context) >= max_context_length - 100
        }
        
        return RAGStep(
            step_number=6,
            step_name="ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (Context Building)",
            description="í•„í„°ë§ëœ ê²°ê³¼ë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¡°í•©í•©ë‹ˆë‹¤",
            input_data={'available_results': len(top_results)},
            output_data=output_data,
            processing_time=processing_time,
            status='success',
            details={
                'max_length': max_context_length,
                'parts_included': len(context_parts)
            },
            timestamp=datetime.now().isoformat()
        )

    async def _debug_step7_prompt_generation(self, question: str, context_data: Dict, question_analysis: Dict) -> RAGStep:
        """Step 7: LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        start_time = time.time()
        
        context = context_data['final_context']
        question_type = question_analysis['question_type']
        
        # ì§ˆë¬¸ íƒ€ì…ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        prompt_templates = {
            'what_is': """ë‹¤ìŒ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ {topic}ì´ ë¬´ì—‡ì¸ì§€ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ì •ì˜ ë° ê°œìš”
2. ì£¼ìš” íŠ¹ì§•
3. ì‚¬ìš© ì‚¬ë¡€
4. ì¤‘ìš”í•œ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­

ë‹µë³€:""",
            
            'how_to': """ë‹¤ìŒ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ {topic} ë°©ë²•ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ë‹¨ê³„ë³„ ì§€ì¹¨
2. ì „ì œì¡°ê±´
3. ì£¼ì˜ì‚¬í•­
4. ì˜ˆìƒ ê²°ê³¼

ë‹µë³€:""",
            
            'general': """ë‹¤ìŒ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€:"""
        }
        
        # í”„ë¡¬í”„íŠ¸ ì„ íƒ ë° ìƒì„±
        template = prompt_templates.get(question_type, prompt_templates['general'])
        
        # ì£¼ì œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        topic = question.replace('ë­ì•¼', '').replace('what is', '').strip()
        
        final_prompt = template.format(
            topic=topic,
            context=context,
            question=question
        )
        
        processing_time = time.time() - start_time
        
        output_data = {
            'final_prompt': final_prompt,
            'prompt_length': len(final_prompt),
            'template_used': question_type,
            'topic_extracted': topic,
            'context_included': len(context) > 0
        }
        
        return RAGStep(
            step_number=7,
            step_name="í”„ë¡¬í”„íŠ¸ ìƒì„± (Prompt Generation)",
            description="LLMì„ ìœ„í•œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤",
            input_data={'question': question, 'question_type': question_type},
            output_data=output_data,
            processing_time=processing_time,
            status='success',
            details={
                'templates_available': len(prompt_templates),
                'context_characters': len(context)
            },
            timestamp=datetime.now().isoformat()
        )

    async def _debug_step8_response_generation(self, prompt_data: Dict) -> RAGStep:
        """Step 8: ì‘ë‹µ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)"""
        start_time = time.time()
        
        prompt = prompt_data['final_prompt']
        
        # OpenAI API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        # ì‹¤ì œë¡œëŠ” OpenAI APIë¥¼ í˜¸ì¶œí•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
        
        simulated_response = """ğŸ¯ **IPCDA (In-Process Communication Direct Access)ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤!** 

ğŸ“– **ì •ì˜ ë° ê°œìš”:**
IPCDAëŠ” Altibaseì—ì„œ ì œê³µí•˜ëŠ” ê³ ì„±ëŠ¥ í†µì‹  ë°©ì‹ìœ¼ë¡œ, ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ì™€ ë””ìŠ¤í¬ ë°ì´í„°ë² ì´ìŠ¤ ê°„ì˜ ì§ì ‘ì ì¸ í†µì‹ ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

ğŸš€ **ì£¼ìš” íŠ¹ì§•:**
â€¢ ë©”ëª¨ë¦¬ ê°„ ì§ì ‘ ì ‘ê·¼ìœ¼ë¡œ ì´ˆê³ ì† ì²˜ë¦¬ âš¡
â€¢ ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™” ğŸ”„
â€¢ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì— ìµœì í™” ğŸ“Š

âš™ï¸ **ì„¤ì • ë°©ë²•:**
altibase.properties íŒŒì¼ì—ì„œ IPCDA_CHANNEL_COUNTì™€ IPCDA_DATABLOCK_SIZE ë“±ì„ ì„¤ì •í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ì •ë³´ëŠ” ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."""
        
        # ì‘ë‹µ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
        response_metrics = {
            'length': len(simulated_response),
            'contains_korean': any(ord(char) > 127 for char in simulated_response),
            'contains_emojis': any(char in 'ğŸ¯ğŸ“–ğŸš€âš¡ğŸ”„ğŸ“Šâš™ï¸' for char in simulated_response),
            'structured': '**' in simulated_response and 'â€¢' in simulated_response
        }
        
        processing_time = time.time() - start_time
        
        output_data = {
            'final_answer': simulated_response,
            'response_metrics': response_metrics,
            'generation_method': 'simulated',  # ì‹¤ì œë¡œëŠ” 'openai_api'
            'tokens_estimated': len(simulated_response) // 4  # ëŒ€ëµì ì¸ í† í° ìˆ˜
        }
        
        return RAGStep(
            step_number=8,
            step_name="ì‘ë‹µ ìƒì„± (Response Generation)",
            description="LLMì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤",
            input_data={'prompt_length': len(prompt)},
            output_data=output_data,
            processing_time=processing_time,
            status='success',
            details={
                'model_used': 'gpt-4o-mini (simulated)',
                'temperature': 0.3
            },
            timestamp=datetime.now().isoformat()
        )

    def _calculate_question_complexity(self, question: str) -> float:
        """ì§ˆë¬¸ ë³µì¡ë„ ê³„ì‚°"""
        complexity = 0.0
        
        # ë‹¨ì–´ ìˆ˜
        word_count = len(question.split())
        complexity += min(word_count / 10, 1.0)
        
        # ì§ˆë¬¸ í‘œì‹œ ìˆ˜
        question_marks = question.count('?') + question.count('ï¼Ÿ')
        complexity += min(question_marks / 3, 0.5)
        
        # ê¸°ìˆ  ìš©ì–´ í¬í•¨ ì—¬ë¶€
        technical_terms = sum(1 for domain_words in self.domain_keywords.values() 
                             for word in domain_words if word in question.lower())
        complexity += min(technical_terms / 5, 0.5)
        
        return min(complexity, 1.0)

    def _simulate_search_results(self, query: str, count: int) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜"""
        simulated_results = []
        
        for i in range(count):
            score = max(0.3, 1.0 - (i * 0.15))  # ì ìˆ˜ê°€ ê°ì†Œí•˜ëŠ” íŒ¨í„´
            
            result = {
                'score': score,
                'payload': {
                    'text': f"ì´ê²ƒì€ '{query}' ê²€ìƒ‰ì–´ì— ëŒ€í•œ ì‹œë®¬ë ˆì´ì…˜ëœ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤. ë¬¸ì„œ {i+1}ë²ˆì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. IPCDAëŠ” Altibaseì˜ ê³ ì„±ëŠ¥ í†µì‹  ê¸°ìˆ ë¡œ, ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ì™€ ë””ìŠ¤í¬ ë°ì´í„°ë² ì´ìŠ¤ ê°„ì˜ ì§ì ‘ í†µì‹ ì„ ì§€ì›í•©ë‹ˆë‹¤.",
                    'source_file': f'altibase_manual_part{i+1}.pdf',
                    'chunk_id': f'chunk_{i+1}_{hash(query) % 1000}'
                }
            }
            
            simulated_results.append(result)
        
        return simulated_results

    def _calculate_performance_metrics(self, steps: List[RAGStep], total_time: float) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        step_times = [step.processing_time for step in steps]
        
        return {
            'total_time': total_time,
            'step_times': {step.step_name: step.processing_time for step in steps},
            'slowest_step': max(steps, key=lambda x: x.processing_time).step_name if steps else None,
            'fastest_step': min(steps, key=lambda x: x.processing_time).step_name if steps else None,
            'average_step_time': sum(step_times) / len(step_times) if step_times else 0,
            'successful_steps': len([s for s in steps if s.status == 'success']),
            'total_steps': len(steps)
        }

    def _print_step_details(self, step: RAGStep):
        """ë‹¨ê³„ë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥"""
        
        status_emoji = {
            'success': 'âœ…',
            'warning': 'âš ï¸',
            'failed': 'âŒ'
        }
        
        print(f"\n{status_emoji.get(step.status, 'ğŸ”„')} **Step {step.step_number}: {step.step_name}**")
        print(f"ğŸ“ ì„¤ëª…: {step.description}")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {step.processing_time:.3f}ì´ˆ")
        
        # ì…ë ¥ ë°ì´í„° ìš”ì•½
        if step.input_data:
            print(f"ğŸ“¥ ì…ë ¥: {self._summarize_data(step.input_data)}")
        
        # ì¶œë ¥ ë°ì´í„° ìš”ì•½
        if step.output_data:
            print(f"ğŸ“¤ ì¶œë ¥: {self._summarize_data(step.output_data)}")
        
        # ìƒì„¸ ì •ë³´
        if step.details:
            print(f"ğŸ” ì„¸ë¶€ì‚¬í•­: {step.details}")
        
        print("-" * 60)

    def _summarize_data(self, data: Any) -> str:
        """ë°ì´í„° ìš”ì•½"""
        if isinstance(data, dict):
            summary_parts = []
            for key, value in data.items():
                if isinstance(value, list):
                    summary_parts.append(f"{key}({len(value)}ê°œ)")
                elif isinstance(value, str) and len(value) > 50:
                    summary_parts.append(f"{key}({len(value)}ì)")
                else:
                    summary_parts.append(f"{key}={value}")
            return ", ".join(summary_parts)
        elif isinstance(data, list):
            return f"ë¦¬ìŠ¤íŠ¸({len(data)}ê°œ í•­ëª©)"
        else:
            return str(data)

    def _print_final_summary(self, debug_result: RAGDebugResult):
        """ìµœì¢… ìš”ì•½ ì¶œë ¥"""
        
        print(f"\nğŸ‰ {'='*60}")
        print(f"ğŸ RAG í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        print(f"{'='*60}")
        
        print(f"â“ ì›ë³¸ ì§ˆë¬¸: {debug_result.question}")
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {debug_result.total_processing_time:.3f}ì´ˆ")
        print(f"âœ… ì„±ê³µ ì—¬ë¶€: {'ì„±ê³µ' if debug_result.success else 'ì‹¤íŒ¨'}")
        
        if debug_result.performance_metrics:
            metrics = debug_result.performance_metrics
            print(f"\nğŸ“Š **ì„±ëŠ¥ ë©”íŠ¸ë¦­:**")
            print(f"   â€¢ ê°€ì¥ ëŠë¦° ë‹¨ê³„: {metrics['slowest_step']}")
            print(f"   â€¢ ê°€ì¥ ë¹ ë¥¸ ë‹¨ê³„: {metrics['fastest_step']}")
            print(f"   â€¢ í‰ê·  ë‹¨ê³„ ì‹œê°„: {metrics['average_step_time']:.3f}ì´ˆ")
            print(f"   â€¢ ì„±ê³µí•œ ë‹¨ê³„: {metrics['successful_steps']}/{metrics['total_steps']}")
        
        print(f"\nğŸ¯ **ìµœì¢… ë‹µë³€:**")
        print(debug_result.final_result)
        
        print(f"\nğŸ” {'='*60}")

    def save_debug_result(self, debug_result: RAGDebugResult, filename: str = None):
        """ë””ë²„ê·¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_debug_{timestamp}.json"
        
        # dataclassë¥¼ dictë¡œ ë³€í™˜
        result_dict = asdict(debug_result)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ë””ë²„ê·¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def get_debug_history(self) -> List[RAGDebugResult]:
        """ë””ë²„ê·¸ íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.debug_results

    async def compare_queries(self, questions: List[str]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì§ˆë¬¸ì˜ RAG í”„ë¡œì„¸ìŠ¤ ë¹„êµ"""
        
        print(f"\nğŸ” ì—¬ëŸ¬ ì§ˆë¬¸ RAG í”„ë¡œì„¸ìŠ¤ ë¹„êµ ì‹œì‘")
        print(f"ğŸ“ ì§ˆë¬¸ ìˆ˜: {len(questions)}")
        print("-" * 60)
        
        comparison_results = []
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ”¸ ì§ˆë¬¸ {i}: {question}")
            result = await self.debug_rag_process(question, show_details=False)
            comparison_results.append(result)
        
        # ë¹„êµ ë¶„ì„
        comparison_analysis = {
            'questions': questions,
            'results': comparison_results,
            'performance_comparison': {
                'fastest_question': min(comparison_results, key=lambda x: x.total_processing_time).question,
                'slowest_question': max(comparison_results, key=lambda x: x.total_processing_time).question,
                'average_time': sum(r.total_processing_time for r in comparison_results) / len(comparison_results),
                'success_rate': len([r for r in comparison_results if r.success]) / len(comparison_results)
            }
        }
        
        # ë¹„êµ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š **ë¹„êµ ë¶„ì„ ê²°ê³¼:**")
        print(f"   â€¢ ê°€ì¥ ë¹ ë¥¸ ì§ˆë¬¸: {comparison_analysis['performance_comparison']['fastest_question']}")
        print(f"   â€¢ ê°€ì¥ ëŠë¦° ì§ˆë¬¸: {comparison_analysis['performance_comparison']['slowest_question']}")
        print(f"   â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {comparison_analysis['performance_comparison']['average_time']:.3f}ì´ˆ")
        print(f"   â€¢ ì„±ê³µë¥ : {comparison_analysis['performance_comparison']['success_rate']:.1%}")
        
        return comparison_analysis


# í¸ì˜ í•¨ìˆ˜ë“¤
async def debug_single_question(question: str, vector_manager=None, query_engine=None):
    """ë‹¨ì¼ ì§ˆë¬¸ RAG ë””ë²„ê¹…"""
    debugger = RAGProcessDebugger(vector_manager, query_engine)
    return await debugger.debug_rag_process(question)

async def debug_multiple_questions(questions: List[str], vector_manager=None, query_engine=None):
    """ì—¬ëŸ¬ ì§ˆë¬¸ RAG ë¹„êµ ë””ë²„ê¹…"""
    debugger = RAGProcessDebugger(vector_manager, query_engine)
    return await debugger.compare_queries(questions)

async def quick_rag_demo():
    """RAG í”„ë¡œì„¸ìŠ¤ ë¹ ë¥¸ ë°ëª¨"""
    print("ğŸš€ RAG í”„ë¡œì„¸ìŠ¤ ë¹ ë¥¸ ë°ëª¨ ì‹œì‘!")
    
    demo_questions = [
        "IPCDAê°€ ë­ì•¼?",
        "How to configure database performance?",
        "Altibase ì„¤ì¹˜ ë°©ë²•ì€?"
    ]
    
    debugger = RAGProcessDebugger(None, None)  # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
    
    for question in demo_questions:
        print(f"\n{'ğŸ” ' + '='*50}")
        await debugger.debug_rag_process(question, show_details=True)


if __name__ == "__main__":
    # ë‹¨ë… ì‹¤í–‰ì‹œ ë°ëª¨ ì‹¤í–‰
    print("ğŸ” RAG í”„ë¡œì„¸ìŠ¤ ë””ë²„ê±° - ë…ë¦½ ì‹¤í–‰ ëª¨ë“œ")
    asyncio.run(quick_rag_demo())