#!/usr/bin/env python3
"""
Simple Employee Q&A Chatbot for Altibase
A command-line chatbot that uses your vector database to answer questions about Altibase manuals.
"""

import os
import json
import time
import logging
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleChatbot:
    """Simple Q&A Chatbot for Employee Onboarding"""
    
    def __init__(self, vector_db_path: str = "manual_vector_db", model_type: str = "claude"):
        """
        Initialize the chatbot
        
        Args:
            vector_db_path: Path to your vector database
            model_type: AI model to use ('claude', 'openai', or 'llama3')
        """
        self.vector_db_path = vector_db_path
        self.model_type = model_type.lower()
        self.vector_db = None
        self.vectorizer = None
        self.llm_client = None
        self.conversation_history = []
        
        print("ğŸ¤– Altibase Employee Q&A Chatbot ì´ˆê¸°í™” ì¤‘...")
        self._load_vector_database()
        self._initialize_llm()
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _load_vector_database(self):
        """Load your existing vector database"""
        try:
            # Import your existing vector system
            from paste import ManualVectorDB, ManualVectorizer
            
            print(f"ğŸ“š ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©: {self.vector_db_path}")
            self.vectorizer = ManualVectorizer()
            self.vector_db = ManualVectorDB.load(self.vector_db_path, self.vectorizer.vector_size)
            
            print(f"âœ… ë¡œë“œ ì™„ë£Œ: {self.vector_db.total_vectors:,}ê°œ ë²¡í„°, {len(self.vector_db.chunks):,}ê°œ ì²­í¬")
            
        except ImportError:
            print("âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   paste.py íŒŒì¼ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
        except Exception as e:
            print(f"âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            exit(1)
    
    def _initialize_llm(self):
        """Initialize the selected LLM"""
        print(f"ğŸ§  {self.model_type.upper()} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        if self.model_type == "claude":
            self._init_claude()
        elif self.model_type == "openai":
            self._init_openai()
        elif self.model_type == "llama3":
            self._init_llama3()
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {self.model_type}")
            print("   ì§€ì› ëª¨ë¸: claude, openai, llama3")
            exit(1)
    
    def _init_claude(self):
        """Initialize Claude AI"""
        try:
            import anthropic
            
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                print("âŒ ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   export ANTHROPIC_API_KEY='your_api_key'")
                exit(1)
            
            self.llm_client = anthropic.Anthropic(api_key=api_key)
            print("âœ… Claude AI ì´ˆê¸°í™” ì™„ë£Œ")
            
        except ImportError:
            print("âŒ anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install anthropic")
            exit(1)
    
    def _init_openai(self):
        """Initialize OpenAI"""
        try:
            import openai
            
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   export OPENAI_API_KEY='your_api_key'")
                exit(1)
            
            self.llm_client = openai.OpenAI(api_key=api_key)
            print("âœ… OpenAI ì´ˆê¸°í™” ì™„ë£Œ")
            
        except ImportError:
            print("âŒ openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install openai")
            exit(1)
    
    def _init_llama3(self):
        """Initialize Llama3 via Ollama"""
        try:
            import requests
            
            # Test Ollama connection
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={"model": "llama3", "prompt": "test", "stream": False},
                timeout=5
            )
            
            if response.status_code == 200:
                self.llm_client = "ollama"
                print("âœ… Llama3 (Ollama) ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                raise Exception("Ollama ì—°ê²° ì‹¤íŒ¨")
                
        except Exception as e:
            print("âŒ Llama3 ì´ˆê¸°í™” ì‹¤íŒ¨")
            print("   1. Ollamaë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: https://ollama.ai")
            print("   2. Llama3 ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: ollama pull llama3")
            print("   3. Ollama ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”: ollama serve")
            exit(1)
    
    def get_relevant_context(self, question: str, top_k: int = 5) -> Tuple[str, List[Dict]]:
        """Get relevant context from vector database"""
        try:
            # Search for relevant chunks
            results = self.vector_db.search(
                question, 
                k=top_k, 
                vectorizer=self.vectorizer
            )
            
            if not results:
                return "", []
            
            # Build context
            context_parts = []
            citations = []
            
            for i, result in enumerate(results):
                chunk = result['chunk']
                metadata = chunk.get('metadata', {})
                
                context_part = f"""
[ì°¸ê³ ìë£Œ {i+1}] {chunk['manual_title']} - {chunk['section_key']}
ì±•í„°: {metadata.get('chapter', 'N/A')}
ìœ ì‚¬ë„: {result['similarity']*100:.1f}%

{chunk['text'][:600]}...
"""
                context_parts.append(context_part)
                
                citations.append({
                    'manual_title': chunk['manual_title'],
                    'section_key': chunk['section_key'],
                    'chapter': metadata.get('chapter', 'N/A'),
                    'similarity': result['similarity']
                })
            
            return "\n".join(context_parts), citations
            
        except Exception as e:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return "", []
    
    def generate_response(self, question: str, context: str) -> str:
        """Generate response using the selected LLM"""
        try:
            if self.model_type == "claude":
                return self._generate_claude_response(question, context)
            elif self.model_type == "openai":
                return self._generate_openai_response(question, context)
            elif self.model_type == "llama3":
                return self._generate_llama3_response(question, context)
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _generate_claude_response(self, question: str, context: str) -> str:
        """Generate response using Claude"""
        system_prompt = """ë‹¹ì‹ ì€ Altibase ì œí’ˆ ì „ë¬¸ê°€ì´ë©°, ì‹ ì… ì§ì›ë“¤ì˜ ì˜¨ë³´ë”©ì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì œê³µëœ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
2. ë‹µë³€ì€ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
3. ê°€ëŠ¥í•œ ê²½ìš° êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ë‹¨ê³„ë³„ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•˜ë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”"""

        user_prompt = f"""ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ìœ„ì˜ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        # Add conversation history
        messages = []
        for turn in self.conversation_history[-5:]:  # Last 5 turns
            messages.append({"role": "user", "content": turn["user"]})
            messages.append({"role": "assistant", "content": turn["assistant"]})
        
        messages.append({"role": "user", "content": user_prompt})
        
        response = self.llm_client.messages.create(
            model="claude-3-sonnet-20240229",
            max_tokens=2000,
            temperature=0.3,
            system=system_prompt,
            messages=messages
        )
        
        return response.content[0].text
    
    def _generate_openai_response(self, question: str, context: str) -> str:
        """Generate response using OpenAI"""
        system_prompt = "ë‹¹ì‹ ì€ Altibase ì œí’ˆ ì „ë¬¸ê°€ì´ë©° ì‹ ì…ì§ì› ì˜¨ë³´ë”©ì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        user_prompt = f"""ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        messages = [{"role": "system", "content": system_prompt}]
        
        # Add conversation history
        for turn in self.conversation_history[-5:]:
            messages.append({"role": "user", "content": turn["user"]})
            messages.append({"role": "assistant", "content": turn["assistant"]})
        
        messages.append({"role": "user", "content": user_prompt})
        
        response = self.llm_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.3,
            max_tokens=2000
        )
        
        return response.choices[0].message.content
    
    def _generate_llama3_response(self, question: str, context: str) -> str:
        """Generate response using Llama3 via Ollama"""
        import requests
        
        prompt = f"""ë‹¹ì‹ ì€ Altibase ì œí’ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

ë§¤ë‰´ì–¼ ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3",
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.5,
                    "num_predict": 1500
                }
            },
            timeout=60
        )
        
        if response.status_code == 200:
            return response.json()["response"]
        else:
            raise Exception(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
    
    def chat(self, question: str) -> Dict[str, Any]:
        """Main chat function"""
        start_time = time.time()
        
        # Get relevant context
        print("ğŸ” ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì¤‘...")
        context, citations = self.get_relevant_context(question)
        
        # Generate response
        print("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘...")
        response = self.generate_response(question, context)
        
        # Save to history
        self.conversation_history.append({
            "user": question,
            "assistant": response,
            "timestamp": datetime.now().isoformat()
        })
        
        # Keep only last 10 conversations
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
        
        return {
            "question": question,
            "answer": response,
            "citations": citations,
            "response_time": time.time() - start_time,
            "model": self.model_type
        }
    
    def run_cli(self):
        """Run the command-line interface"""
        print("\n" + "="*80)
        print("          ğŸ¤– Altibase Employee Q&A Chatbot")
        print("="*80)
        print(f"ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {self.model_type.upper()}")
        print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤: {self.vector_db.total_vectors:,}ê°œ ë²¡í„°")
        print("\nëª…ë ¹ì–´:")
        print("  ì§ˆë¬¸ ì…ë ¥     - ì¼ë°˜ ì§ˆë¬¸")
        print("  /help        - ë„ì›€ë§")
        print("  /clear       - ëŒ€í™” ê¸°ë¡ ì‚­ì œ")
        print("  /quit        - ì¢…ë£Œ")
        print("="*80)
        
        while True:
            try:
                question = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if not question:
                    continue
                
                # Handle commands
                if question == "/quit":
                    print("ğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                elif question == "/help":
                    self._show_help()
                    continue
                
                elif question == "/clear":
                    self.conversation_history = []
                    print("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    continue
                
                # Generate response
                result = self.chat(question)
                
                # Display response
                print(f"\nğŸ¤– **ë‹µë³€:**")
                print(result['answer'])
                
                # Display citations
                if result['citations']:
                    print(f"\nğŸ“š **ì°¸ê³  ìë£Œ:**")
                    for i, citation in enumerate(result['citations'][:3]):
                        print(f"{i+1}. {citation['manual_title']} - {citation['section_key']}")
                        print(f"   ğŸ“Š ìœ ì‚¬ë„: {citation['similarity']*100:.1f}%")
                
                print(f"\nâ±ï¸ ì‘ë‹µ ì‹œê°„: {result['response_time']:.2f}ì´ˆ")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _show_help(self):
        """Show help information"""
        print("""
ğŸ“‹ ì‚¬ìš©ë²•:
â€¢ ìì—°ì–´ë¡œ ì§ˆë¬¸í•˜ì„¸ìš” (ì˜ˆ: "Altibase ì„¤ì¹˜ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”")
â€¢ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤

ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ:
â€¢ "Altibaseê°€ ë¬´ì—‡ì¸ê°€ìš”?"
â€¢ "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
â€¢ "SQL ì¿¼ë¦¬ ìµœì í™” ë°©ë²•ì€?"
â€¢ "ë°±ì—…ê³¼ ë³µêµ¬ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
â€¢ "ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
â€¢ "ì—ëŸ¬ ì½”ë“œë³„ í•´ê²° ë°©ë²•ì€?"

ğŸ”§ ëª…ë ¹ì–´:
/help  - ì´ ë„ì›€ë§ í‘œì‹œ
/clear - ëŒ€í™” ê¸°ë¡ ì‚­ì œ
/quit  - í”„ë¡œê·¸ë¨ ì¢…ë£Œ
""")

def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Simple Altibase Employee Q&A Chatbot')
    parser.add_argument('--vector-db', type=str, default='manual_vector_db',
                       help='Vector database directory path')
    parser.add_argument('--model', type=str, default='claude',
                       choices=['claude', 'openai', 'llama3'],
                       help='AI model to use')
    
    args = parser.parse_args()
    
    try:
        # Initialize and run chatbot
        chatbot = SimpleChatbot(args.vector_db, args.model)
        chatbot.run_cli()
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()