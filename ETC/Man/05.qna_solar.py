#!/usr/bin/env python3
"""
Simple Employee Q&A Chatbot for Altibase
A command-line chatbot that uses your vector database to answer questions about Altibase manuals.
"""

import os
import json
import time
import logging
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SimpleChatbot:
    """Simple Q&A Chatbot for Employee Onboarding"""
    
    def __init__(self, vector_db_path: str = "manual_vector_db", model_type: str = "claude"):
        """
        Initialize the chatbot
        
        Args:
            vector_db_path: Path to your vector database
            model_type: AI model to use ('claude', 'solar', or 'qwen')
        """
        self.vector_db_path = vector_db_path
        self.model_type = model_type.lower()
        self.vector_db = None
        self.vectorizer = None
        self.llm_client = None
        self.conversation_history = []
        
        # Model information
        self.model_info = {
            'claude': {
                'name': 'Claude 3 Sonnet',
                'provider': 'Anthropic API',
                'description': 'ìµœê³  í’ˆì§ˆ, API í‚¤ í•„ìš”, ìœ ë£Œ',
                'best_for': 'ì „ë°˜ì  ìµœê³  í’ˆì§ˆ'
            },
            'solar': {
                'name': 'Solar 10.7B',
                'provider': 'Ollama (Local)',
                'description': 'í•œêµ­ì–´ ìµœì í™”, ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰',
                'best_for': 'í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ'
            },
            'qwen': {
                'name': 'Qwen2.5 7B',
                'provider': 'Ollama (Local)',
                'description': 'ê¸°ìˆ  ë¬¸ì„œ íŠ¹í™”, ë¬´ë£Œ, ê²½ëŸ‰',
                'best_for': 'ê¸°ìˆ  ë¬¸ì„œ ì¼ë°˜'
            }
        }
        
        print("ğŸ¤– Altibase Employee Q&A Chatbot ì´ˆê¸°í™” ì¤‘...")
        self._show_model_info()
        self._load_vector_database()
        self._initialize_llm()
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _show_model_info(self):
        """Show information about the selected model"""
        if self.model_type in self.model_info:
            info = self.model_info[self.model_type]
            print(f"ğŸ§  ì„ íƒëœ ëª¨ë¸: {info['name']} ({info['provider']})")
            print(f"   íŠ¹ì§•: {info['description']}")
            print(f"   ìµœì  ìš©ë„: {info['best_for']}")
        
    def _load_vector_database(self):
        """Load your existing vector database"""
        try:
            # Import your existing vector system
            from paste import ManualVectorDB, ManualVectorizer
            
            print(f"ğŸ“š ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©: {self.vector_db_path}")
            self.vectorizer = ManualVectorizer()
            self.vector_db = ManualVectorDB.load(self.vector_db_path, self.vectorizer.vector_size)
            
            print(f"âœ… ë¡œë“œ ì™„ë£Œ: {self.vector_db.total_vectors:,}ê°œ ë²¡í„°, {len(self.vector_db.chunks):,}ê°œ ì²­í¬")
            
        except ImportError:
            print("âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   paste.py íŒŒì¼ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
        except Exception as e:
            print(f"âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            exit(1)
    
    def _initialize_llm(self):
        """Initialize the selected LLM"""
        print(f"ğŸš€ {self.model_type.upper()} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        if self.model_type == "claude":
            self._init_claude()
        elif self.model_type == "solar":
            self._init_local_llm("solar:10.7b")
        elif self.model_type == "qwen":
            self._init_local_llm("qwen2.5:7b")
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {self.model_type}")
            print("   ì§€ì› ëª¨ë¸: claude, solar, qwen")
            self._show_model_comparison()
            exit(1)
    
    def _init_claude(self):
        """Initialize Claude AI"""
        try:
            import anthropic
            
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                print("âŒ ANTHROPIC_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   export ANTHROPIC_API_KEY='your_api_key'")
                exit(1)
            
            self.llm_client = anthropic.Anthropic(api_key=api_key)
            print("âœ… Claude AI ì´ˆê¸°í™” ì™„ë£Œ")
            
        except ImportError:
            print("âŒ anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install anthropic")
            exit(1)
    
    def _init_local_llm(self, model_name: str):
        """Initialize local LLM via Ollama"""
        try:
            import requests
            
            # Test Ollama connection
            print(f"ğŸ”Œ Ollama ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘...")
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code != 200:
                    raise Exception("Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except requests.exceptions.RequestException:
                print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                self._show_ollama_setup(model_name)
                exit(1)
            
            # Check if model is available
            models = response.json().get('models', [])
            available_models = [model['name'] for model in models]
            
            # Check for exact model name or partial match
            model_found = False
            for available_model in available_models:
                if model_name in available_model or available_model.startswith(model_name.split(':')[0]):
                    model_found = True
                    # Use the exact model name found
                    self.llm_client = available_model
                    print(f"âœ… ë°œê²¬ëœ ëª¨ë¸ ì‚¬ìš©: {available_model}")
                    break
            
            if not model_found:
                print(f"âŒ {model_name} ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {[m['name'] for m in models]}")
                print(f"   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: ollama pull {model_name}")
                self._show_ollama_setup(model_name)
                exit(1)
            
            # Test model generation
            test_response = requests.post(
                "http://localhost:11434/api/generate",
                json={"model": self.llm_client, "prompt": "Hello", "stream": False},
                timeout=30
            )
            
            if test_response.status_code != 200:
                raise Exception(f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_response.status_code}")
                
            print(f"âœ… {self.llm_client} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            print(f"âŒ {model_name} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self._show_ollama_setup(model_name)
            exit(1)
    
    def _show_ollama_setup(self, model_name: str):
        """Show Ollama setup instructions"""
        print(f"\nğŸ“‹ {model_name} ì„¤ì • ë°©ë²•:")
        print("1. Ollama ì„¤ì¹˜: https://ollama.ai")
        print("2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ:")
        print(f"   ollama pull {model_name}")
        print("3. Ollama ì„œë²„ ì‹œì‘:")
        print("   ollama serve")
        print("4. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸:")
        print("   curl http://localhost:11434/api/tags")
        print("5. ë‹¤ì‹œ ì‹¤í–‰:")
        print(f"   python {__file__} --model {self.model_type}")
    
    def _show_model_comparison(self):
        """Show model comparison"""
        print("\nğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¹„êµ:")
        print("="*60)
        for model_type, info in self.model_info.items():
            print(f"ğŸ¤– {info['name']} ({model_type})")
            print(f"   ì œê³µì: {info['provider']}")
            print(f"   íŠ¹ì§•: {info['description']}")
            print(f"   ìµœì  ìš©ë„: {info['best_for']}")
            print()
        
        print("ğŸ’¡ ì¶”ì²œ:")
        print("â€¢ ìµœê³  í’ˆì§ˆì„ ì›í•œë‹¤ë©´: claude")
        print("â€¢ í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œì— íŠ¹í™”: solar") 
        print("â€¢ ê°€ë²¼ìš´ ë¡œì»¬ ëª¨ë¸: qwen")
    
    def get_relevant_context(self, question: str, top_k: int = 5) -> Tuple[str, List[Dict]]:
        """Get relevant context from vector database"""
        try:
            # Search for relevant chunks
            results = self.vector_db.search(
                question, 
                k=top_k, 
                vectorizer=self.vectorizer
            )
            
            if not results:
                return "", []
            
            # Build context
            context_parts = []
            citations = []
            
            for i, result in enumerate(results):
                chunk = result['chunk']
                metadata = chunk.get('metadata', {})
                
                context_part = f"""
[ì°¸ê³ ìë£Œ {i+1}] {chunk['manual_title']} - {chunk['section_key']}
ì±•í„°: {metadata.get('chapter', 'N/A')}
ìœ ì‚¬ë„: {result['similarity']*100:.1f}%

{chunk['text'][:600]}...
"""
                context_parts.append(context_part)
                
                citations.append({
                    'manual_title': chunk['manual_title'],
                    'section_key': chunk['section_key'],
                    'chapter': metadata.get('chapter', 'N/A'),
                    'similarity': result['similarity']
                })
            
            return "\n".join(context_parts), citations
            
        except Exception as e:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return "", []
    
    def generate_response(self, question: str, context: str) -> str:
        """Generate response using the selected LLM"""
        try:
            if self.model_type == "claude":
                return self._generate_claude_response(question, context)
            elif self.model_type in ["solar", "qwen"]:
                return self._generate_local_response(question, context, self.llm_client)
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _generate_claude_response(self, question: str, context: str) -> str:
        """Generate response using Claude"""
        system_prompt = """ë‹¹ì‹ ì€ Altibase ì œí’ˆ ì „ë¬¸ê°€ì´ë©°, ì‹ ì… ì§ì›ë“¤ì˜ ì˜¨ë³´ë”©ì„ ë•ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì œê³µëœ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
2. ë‹µë³€ì€ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”
3. ê°€ëŠ¥í•œ ê²½ìš° êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ë‹¨ê³„ë³„ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•˜ë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”"""

        user_prompt = f"""ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ìœ„ì˜ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        # Add conversation history
        messages = []
        for turn in self.conversation_history[-5:]:  # Last 5 turns
            messages.append({"role": "user", "content": turn["user"]})
            messages.append({"role": "assistant", "content": turn["assistant"]})
        
        messages.append({"role": "user", "content": user_prompt})
        
        response = self.llm_client.messages.create(
            model="claude-3-sonnet-20240229",
            max_tokens=2000,
            temperature=0.3,
            system=system_prompt,
            messages=messages
        )
        
        return response.content[0].text
    
    def _generate_local_response(self, question: str, context: str, model_name: str) -> str:
        """Generate response using local LLM via Ollama"""
        import requests
        
        # Optimize prompt based on model
        if "solar" in model_name.lower():
            # Solar is optimized for Korean
            system_prompt = """ë‹¹ì‹ ì€ Altibase ì œí’ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹ ì… ì§ì›ë“¤ì—ê²Œ ì¹œê·¼í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ì§€ì¹¨:
1. ì œê³µëœ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ì •í™•íˆ í™œìš©í•˜ì„¸ìš”
2. ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”  
3. ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”
4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”"""
            
            prompt = f"""{system_prompt}

ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        else:  # qwen and others
            # Qwen is good with structured prompts
            system_prompt = """ë‹¹ì‹ ì€ Altibase ì œí’ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹ ì… ì§ì›ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì§€ì¹¨:
- ì œê³µëœ ë§¤ë‰´ì–¼ ì •ë³´ë¥¼ ì •í™•íˆ í™œìš©í•˜ì„¸ìš”
- ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
- ì‹¤ë¬´ì— ë„ì›€ì´ ë˜ëŠ” ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”"""
            
            prompt = f"""{system_prompt}

ë§¤ë‰´ì–¼ ì •ë³´:
{context}

ì§ˆë¬¸: {question}

í•œêµ­ì–´ë¡œ ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:"""
        
        try:
            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "top_p": 0.9,
                        "top_k": 40,
                        "num_predict": 2000,
                        "repeat_penalty": 1.1
                    }
                },
                timeout=120  # Local models might be slower
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                raise Exception(f"Ollama API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                
        except requests.exceptions.Timeout:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except Exception as e:
            raise Exception(f"ë¡œì»¬ ëª¨ë¸ ì˜¤ë¥˜: {str(e)}")
    
    def chat(self, question: str) -> Dict[str, Any]:
        """Main chat function"""
        start_time = time.time()
        
        # Get relevant context
        print("ğŸ” ê´€ë ¨ ì •ë³´ ê²€ìƒ‰ ì¤‘...")
        context, citations = self.get_relevant_context(question)
        
        # Generate response
        print("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘...")
        response = self.generate_response(question, context)
        
        # Save to history
        self.conversation_history.append({
            "user": question,
            "assistant": response,
            "timestamp": datetime.now().isoformat()
        })
        
        # Keep only last 10 conversations
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
        
        return {
            "question": question,
            "answer": response,
            "citations": citations,
            "response_time": time.time() - start_time,
            "model": self.model_type
        }
    
    def run_cli(self):
        """Run the command-line interface"""
        print("\n" + "="*80)
        print("          ğŸ¤– Altibase Employee Q&A Chatbot")
        print("="*80)
        
        model_info = self.model_info[self.model_type]
        print(f"ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {model_info['name']} ({model_info['provider']})")
        print(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤: {self.vector_db.total_vectors:,}ê°œ ë²¡í„°")
        print(f"ìµœì  ìš©ë„: {model_info['best_for']}")
        
        print("\nëª…ë ¹ì–´:")
        print("  ì§ˆë¬¸ ì…ë ¥      - ì¼ë°˜ ì§ˆë¬¸")
        print("  /help         - ë„ì›€ë§")
        print("  /models       - ëª¨ë¸ ë¹„êµ")
        print("  /switch       - ëª¨ë¸ ë³€ê²½")
        print("  /clear        - ëŒ€í™” ê¸°ë¡ ì‚­ì œ")
        print("  /quit         - ì¢…ë£Œ")
        print("="*80)
        
        while True:
            try:
                question = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if not question:
                    continue
                
                # Handle commands
                if question == "/quit":
                    print("ğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                elif question == "/help":
                    self._show_help()
                    continue
                
                elif question == "/models":
                    self._show_model_comparison()
                    continue
                
                elif question == "/switch":
                    self._switch_model()
                    continue
                
                elif question == "/clear":
                    self.conversation_history = []
                    print("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    continue
                
                # Generate response
                result = self.chat(question)
                
                # Display response
                print(f"\nğŸ¤– **ë‹µë³€:**")
                print(result['answer'])
                
                # Display citations
                if result['citations']:
                    print(f"\nğŸ“š **ì°¸ê³  ìë£Œ:**")
                    for i, citation in enumerate(result['citations'][:3]):
                        print(f"{i+1}. {citation['manual_title']} - {citation['section_key']}")
                        print(f"   ğŸ“Š ìœ ì‚¬ë„: {citation['similarity']*100:.1f}%")
                
                print(f"\nâ±ï¸ ì‘ë‹µ ì‹œê°„: {result['response_time']:.2f}ì´ˆ | ëª¨ë¸: {result['model'].upper()}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _switch_model(self):
        """Switch between models"""
        print("\nğŸ”„ ëª¨ë¸ ë³€ê²½:")
        print("1. claude  - Claude 3 Sonnet (ìµœê³  í’ˆì§ˆ, API í‚¤ í•„ìš”)")
        print("2. solar   - Solar 10.7B (í•œêµ­ì–´ íŠ¹í™”, ë¡œì»¬)")
        print("3. qwen    - Qwen2.5 7B (ê¸°ìˆ  ë¬¸ì„œ íŠ¹í™”, ë¡œì»¬)")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        model_map = {'1': 'claude', '2': 'solar', '3': 'qwen'}
        
        if choice in model_map:
            new_model = model_map[choice]
            if new_model != self.model_type:
                print(f"\nğŸ”„ {new_model} ëª¨ë¸ë¡œ ë³€ê²½ ì¤‘...")
                try:
                    old_model = self.model_type
                    self.model_type = new_model
                    self._initialize_llm()
                    print(f"âœ… {self.model_info[new_model]['name']} ëª¨ë¸ë¡œ ë³€ê²½ ì™„ë£Œ!")
                except Exception as e:
                    print(f"âŒ ëª¨ë¸ ë³€ê²½ ì‹¤íŒ¨: {e}")
                    print("ê¸°ì¡´ ëª¨ë¸ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
                    self.model_type = old_model
            else:
                print("â„¹ï¸  ì´ë¯¸ ì„ íƒëœ ëª¨ë¸ì…ë‹ˆë‹¤.")
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
    
    def _show_help(self):
        """Show help information"""
        print(f"""
ğŸ“‹ ì‚¬ìš©ë²•:
â€¢ ìì—°ì–´ë¡œ ì§ˆë¬¸í•˜ì„¸ìš” (ì˜ˆ: "Altibase ì„¤ì¹˜ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”")
â€¢ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¼ìˆ˜ë¡ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤

ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ:
â€¢ "Altibaseê°€ ë¬´ì—‡ì¸ê°€ìš”?"
â€¢ "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
â€¢ "SQL ì¿¼ë¦¬ ìµœì í™” ë°©ë²•ì€?"
â€¢ "ë°±ì—…ê³¼ ë³µêµ¬ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
â€¢ "ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
â€¢ "ì—ëŸ¬ ì½”ë“œë³„ í•´ê²° ë°©ë²•ì€?"
â€¢ "í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
â€¢ "ë©”ëª¨ë¦¬ ê´€ë¦¬ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"

ğŸ”§ ëª…ë ¹ì–´:
/help    - ì´ ë„ì›€ë§ í‘œì‹œ
/models  - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¹„êµ
/switch  - ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½
/clear   - ëŒ€í™” ê¸°ë¡ ì‚­ì œ
/quit    - í”„ë¡œê·¸ë¨ ì¢…ë£Œ

ğŸ¤– í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸: {self.model_info[self.model_type]['name']}
   íŠ¹ì§•: {self.model_info[self.model_type]['description']}
""")

def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Altibase Employee Q&A Chatbot with Claude API and Local LLM')
    parser.add_argument('--vector-db', type=str, default='manual_vector_db',
                       help='Vector database directory path')
    parser.add_argument('--model', type=str, default='claude',
                       choices=['claude', 'solar', 'qwen'],
                       help='AI model to use')
    
    args = parser.parse_args()
    
    # Show initial model information
    print("ğŸš€ Altibase Employee Q&A Chatbot")
    print("="*50)
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
    print("â€¢ claude - Claude 3 Sonnet (ìµœê³  í’ˆì§ˆ, API í‚¤ í•„ìš”)")
    print("â€¢ solar  - Solar 10.7B (í•œêµ­ì–´ íŠ¹í™”, ë¬´ë£Œ ë¡œì»¬)")
    print("â€¢ qwen   - Qwen2.5 7B (ê¸°ìˆ  ë¬¸ì„œ íŠ¹í™”, ë¬´ë£Œ ë¡œì»¬)")
    print(f"\nì„ íƒëœ ëª¨ë¸: {args.model}")
    
    if args.model == 'claude':
        if not os.getenv('ANTHROPIC_API_KEY'):
            print("\nâš ï¸  Claude ì‚¬ìš©ì„ ìœ„í•´ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤:")
            print("   export ANTHROPIC_API_KEY='your_api_key'")
            print("   ë˜ëŠ” ë¡œì»¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”: --model solar")
    elif args.model in ['solar', 'qwen']:
        print(f"\nğŸ’¡ {args.model} ëª¨ë¸ ì„¤ì • ë°©ë²•:")
        model_name = "solar:10.7b" if args.model == 'solar' else "qwen2.5:7b"
        print(f"1. Ollama ì„¤ì¹˜: https://ollama.ai")
        print(f"2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: ollama pull {model_name}")
        print(f"3. ì„œë²„ ì‹œì‘: ollama serve")
        print(f"4. í™•ì¸: curl http://localhost:11434/api/tags")
    
    print("="*50)
    
    try:
        # Initialize and run chatbot
        chatbot = SimpleChatbot(args.vector_db, args.model)
        chatbot.run_cli()
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()