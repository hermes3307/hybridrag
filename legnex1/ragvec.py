import requests
from bs4 import BeautifulSoup
import os
import anthropic
import time
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from typing import List, Dict, Tuple
import re

class LIGNex1RAGInterviewChatbot:
    def __init__(self):
        # Claude API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
        api_key = os.getenv('CLAUDE_API_KEY')
        if api_key:
            try:
                self.client = anthropic.Anthropic(api_key=api_key)
                self.ai_mode = True
            except:
                self.ai_mode = False
        else:
            self.ai_mode = False
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©ì¤‘ì…ë‹ˆë‹¤...")
        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        # ì›¹ì‚¬ì´íŠ¸ URL ëª©ë¡
        self.urls = [
            "https://www.lignex1.com/people/talent.do",
            "https://www.lignex1.com/people/life.do", 
            "https://www.lignex1.com/people/welfare.do",
            "https://www.lignex1.com/people/education.do",
            "https://www.lignex1.com/people/jobs.do",
            "https://www.lignex1.com/people/jobsemp.do"
        ]
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨
        self.vector_db = None
        self.documents = []
        self.document_metadata = []
        
        # ì§ˆë¬¸ ë° ìƒíƒœ
        self.questions = []
        self.current_question = 0
        
        # ë²¡í„° DB íŒŒì¼ ê²½ë¡œ
        self.vector_db_path = "lignex1_vector_db.pkl"
        self.faiss_index_path = "lignex1_faiss.index"
        
    def extract_text_from_url(self, url: str) -> str:
        """URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = soup.get_text()
            
            # ì •ë¦¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
            
        except Exception as e:
            print(f"URL {url} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• """
        sentences = re.split(r'[.!?]\s+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 50]
    
    def build_vector_database(self):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        # ê¸°ì¡´ ë²¡í„° DBê°€ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(self.vector_db_path) and os.path.exists(self.faiss_index_path):
            print("ğŸ“š ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
            self.load_vector_database()
            return
        
        print("ğŸ”¨ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•ì¤‘ì…ë‹ˆë‹¤...")
        all_chunks = []
        all_metadata = []
        
        # ê° URLì—ì„œ ë‚´ìš© ì¶”ì¶œ ë° ì²­í‚¹
        for url in self.urls:
            print(f"ì²˜ë¦¬ì¤‘: {url}")
            content = self.extract_text_from_url(url)
            if content:
                chunks = self.chunk_text(content)
                for i, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'url': url,
                        'chunk_id': i,
                        'page_type': self.get_page_type(url)
                    })
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        self.documents = all_chunks
        self.document_metadata = all_metadata
        
        # ì„ë² ë”© ìƒì„±
        print("ğŸ”¢ ì„ë² ë”©ì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤...")
        embeddings = self.embedding_model.encode(all_chunks, show_progress_bar=True)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        print("ğŸ—‚ï¸ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±ì¤‘ì…ë‹ˆë‹¤...")
        dimension = embeddings.shape[1]
        self.vector_db = faiss.IndexFlatIP(dimension)  # Inner Product (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
        
        # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        faiss.normalize_L2(embeddings)
        self.vector_db.add(embeddings.astype('float32'))
        
        # ë²¡í„° DB ì €ì¥
        self.save_vector_database()
        print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    
    def get_page_type(self, url: str) -> str:
        """URLì—ì„œ í˜ì´ì§€ íƒ€ì… ì¶”ì¶œ"""
        if 'talent' in url:
            return 'ì¸ì¬ìƒ'
        elif 'life' in url:
            return 'ì¡°ì§ë¬¸í™”'
        elif 'welfare' in url:
            return 'ë³µë¦¬í›„ìƒ'
        elif 'education' in url:
            return 'êµìœ¡ì œë„'
        elif 'jobs' in url:
            return 'ì±„ìš©ì •ë³´'
        elif 'jobsemp' in url:
            return 'ì±„ìš©ì „í˜•'
        return 'ê¸°íƒ€'
    
    def save_vector_database(self):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        # ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(self.vector_db_path, 'wb') as f:
            pickle.dump({
                'documents': self.documents,
                'metadata': self.document_metadata
            }, f)
        
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        faiss.write_index(self.vector_db, self.faiss_index_path)
    
    def load_vector_database(self):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©"""
        try:
            # ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ë¡œë”©
            with open(self.vector_db_path, 'rb') as f:
                data = pickle.load(f)
                self.documents = data['documents']
                self.document_metadata = data['metadata']
            
            # FAISS ì¸ë±ìŠ¤ ë¡œë”©
            self.vector_db = faiss.read_index(self.faiss_index_path)
            
            print(f"âœ… ë²¡í„° DB ë¡œë”© ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
        except Exception as e:
            print(f"ë²¡í„° DB ë¡œë”© ì‹¤íŒ¨: {e}")
            self.build_vector_database()
    
    def search_relevant_content(self, query: str, top_k: int = 3) -> List[Dict]:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš© ê²€ìƒ‰"""
        if self.vector_db is None:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰
        scores, indices = self.vector_db.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.documents):
                results.append({
                    'content': self.documents[idx],
                    'metadata': self.document_metadata[idx],
                    'score': float(score),
                    'rank': i + 1
                })
        
        return results
    
    def generate_rag_questions(self) -> List[str]:
        """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ìƒì„±"""
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰
        categories = ['ì¸ì¬ìƒ', 'ì¡°ì§ë¬¸í™”', 'ë³µë¦¬í›„ìƒ', 'êµìœ¡ì œë„', 'ì±„ìš©']
        questions = []
        
        for category in categories:
            relevant_docs = self.search_relevant_content(category, top_k=2)
            context = "\n".join([doc['content'] for doc in relevant_docs])
            
            if self.ai_mode and context:
                # Claude APIë¥¼ ì‚¬ìš©í•œ ì§ˆë¬¸ ìƒì„±
                try:
                    prompt = f"""
                    ë‹¤ìŒ LIGë„¥ìŠ¤ì› {category} ê´€ë ¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
                    
                    {context[:800]}
                    
                    ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ê³  ì§€ì›ìì˜ ì´í•´ë„ì™€ ì í•©ì„±ì„ í‰ê°€í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
                    í˜•ì‹: ì§ˆë¬¸ë§Œ ë°˜í™˜ (ë²ˆí˜¸ ì—†ì´)
                    """
                    
                    message = self.client.messages.create(
                        model="claude-3-5-sonnet-20241022",
                        max_tokens=200,
                        messages=[{"role": "user", "content": prompt}]
                    )
                    
                    question = message.content[0].text.strip()
                    if question:
                        questions.append(question)
                        
                except Exception as e:
                    print(f"AI ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ ({category}): {e}")
        
        # ê¸°ë³¸ ì§ˆë¬¸ ì¶”ê°€
        base_questions = [
            "LIGë„¥ìŠ¤ì›ì˜ ì¸ì¬ìƒì¸ 'OPEN'ê³¼ 'POSITIVE'ì— ëŒ€í•´ ì„¤ëª…í•˜ê³ , ë³¸ì¸ì´ ì´ì— ë¶€í•©í•˜ëŠ” ì‚¬ë¡€ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "ë°©ìœ„ì‚°ì—… ë¶„ì•¼ì—ì„œ ì¼í•˜ëŠ” ê²ƒì— ëŒ€í•œ ë³¸ì¸ì˜ ìƒê°ê³¼ ê°ì˜¤ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "LIGë„¥ìŠ¤ì›ì˜ ì¡°ì§ë¬¸í™” ì¤‘ ë³¸ì¸ê³¼ ê°€ì¥ ë¶€í•©í•˜ëŠ” ê°€ì¹˜ëŠ” ë¬´ì—‡ì´ë©° ê·¸ ì´ìœ ëŠ”?",
            "í˜ì‹ ê³¼ ì°½ì¡°ì  ì‚¬ê³ ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•œ ê²½í—˜ì´ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "LIGë„¥ìŠ¤ì›ì—ì„œ ì–´ë–¤ ì „ë¬¸ê°€ê°€ ë˜ê³  ì‹¶ìœ¼ë©°, ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        # AI ìƒì„± ì§ˆë¬¸ì´ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ë³´ì™„
        if len(questions) < 3:
            questions.extend(base_questions[:5-len(questions)])
        
        return questions[:5]  # ìµœëŒ€ 5ê°œ ì§ˆë¬¸
    
    def evaluate_answer_with_rag(self, question: str, answer: str) -> str:
        """RAGë¥¼ ì‚¬ìš©í•œ ë‹µë³€ í‰ê°€"""
        # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        relevant_docs = self.search_relevant_content(question + " " + answer, top_k=3)
        context = "\n".join([f"[{doc['metadata']['page_type']}] {doc['content'][:200]}..." 
                           for doc in relevant_docs])
        
        # AI í‰ê°€
        if self.ai_mode and context:
            try:
                prompt = f"""
                LIGë„¥ìŠ¤ì› ì„ì›ìœ¼ë¡œì„œ ë‹¤ìŒ ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”.
                
                ê´€ë ¨ íšŒì‚¬ ì •ë³´:
                {context}
                
                ì§ˆë¬¸: {question}
                ë‹µë³€: {answer}
                
                í‰ê°€ ê¸°ì¤€:
                1. íšŒì‚¬ ì •ë³´ì™€ì˜ ì¼ì¹˜ë„
                2. ë‹µë³€ì˜ êµ¬ì²´ì„±ê³¼ ì§„ì •ì„±
                3. LIGë„¥ìŠ¤ì› ê°€ì¹˜ê´€ ë¶€í•©ë„
                
                ê°„ë‹¨ëª…ë£Œí•œ í”¼ë“œë°±(3-4ë¬¸ì¥)ì„ ì œê³µí•´ì£¼ì„¸ìš”.
                """
                
                message = self.client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=300,
                    messages=[{"role": "user", "content": prompt}]
                )
                
                return f"ğŸ¤– RAG ê¸°ë°˜ AI í‰ê°€:\n{message.content[0].text}"
                
            except Exception as e:
                print(f"AI í‰ê°€ ì‹¤íŒ¨: {e}")
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€
        return self.keyword_evaluation(question, answer, relevant_docs)
    
    def keyword_evaluation(self, question: str, answer: str, context_docs: List[Dict]) -> str:
        """í–¥ìƒëœ í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€"""
        answer_lower = answer.lower()
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        context_keywords = set()
        for doc in context_docs:
            content_words = re.findall(r'\b\w+\b', doc['content'].lower())
            context_keywords.update([word for word in content_words if len(word) > 2])
        
        # ê¸°ë³¸ í‚¤ì›Œë“œë“¤
        company_keywords = ['lignex1', 'ligë„¥ìŠ¤ì›', 'ë°©ìœ„ì‚°ì—…', 'êµ­ë°©', 'ì•ˆë³´']
        value_keywords = ['open', 'positive', 'ê°œë°©', 'ê¸ì •', 'pride', 'trust', 'passion', 'enjoy']
        professional_keywords = ['ì „ë¬¸ê°€', 'ì—­ëŸ‰', 'ì„±ì¥', 'í•™ìŠµ', 'í˜ì‹ ', 'ì°½ì˜', 'ë„ì „']
        
        score = 0
        feedback_points = []
        
        # ë‹µë³€ ê¸¸ì´ ì²´í¬
        if len(answer.strip()) < 10:
            return "ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
        
        # ë¶€ì ì ˆí•œ ë‹µë³€ ì²´í¬
        inappropriate = ['ë˜¥', 'ë°”ë³´', 'ì‹«ë‹¤', 'ëª¨ë¥´ê² ë‹¤', 'ì•„ë¬´ê±°ë‚˜']
        if any(word in answer_lower for word in inappropriate):
            return "âŒ ë©´ì ‘ì— ì ì ˆí•˜ì§€ ì•Šì€ ë‹µë³€ì…ë‹ˆë‹¤. ì§„ì†”í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± í‰ê°€
        context_match = len([word for word in answer_lower.split() if word in context_keywords])
        if context_match > 3:
            score += 3
            feedback_points.append("âœ… íšŒì‚¬ ì •ë³´ì™€ ì˜ ì—°ê²°ëœ ë‹µë³€ì…ë‹ˆë‹¤")
        elif context_match > 1:
            score += 1
            feedback_points.append("âœ… íšŒì‚¬ ì •ë³´ë¥¼ ì¼ë¶€ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤")
        
        # ê¸°ì¡´ í‰ê°€ ë¡œì§ë“¤
        if any(keyword in answer_lower for keyword in company_keywords):
            score += 2
            feedback_points.append("âœ… íšŒì‚¬ì— ëŒ€í•œ ì´í•´ë„ê°€ ì¢‹ìŠµë‹ˆë‹¤")
        
        if any(keyword in answer_lower for keyword in value_keywords):
            score += 2
            feedback_points.append("âœ… íšŒì‚¬ ê°€ì¹˜ê´€ê³¼ ì˜ ë¶€í•©í•©ë‹ˆë‹¤")
        
        if any(keyword in answer_lower for keyword in professional_keywords):
            score += 1
            feedback_points.append("âœ… ì „ë¬¸ì ì¸ ì‚¬ê³ ê°€ ë‹ë³´ì…ë‹ˆë‹¤")
        
        # ì¢…í•© í‰ê°€
        if score >= 6:
            overall = "ğŸŒŸ ë§¤ìš° ìš°ìˆ˜í•œ ë‹µë³€ì…ë‹ˆë‹¤!"
        elif score >= 4:
            overall = "ğŸ‘ ì¢‹ì€ ë‹µë³€ì…ë‹ˆë‹¤."
        elif score >= 2:
            overall = "ğŸ’¡ ê´œì°®ì€ ë‹µë³€ì´ì§€ë§Œ ë” ë³´ì™„í•´ë³´ì„¸ìš”."
        else:
            overall = "ğŸ“š íšŒì‚¬ ì •ë³´ë¥¼ ë” ë°˜ì˜í•œ ë‹µë³€ì„ í•´ë³´ì„¸ìš”."
        
        if not feedback_points:
            feedback_points.append("ë” êµ¬ì²´ì ì¸ ì‚¬ë¡€ë‚˜ íšŒì‚¬ ê´€ë ¨ ë‚´ìš©ì„ í¬í•¨í•´ë³´ì„¸ìš”")
        
        # ê´€ë ¨ ì •ë³´ íŒíŠ¸ ì œê³µ
        if context_docs:
            hint_info = context_docs[0]['metadata']['page_type']
            feedback_points.append(f"ğŸ’¡ íŒíŠ¸: {hint_info} ê´€ë ¨ ë‚´ìš©ì„ ë” í™œìš©í•´ë³´ì„¸ìš”")
        
        return f"ğŸ“Š RAG ê¸°ë°˜ í‚¤ì›Œë“œ í‰ê°€:\n{overall}\n" + "\n".join(feedback_points)
    
    def run_interview(self):
        """ë©´ì ‘ ì§„í–‰"""
        print("\n" + "="*60)
        print("ğŸ¯ LIGë„¥ìŠ¤ì› RAG ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
        print("="*60)
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•/ë¡œë”©
        self.build_vector_database()
        
        # RAG ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
        print("ğŸ§  RAG ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤...")
        self.questions = self.generate_rag_questions()
        
        print(f"\nğŸ“‹ ì´ {len(self.questions)}ê°œì˜ ì§ˆë¬¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ê° ì§ˆë¬¸ì— ì„±ì‹¤íˆ ë‹µë³€í•´ ì£¼ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
        print("ğŸ’¡ ë‹µë³€ì€ LIGë„¥ìŠ¤ì› ì›¹ì‚¬ì´íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.\n")
        
        for i, question in enumerate(self.questions, 1):
            print(f"\nğŸ“Œ ì§ˆë¬¸ {i}/{len(self.questions)}")
            print("-" * 50)
            print(question)
            print("-" * 50)
            
            answer = input("\nğŸ’¬ ë‹µë³€: ").strip()
            
            if answer.lower() == 'quit':
                print("\në©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!")
                break
                
            if answer:
                print("\nâ³ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ì„ í‰ê°€ì¤‘ì…ë‹ˆë‹¤...")
                feedback = self.evaluate_answer_with_rag(question, answer)
                print(f"\nğŸ“ í‰ê°€ ë° í”¼ë“œë°±:")
                print(feedback)
                print("\n" + "="*50)
            else:
                print("ë‹µë³€ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                continue
        
        print("\nğŸ‰ RAG ê¸°ë°˜ ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("LIGë„¥ìŠ¤ì› ì§€ì›ì— í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ’ª")

def main():
    print("ğŸš€ LIGë„¥ìŠ¤ì› RAG ë©´ì ‘ ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    
    # í•„ìš” íŒ¨í‚¤ì§€ í™•ì¸
    try:
        import sentence_transformers
        import faiss
    except ImportError:
        print("âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("pip install sentence-transformers faiss-cpu")
        return
    
    # API í‚¤ í™•ì¸ (ì„ íƒì‚¬í•­)
    api_key = os.getenv('CLAUDE_API_KEY')
    if api_key:
        print("âœ… Claude API í‚¤ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. RAG + AI í‰ê°€ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    else:
        print("â„¹ï¸ Claude API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. RAG + í‚¤ì›Œë“œ í‰ê°€ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # API í‚¤ ì…ë ¥ ì˜µì…˜
        user_input = input("\nClaude API í‚¤ë¥¼ ì…ë ¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
        if user_input == 'y':
            user_key = input("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
            if user_key:
                os.environ['CLAUDE_API_KEY'] = user_key
                print("âœ… API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    try:
        # RAG ì±—ë´‡ ì‹¤í–‰
        chatbot = LIGNex1RAGInterviewChatbot()
        chatbot.run_interview()
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()