import requests
from bs4 import BeautifulSoup
import os
import time
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from typing import List, Dict, Tuple
import re
import json

class LIGNex1RAGInterviewChatbot:
    def __init__(self):
        # ë‹¤ì¤‘ AI ëª¨ë¸ ì§€ì›
        self.ai_providers = {
            'openai': self.setup_openai(),
            'claude': self.setup_claude(),
            'ollama': self.setup_ollama(),
            'huggingface': self.setup_huggingface()
        }
        
        self.active_provider = self.select_best_provider()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©ì¤‘ì…ë‹ˆë‹¤...")
        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
        # ì›¹ì‚¬ì´íŠ¸ URL ëª©ë¡
        self.urls = [
            "https://www.lignex1.com/people/talent.do",
            "https://www.lignex1.com/people/life.do", 
            "https://www.lignex1.com/people/welfare.do",
            "https://www.lignex1.com/people/education.do",
            "https://www.lignex1.com/people/jobs.do",
            "https://www.lignex1.com/people/jobsemp.do"
        ]
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨
        self.vector_db = None
        self.documents = []
        self.document_metadata = []
        
        # ì§ˆë¬¸ ë° ìƒíƒœ
        self.questions = []
        self.current_question = 0
        
        # ë²¡í„° DB íŒŒì¼ ê²½ë¡œ
        self.vector_db_path = "lignex1_vector_db.pkl"
        self.faiss_index_path = "lignex1_faiss.index"
    
    def advanced_keyword_evaluation(self, question: str, answer: str, context_docs: List[Dict]) -> str:
        """ê³ ê¸‰ í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€ ì‹œìŠ¤í…œ"""
        answer_lower = answer.lower()
        question_lower = question.lower()
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        context_keywords = set()
        context_text = ""
        for doc in context_docs:
            context_text += doc['content'].lower() + " "
            content_words = re.findall(r'\b\w+\b', doc['content'].lower())
            context_keywords.update([word for word in content_words if len(word) > 2])
        
        # ê³ ê¸‰ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬
        core_values = {
            'open': ['ê°œë°©', 'open', 'í˜ì‹ ', 'ì°½ì˜', 'ë³€í™”', 'ë„ì „', 'ìƒˆë¡œìš´', 'ì°½ì¡°'],
            'positive': ['ê¸ì •', 'positive', 'ì—´ì •', 'ëª©í‘œ', 'ë‹¬ì„±', 'ì„±ì·¨', 'ì˜ì§€', 'ë…¸ë ¥']
        }
        
        culture_values = {
            'pride': ['ìë¶€ì‹¬', 'pride', 'ìì‹ ê°', 'ë¿Œë“¯', 'ì„±ê³¼', 'ì—…ì '],
            'trust': ['ì‹ ë¢°', 'trust', 'ë¯¿ìŒ', 'ì†Œí†µ', 'í˜‘ë ¥', 'íŒ€ì›Œí¬'],
            'passion': ['ì—´ì •', 'passion', 'ëª°ì…', 'ì „ë…', 'ì§‘ì¤‘'],
            'enjoy': ['ì¦ê±°ì›€', 'enjoy', 'ì¬ë¯¸', 'í–‰ë³µ', 'ë§Œì¡±', 'ê· í˜•']
        }
        
        professional_terms = ['ì „ë¬¸ê°€', 'ì—­ëŸ‰', 'ì„±ì¥', 'í•™ìŠµ', 'ë°œì „', 'í–¥ìƒ', 'ê²½í—˜', 'í”„ë¡œì íŠ¸', 'ì„±ê³¼', 'ê²°ê³¼']
        company_terms = ['lignex1', 'ligë„¥ìŠ¤ì›', 'ë°©ìœ„ì‚°ì—…', 'êµ­ë°©', 'ì•ˆë³´', 'êµ°ì‚¬', 'êµ­ê°€']
        
        # í‰ê°€ ì ìˆ˜ ê³„ì‚°
        scores = {
            'context_relevance': 0,
            'core_values': 0,
            'culture_fit': 0,
            'professionalism': 0,
            'company_understanding': 0,
            'specificity': 0,
            'authenticity': 0
        }
        
        feedback_details = []
        
        # 1. ë§¥ë½ ê´€ë ¨ì„± í‰ê°€ (30ì )
        context_matches = len([word for word in answer_lower.split() if word in context_keywords])
        if context_matches > 5:
            scores['context_relevance'] = 30
            feedback_details.append("âœ… íšŒì‚¬ ì •ë³´ë¥¼ ë§¤ìš° ì˜ í™œìš©í•œ ë‹µë³€")
        elif context_matches > 2:
            scores['context_relevance'] = 20
            feedback_details.append("âœ… íšŒì‚¬ ì •ë³´ë¥¼ ì ì ˆíˆ ë°˜ì˜í•œ ë‹µë³€")
        elif context_matches > 0:
            scores['context_relevance'] = 10
            feedback_details.append("âš¡ íšŒì‚¬ ì •ë³´ í™œìš©ë„ë¥¼ ë†’ì—¬ë³´ì„¸ìš”")
        
        # 2. í•µì‹¬ ê°€ì¹˜ ë¶€í•©ë„ (25ì )
        for value_type, keywords in core_values.items():
            if any(keyword in answer_lower for keyword in keywords):
                scores['core_values'] += 12
                feedback_details.append(f"âœ… {value_type.upper()} ê°€ì¹˜ê´€ì´ ì˜ ë“œëŸ¬ë‚¨")
        
        # 3. ì¡°ì§ë¬¸í™” ì í•©ì„± (20ì )
        culture_match_count = 0
        for culture_type, keywords in culture_values.items():
            if any(keyword in answer_lower for keyword in keywords):
                culture_match_count += 1
                scores['culture_fit'] += 5
        
        if culture_match_count >= 2:
            feedback_details.append("âœ… ì¡°ì§ë¬¸í™”ì™€ ì˜ ë¶€í•©í•˜ëŠ” ë‹µë³€")
        elif culture_match_count == 1:
            feedback_details.append("âš¡ ì¡°ì§ë¬¸í™” ì´í•´ë„ë¥¼ ë” ë³´ì—¬ì£¼ì„¸ìš”")
        
        # 4. ì „ë¬¸ì„± (10ì )
        prof_matches = sum(1 for term in professional_terms if term in answer_lower)
        if prof_matches >= 3:
            scores['professionalism'] = 10
            feedback_details.append("âœ… ì „ë¬¸ì ì¸ ê´€ì ì´ ë‹ë³´ì„")
        elif prof_matches >= 1:
            scores['professionalism'] = 5
        
        # 5. íšŒì‚¬ ì´í•´ë„ (10ì )
        comp_matches = sum(1 for term in company_terms if term in answer_lower)
        if comp_matches >= 1:
            scores['company_understanding'] = 10
            feedback_details.append("âœ… íšŒì‚¬ì— ëŒ€í•œ ì´í•´ë„ê°€ ì¢‹ìŒ")
        
        # 6. êµ¬ì²´ì„± í‰ê°€ (3ì )
        experience_indicators = ['ê²½í—˜', 'ì‚¬ë¡€', 'í”„ë¡œì íŠ¸', 'í™œë™', 'ë•Œ', 'ìƒí™©', 'ê³¼ì •']
        if any(indicator in answer_lower for indicator in experience_indicators):
            scores['specificity'] = 3
            feedback_details.append("âœ… êµ¬ì²´ì ì¸ ê²½í—˜ì„ ì˜ ì„¤ëª…í•¨")
        
        # 7. ì§„ì •ì„± í‰ê°€ (2ì )
        if len(answer.strip()) >= 100 and '.' in answer:
            scores['authenticity'] = 2
        elif len(answer.strip()) >= 50:
            scores['authenticity'] = 1
        
        # ë¶€ì ì ˆí•œ ë‹µë³€ ì²´í¬
        inappropriate = ['ë˜¥', 'ë°”ë³´', 'ì‹«ë‹¤', 'ëª¨ë¥´ê² ë‹¤', 'ì•„ë¬´ê±°ë‚˜', 'ëª°ë¼']
        if any(word in answer_lower for word in inappropriate):
            return "âŒ ë©´ì ‘ì— ì ì ˆí•˜ì§€ ì•Šì€ ë‹µë³€ì…ë‹ˆë‹¤. ì§„ì†”í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
        
        # ë„ˆë¬´ ì§§ì€ ë‹µë³€
        if len(answer.strip()) < 20:
            return "ğŸ“ ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ì„¤ëª…ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
        
        # ì´ì  ê³„ì‚°
        total_score = sum(scores.values())
        
        # ë“±ê¸‰ ê²°ì •
        if total_score >= 80:
            grade = "ğŸŒŸ ìš°ìˆ˜ (A)"
            overall_comment = "ë§¤ìš° ì¸ìƒì ì¸ ë‹µë³€ì…ë‹ˆë‹¤!"
        elif total_score >= 60:
            grade = "ğŸ‘ ì–‘í˜¸ (B)"
            overall_comment = "ì¢‹ì€ ë‹µë³€ì…ë‹ˆë‹¤."
        elif total_score >= 40:
            grade = "âš¡ ë³´í†µ (C)"
            overall_comment = "ë‹µë³€ì„ ë” ë³´ì™„í•´ë³´ì„¸ìš”."
        else:
            grade = "ğŸ“š ë¯¸í¡ (D)"
            overall_comment = "íšŒì‚¬ ì •ë³´ë¥¼ ë” í™œìš©í•œ ë‹µë³€ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        # ê°œì„  ì œì•ˆ
        improvement_suggestions = []
        if scores['context_relevance'] < 20:
            improvement_suggestions.append("ğŸ’¡ íšŒì‚¬ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ë¥¼ ë” í™œìš©í•´ë³´ì„¸ìš”")
        if scores['core_values'] < 15:
            improvement_suggestions.append("ğŸ’¡ OPEN, POSITIVE ê°€ì¹˜ê´€ì„ ë” ê°•ì¡°í•´ë³´ì„¸ìš”")
        if scores['culture_fit'] < 10:
            improvement_suggestions.append("ğŸ’¡ Pride, Trust, Passion, Enjoy ë¬¸í™”ë¥¼ ì–¸ê¸‰í•´ë³´ì„¸ìš”")
        if scores['specificity'] < 2:
            improvement_suggestions.append("ğŸ’¡ êµ¬ì²´ì ì¸ ê²½í—˜ì´ë‚˜ ì‚¬ë¡€ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”")
        
        # ê´€ë ¨ ì •ë³´ íŒíŠ¸ ì œê³µ
        if context_docs:
            relevant_categories = list(set([doc['metadata']['page_type'] for doc in context_docs]))
            if relevant_categories:
                improvement_suggestions.append(f"ğŸ’¡ ì°¸ê³ : {', '.join(relevant_categories)} ê´€ë ¨ ë‚´ìš©ì„ ë” í™œìš©í•´ë³´ì„¸ìš”")
        
        # ê²°ê³¼ í¬ë§·íŒ…
        result = f"""ğŸ“Š ê³ ê¸‰ RAG í‰ê°€ ì‹œìŠ¤í…œ:
{grade} - ì´ì : {total_score}/100ì 
{overall_comment}

ğŸ“ˆ ì„¸ë¶€ ì ìˆ˜:
â€¢ íšŒì‚¬ì •ë³´ í™œìš©: {scores['context_relevance']}/30ì 
â€¢ í•µì‹¬ê°€ì¹˜ ë¶€í•©: {scores['core_values']}/25ì   
â€¢ ì¡°ì§ë¬¸í™” ì í•©: {scores['culture_fit']}/20ì 
â€¢ ì „ë¬¸ì„±: {scores['professionalism']}/10ì 
â€¢ íšŒì‚¬ ì´í•´ë„: {scores['company_understanding']}/10ì 
â€¢ êµ¬ì²´ì„±: {scores['specificity']}/3ì 
â€¢ ì§„ì •ì„±: {scores['authenticity']}/2ì 

âœ… ê°•ì :
""" + "\n".join(feedback_details)
        
        if improvement_suggestions:
            result += f"\n\nğŸ¯ ê°œì„  ì œì•ˆ:\n" + "\n".join(improvement_suggestions)
        
        return result

    def setup_openai(self):
        """OpenAI GPT-4 ì„¤ì •"""
        api_key = os.getenv('OPENAI_API_KEY')
        if api_key:
            try:
                import openai
                return openai.OpenAI(api_key=api_key)
            except ImportError:
                print("âš ï¸ OpenAI íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. pip install openai ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
                return None
            except Exception as e:
                print(f"OpenAI ì„¤ì • ì‹¤íŒ¨: {e}")
                return None
        print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    def setup_claude(self):
        """Claude ì„¤ì •"""
        api_key = os.getenv('CLAUDE_API_KEY')
        if api_key:
            try:
                import anthropic
                return anthropic.Anthropic(api_key=api_key)
            except Exception as e:
                print(f"Claude ì„¤ì • ì‹¤íŒ¨: {e}")
                return None
        print("âš ï¸ CLAUDE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    def setup_ollama(self):
        """Ollama (ë¡œì»¬ ëª¨ë¸) ì„¤ì •"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                if models:
                    return True
        except:
            print("âš ï¸ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return None
    
    def setup_huggingface(self):
        """Hugging Face ë¬´ë£Œ API ì„¤ì •"""
        api_key = os.getenv('HUGGINGFACE_API_KEY', 'hf_dummy')
        return api_key
    
    def select_best_provider(self):
        """ê°€ì¥ ì¢‹ì€ AI ì œê³µì ì„ íƒ"""
        if self.ai_providers['openai']:
            print("âœ… OpenAI GPT-4 ëª¨ë¸ ì‚¬ìš©")
            return 'openai'
        elif self.ai_providers['claude']:
            print("âœ… Claude ëª¨ë¸ ì‚¬ìš©")
            return 'claude'
        elif self.ai_providers['ollama']:
            print("âœ… Ollama ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©")
            return 'ollama'
        elif self.ai_providers['huggingface']:
            print("âœ… Hugging Face ëª¨ë¸ ì‚¬ìš©")
            return 'huggingface'
        else:
            print("âš ï¸ AI ëª¨ë¸ ì—†ìŒ. í‚¤ì›Œë“œ ê¸°ë°˜ ëª¨ë“œë¡œ ì‹¤í–‰")
            return None
    
    def call_ai_model(self, prompt: str, max_tokens: int = 500) -> str:
        """ì„ íƒëœ AI ëª¨ë¸ í˜¸ì¶œ"""
        try:
            if self.active_provider == 'openai':
                response = self.ai_providers['openai'].chat.completions.create(
                    model="gpt-4o",  # Updated to a more current model
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                return response.choices[0].message.content
            
            elif self.active_provider == 'claude':
                message = self.ai_providers['claude'].messages.create(
                    model="claude-3-opus-20240229",  # Updated to a more generic model
                    max_tokens=max_tokens,
                    messages=[{"role": "user", "content": prompt}]
                )
                return message.content[0].text if message.content else ""
            
            elif self.active_provider == 'ollama':
                response = requests.post("http://localhost:11434/api/generate", 
                    json={
                        "model": "llama3",  # Simplified model name
                        "prompt": prompt,
                        "stream": False
                    }, timeout=30)
                if response.status_code == 200:
                    return response.json()['response']
                
            elif self.active_provider == 'huggingface':
                headers = {"Authorization": f"Bearer {self.ai_providers['huggingface']}"}
                api_url = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
                
                response = requests.post(api_url, 
                    headers=headers,
                    json={"inputs": prompt[:500]},  # ê¸¸ì´ ì œí•œ
                    timeout=20
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if isinstance(result, list) and len(result) > 0:
                        return result[0].get('generated_text', '')
        
        except Exception as e:
            print(f"AI ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ ({self.active_provider}): {e}")
            self.switch_to_next_provider()
            
        return ""
    
    def switch_to_next_provider(self):
        """ë‹¤ìŒ AI ì œê³µìë¡œ ì „í™˜"""
        providers = ['openai', 'claude', 'ollama', 'huggingface']
        if self.active_provider in providers:
            current_idx = providers.index(self.active_provider)
            for i in range(current_idx + 1, len(providers)):
                if self.ai_providers[providers[i]]:
                    self.active_provider = providers[i]
                    print(f"ğŸ”„ {providers[i]} ëª¨ë¸ë¡œ ì „í™˜")
                    return
        self.active_provider = None
        
    def extract_text_from_url(self, url: str) -> str:
        """URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        for attempt in range(3):  # Add retry logic
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(url, headers=headers, timeout=10)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                return text
                
            except Exception as e:
                print(f"URL {url} ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3): {e}")
                time.sleep(2)  # Wait before retry
        return ""
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• """
        sentences = re.split(r'[.!?]\s+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 50]
    
    def build_vector_database(self):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•"""
        if os.path.exists(self.vector_db_path) and os.path.exists(self.faiss_index_path):
            print("ğŸ“š ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
            try:
                self.load_vector_database()
                return
            except Exception as e:
                print(f"ë²¡í„° DB ë¡œë”© ì‹¤íŒ¨: {e}. ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤...")
        
        print("ğŸ”¨ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•ì¤‘ì…ë‹ˆë‹¤...")
        all_chunks = []
        all_metadata = []
        
        for url in self.urls:
            print(f"ì²˜ë¦¬ì¤‘: {url}")
            content = self.extract_text_from_url(url)
            if content:
                chunks = self.chunk_text(content)
                for i, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'url': url,
                        'chunk_id': i,
                        'page_type': self.get_page_type(url)
                    })
            time.sleep(1)
        
        self.documents = all_chunks
        self.document_metadata = all_metadata
        
        print("ğŸ”¢ ì„ë² ë”©ì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤...")
        embeddings = self.embedding_model.encode(all_chunks, show_progress_bar=True)
        
        print("ğŸ—‚ï¸ FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±ì¤‘ì…ë‹ˆë‹¤...")
        dimension = embeddings.shape[1]
        self.vector_db = faiss.IndexFlatIP(dimension)
        
        faiss.normalize_L2(embeddings)
        self.vector_db.add(embeddings.astype('float32'))
        
        self.save_vector_database()
        print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ì™„ë£Œ!")
    
    def get_page_type(self, url: str) -> str:
        """URLì—ì„œ í˜ì´ì§€ íƒ€ì… ì¶”ì¶œ"""
        if 'talent' in url:
            return 'ì¸ì¬ìƒ'
        elif 'life' in url:
            return 'ì¡°ì§ë¬¸í™”'
        elif 'welfare' in url:
            return 'ë³µë¦¬í›„ìƒ'
        elif 'education' in url:
            return 'êµìœ¡ì œë„'
        elif 'jobs' in url:
            return 'ì±„ìš©ì •ë³´'
        elif 'jobsemp' in url:
            return 'ì±„ìš©ì „í˜•'
        return 'ê¸°íƒ€'
    
    def save_vector_database(self):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"""
        with open(self.vector_db_path, 'wb') as f:
            pickle.dump({
                'documents': self.documents,
                'metadata': self.document_metadata
            }, f)
        
        faiss.write_index(self.vector_db, self.faiss_index_path)
    
    def load_vector_database(self):
        """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë”©"""
        with open(self.vector_db_path, 'rb') as f:
            data = pickle.load(f)
            self.documents = data['documents']
            self.document_metadata = data['metadata']
        
        self.vector_db = faiss.read_index(self.faiss_index_path)
        print(f"âœ… ë²¡í„° DB ë¡œë”© ì™„ë£Œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
    
    def search_relevant_content(self, query: str, top_k: int = 3) -> List[Dict]:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš© ê²€ìƒ‰"""
        if self.vector_db is None:
            return []
        
        query_embedding = self.embedding_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.vector_db.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.documents):
                results.append({
                    'content': self.documents[idx],
                    'metadata': self.document_metadata[idx],
                    'score': float(score),
                    'rank': i + 1
                })
        
        return results
    
    def generate_rag_questions(self) -> List[str]:
        """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ìƒì„±"""
        categories = ['ì¸ì¬ìƒ', 'ì¡°ì§ë¬¸í™”', 'ë³µë¦¬í›„ìƒ', 'êµìœ¡ì œë„', 'ì±„ìš©']
        questions = []
        
        for category in categories:
            relevant_docs = self.search_relevant_content(category, top_k=2)
            context = "\n".join([doc['content'] for doc in relevant_docs])
            
            if self.active_provider and context:
                try:
                    prompt = f"""
                    ë‹¤ìŒ LIGë„¥ìŠ¤ì› {category} ê´€ë ¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ì…ì‚¬ì› ë©´ì ‘ ì§ˆë¬¸ 1ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
                    
                    {context[:800]}
                    
                    ìš”êµ¬ì‚¬í•­:
                    - êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸
                    - ì§€ì›ìì˜ ì´í•´ë„ì™€ ì í•©ì„±ì„ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸
                    - í•œêµ­ì–´ë¡œ ì‘ì„±
                    - ì§ˆë¬¸ë§Œ ë°˜í™˜ (ë²ˆí˜¸ë‚˜ ì¶”ê°€ ì„¤ëª… ì—†ì´)
                    
                    ì˜ˆì‹œ í˜•ì‹: "LIGë„¥ìŠ¤ì›ì˜ â—‹â—‹ì— ëŒ€í•´ ì„¤ëª…í•˜ê³ , ë³¸ì¸ì˜ ê²½í—˜ê³¼ ì—°ê²°í•˜ì—¬ ë§ì”€í•´ì£¼ì„¸ìš”."
                    """
                    
                    ai_question = self.call_ai_model(prompt, max_tokens=200)
                    
                    if ai_question and len(ai_question.strip()) > 10:
                        clean_question = ai_question.strip().split('\n')[0]
                        clean_question = re.sub(r'^\d+\.\s*', '', clean_question)
                        clean_question = clean_question.strip('"\'')
                        
                        if clean_question:
                            questions.append(clean_question)
                            print(f"âœ… {category} ì§ˆë¬¸ ìƒì„± ì™„ë£Œ")
                        
                except Exception as e:
                    print(f"AI ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨ ({category}): {e}")
        
        premium_base_questions = [
            "LIGë„¥ìŠ¤ì›ì˜ ì¸ì¬ìƒì¸ 'OPEN'ê³¼ 'POSITIVE'ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , ë³¸ì¸ì´ ì´ ê°€ì¹˜ë¥¼ ì‹¤ì²œí•œ ì‚¬ë¡€ë¥¼ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "ë°©ìœ„ì‚°ì—…ì´ êµ­ê°€ ì•ˆë³´ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ ë³¸ì¸ì˜ ê²¬í•´ì™€ ì´ ë¶„ì•¼ì—ì„œ ì¼í•˜ê³ ì í•˜ëŠ” ë™ê¸°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "LIGë„¥ìŠ¤ì›ì˜ ì¡°ì§ë¬¸í™”ì¸ Pride, Trust, Passion, Enjoy ì¤‘ ë³¸ì¸ê³¼ ê°€ì¥ ë¶€í•©í•˜ëŠ” ê°€ì¹˜ 2ê°œë¥¼ ì„ íƒí•˜ê³  ê·¸ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "í˜ì‹ ê³¼ ì°½ì¡°ì  ì‚¬ê³ ê°€ í•„ìš”í–ˆë˜ í”„ë¡œì íŠ¸ë‚˜ ìƒí™©ì—ì„œ ë³¸ì¸ì´ ì–´ë–¤ ì—­í• ì„ í–ˆëŠ”ì§€ êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "LIGë„¥ìŠ¤ì›ì—ì„œ ì–´ë–¤ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ê°€ ë˜ê³  ì‹¶ìœ¼ë©°, ê·¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë³¸ì¸ì˜ êµ¬ì²´ì ì¸ ê³„íšì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "íŒ€ ë‚´ì—ì„œ ê°ˆë“±ì´ë‚˜ ì˜ê²¬ ì¶©ëŒì´ ë°œìƒí–ˆì„ ë•Œ ì´ë¥¼ í•´ê²°í•œ ê²½í—˜ê³¼ ê·¸ ê³¼ì •ì—ì„œ ë°°ìš´ ì ì„ ë§ì”€í•´ì£¼ì„¸ìš”.",
            "ëê¹Œì§€ í¬ê¸°í•˜ì§€ ì•Šê³  ëª©í‘œë¥¼ ë‹¬ì„±í•œ ê²½í—˜ ì¤‘ ê°€ì¥ ì˜ë¯¸ ìˆì—ˆë˜ ì‚¬ë¡€ì™€ ê·¸ ê³¼ì •ì—ì„œì˜ ì–´ë ¤ì›€ì„ ì–´ë–»ê²Œ ê·¹ë³µí–ˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        ]
        
        while len(questions) < 5:
            remaining_count = 5 - len(questions)
            questions.extend(premium_base_questions[:remaining_count])
            break
        
        return questions[:5]
    
    def evaluate_answer_with_rag(self, question: str, answer: str) -> str:
        """RAGë¥¼ ì‚¬ìš©í•œ ë‹µë³€ í‰ê°€"""
        relevant_docs = self.search_relevant_content(question + " " + answer, top_k=3)
        context = "\n".join([f"[{doc['metadata']['page_type']}] {doc['content'][:200]}..." 
                           for doc in relevant_docs])
        
        if self.active_provider and context:
            try:
                prompt = f"""
                LIGë„¥ìŠ¤ì› ì„ì›ìœ¼ë¡œì„œ ë‹¤ìŒ ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì „ë¬¸ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
                
                ê´€ë ¨ íšŒì‚¬ ì •ë³´:
                {context}
                
                ë©´ì ‘ ì§ˆë¬¸: {question}
                ì§€ì›ì ë‹µë³€: {answer}
                
                í‰ê°€ ê¸°ì¤€:
                1. íšŒì‚¬ ê°€ì¹˜ê´€(OPEN, POSITIVE) ë° ì¡°ì§ë¬¸í™”ì™€ì˜ ë¶€í•©ë„
                2. ì œê³µëœ íšŒì‚¬ ì •ë³´ì™€ì˜ ì—°ê´€ì„± ë° ì´í•´ë„
                3. ë‹µë³€ì˜ êµ¬ì²´ì„±, ì§„ì •ì„±, ë…¼ë¦¬ì„±
                4. ì „ë¬¸ì„±ê³¼ ì„±ì¥ ê°€ëŠ¥ì„±
                
                í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
                - ì¢…í•© í‰ê°€: [ìš°ìˆ˜/ì–‘í˜¸/ë³´í†µ/ë¯¸í¡] 
                - ê°•ì : [êµ¬ì²´ì ì¸ ê°•ì  2-3ê°œ]
                - ê°œì„ ì : [êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆ 1-2ê°œ]
                - ì¶”ì²œ ì ìˆ˜: [100ì  ë§Œì  ê¸°ì¤€]
                
                ì „ë¬¸ì ì´ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”.
                """
                
                ai_feedback = self.call_ai_model(prompt, max_tokens=400)
                
                if ai_feedback:
                    return f"ğŸ¤– AI ì „ë¬¸ í‰ê°€ ({self.active_provider.upper()}):\n{ai_feedback}"
                
            except Exception as e:
                print(f"AI í‰ê°€ ì‹¤íŒ¨: {e}")
        
        return self.advanced_keyword_evaluation(question, answer, relevant_docs)
    
    def keyword_evaluation(self, question: str, answer: str, context_docs: List[Dict]) -> str:
        """í–¥ìƒëœ í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€"""
        answer_lower = answer.lower()
        
        context_keywords = set()
        for doc in context_docs:
            content_words = re.findall(r'\b\w+\b', doc['content'].lower())
            context_keywords.update([word for word in content_words if len(word) > 2])
        
        company_keywords = ['lignex1', 'ligë„¥ìŠ¤ì›', 'ë°©ìœ„ì‚°ì—…', 'êµ­ë°©', 'ì•ˆë³´']
        value_keywords = ['open', 'positive', 'ê°œë°©', 'ê¸ì •', 'pride', 'trust', 'passion', 'enjoy']
        professional_keywords = ['ì „ë¬¸ê°€', 'ì—­ëŸ‰', 'ì„±ì¥', 'í•™ìŠµ', 'í˜ì‹ ', 'ì°½ì˜', 'ë„ì „']
        
        score = 0
        feedback_points = []
        
        if len(answer.strip()) < 10:
            return "ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
        
        inappropriate = ['ë˜¥', 'ë°”ë³´', 'ì‹«ë‹¤', 'ëª¨ë¥´ê² ë‹¤', 'ì•„ë¬´ê±°ë‚˜']
        if any(word in answer_lower for word in inappropriate):
            return "âŒ ë©´ì ‘ì— ì ì ˆí•˜ì§€ ì•Šì€ ë‹µë³€ì…ë‹ˆë‹¤. ì§„ì†”í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
        
        context_match = len([word for word in answer_lower.split() if word in context_keywords])
        if context_match > 3:
            score += 3
            feedback_points.append("âœ… íšŒì‚¬ ì •ë³´ì™€ ì˜ ì—°ê²°ëœ ë‹µë³€ì…ë‹ˆë‹¤")
        elif context_match > 1:
            score += 1
            feedback_points.append("âœ… íšŒì‚¬ ì •ë³´ë¥¼ ì¼ë¶€ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤")
        
        if any(keyword in answer_lower for keyword in company_keywords):
            score += 2
            feedback_points.append("âœ… íšŒì‚¬ì— ëŒ€í•œ ì´í•´ë„ê°€ ì¢‹ìŠµë‹ˆë‹¤")
        
        if any(keyword in answer_lower for keyword in value_keywords):
            score += 2
            feedback_points.append("âœ… íšŒì‚¬ ê°€ì¹˜ê´€ê³¼ ì˜ ë¶€í•©í•©ë‹ˆë‹¤")
        
        if any(keyword in answer_lower for keyword in professional_keywords):
            score += 1
            feedback_points.append("âœ… ì „ë¬¸ì ì¸ ì‚¬ê³ ê°€ ë‹ë³´ì…ë‹ˆë‹¤")
        
        if score >= 6:
            overall = "ğŸŒŸ ë§¤ìš° ìš°ìˆ˜í•œ ë‹µë³€ì…ë‹ˆë‹¤!"
        elif score >= 4:
            overall = "ğŸ‘ ì¢‹ì€ ë‹µë³€ì…ë‹ˆë‹¤."
        elif score >= 2:
            overall = "ğŸ’¡ ê´œì°®ì€ ë‹µë³€ì´ì§€ë§Œ ë” ë³´ì™„í•´ë³´ì„¸ìš”."
        else:
            overall = "ğŸ“š íšŒì‚¬ ì •ë³´ë¥¼ ë” ë°˜ì˜í•œ ë‹µë³€ì„ í•´ë³´ì„¸ìš”."
        
        if not feedback_points:
            feedback_points.append("ë” êµ¬ì²´ì ì¸ ì‚¬ë¡€ë‚˜ íšŒì‚¬ ê´€ë ¨ ë‚´ìš©ì„ í¬í•¨í•´ë³´ì„¸ìš”")
        
        if context_docs:
            hint_info = context_docs[0]['metadata']['page_type']
            feedback_points.append(f"ğŸ’¡ íŒíŠ¸: {hint_info} ê´€ë ¨ ë‚´ìš©ì„ ë” í™œìš©í•´ë³´ì„¸ìš”")
        
        return f"ğŸ“Š RAG ê¸°ë°˜ í‚¤ì›Œë“œ í‰ê°€:\n{overall}\n" + "\n".join(feedback_points)
    
    def run_interview(self):
        """ë©´ì ‘ ì§„í–‰"""
        print("\n" + "="*60)
        print("ğŸ¯ LIGë„¥ìŠ¤ì› RAG ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
        print("="*60)
        
        self.build_vector_database()
        
        print("ğŸ§  RAG ê¸°ë°˜ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤...")
        self.questions = self.generate_rag_questions()
        
        print(f"\nğŸ“‹ ì´ {len(self.questions)}ê°œì˜ ì§ˆë¬¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ê° ì§ˆë¬¸ì— ì„±ì‹¤íˆ ë‹µë³€í•´ ì£¼ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
        print("ğŸ’¡ ë‹µë³€ì€ LIGë„¥ìŠ¤ì› ì›¹ì‚¬ì´íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.\n")
        
        for i, question in enumerate(self.questions, 1):
            print(f"\nğŸ“Œ ì§ˆë¬¸ {i}/{len(self.questions)}")
            print("-" * 50)
            print(question)
            print("-" * 50)
            
            answer = input("\nğŸ’¬ ë‹µë³€: ").strip()
            
            if answer.lower() == 'quit':
                print("\në©´ì ‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!")
                break
                
            if answer:
                print("\nâ³ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ì„ í‰ê°€ì¤‘ì…ë‹ˆë‹¤...")
                feedback = self.evaluate_answer_with_rag(question, answer)
                print(f"\nğŸ“ í‰ê°€ ë° í”¼ë“œë°±:")
                print(feedback)
                print("\n" + "="*50)
            else:
                print("ë‹µë³€ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                continue
        
        print("\nğŸ‰ RAG ê¸°ë°˜ ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("LIGë„¥ìŠ¤ì› ì§€ì›ì— í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ’ª")

def main():
    print("ğŸš€ LIGë„¥ìŠ¤ì› ê³ ê¸‰ RAG ë©´ì ‘ ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("ğŸ”¥ ì§€ì›ë˜ëŠ” AI ëª¨ë¸: OpenAI GPT-4, Claude, Ollama, Hugging Face")
    
    missing_packages = []
    try:
        import sentence_transformers
        import faiss
        import requests
        import bs4
    except ImportError as e:
        missing_packages.append(str(e).split()[-1])
    
    if missing_packages:
        print(f"âŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_packages)}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print(f"pip install {' '.join(missing_packages)} sentence-transformers faiss-cpu requests beautifulsoup4")
        return
    
    available_models = []
    if os.getenv('OPENAI_API_KEY'):
        available_models.append("OpenAI GPT-4")
    if os.getenv('CLAUDE_API_KEY'):
        available_models.append("Claude")
    if os.getenv('HUGGINGFACE_API_KEY'):
        available_models.append("Hugging Face")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        if response.status_code == 200:
            available_models.append("Ollama (ë¡œì»¬)")
    except:
        pass
    
    if available_models:
        print(f"\nâœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {', '.join(available_models)}")
    else:
        print("\nâš ï¸ AI ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê³ ê¸‰ í‚¤ì›Œë“œ í‰ê°€ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
    api_choice = input("API í‚¤ë¥¼ ì„¤ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (o)penai / (c)laude / (h)uggingface / (n)o: ").lower()
    
    if api_choice == 'o':
        key = input("OpenAI API í‚¤ ì…ë ¥: ").strip()
        if key:
            os.environ['OPENAI_API_KEY'] = key
            print("âœ… OpenAI ì„¤ì • ì™„ë£Œ")
    elif api_choice == 'c':
        key = input("Claude API í‚¤ ì…ë ¥: ").strip()
        if key:
            os.environ['CLAUDE_API_KEY'] = key
            print("âœ… Claude ì„¤ì • ì™„ë£Œ")
    elif api_choice == 'h':
        key = input("Hugging Face API í‚¤ ì…ë ¥ (Enterë¡œ ë¬´ë£Œ ì‚¬ìš©): ").strip()
        os.environ['HUGGINGFACE_API_KEY'] = key or 'hf_dummy'
        print("âœ… Hugging Face ì„¤ì • ì™„ë£Œ")
    
    try:
        chatbot = LIGNex1RAGInterviewChatbot()
        chatbot.run_interview()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ë©´ì ‘ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!")
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
        print("1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install openai sentence-transformers faiss-cpu requests beautifulsoup4")
        print("2. API í‚¤ í™•ì¸")
        print("3. ì¸í„°ë„· ì—°ê²° í™•ì¸")

if __name__ == "__main__":
    main()