import json
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import chromadb
from chromadb.config import Settings
import anthropic
from dataclasses import dataclass
import logging
from abc import ABC, abstractmethod
import requests
import urllib.parse
import time
from bs4 import BeautifulSoup
import re
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ êµ¬ì¡°"""
    title: str
    link: str
    description: str
    pub_date: str
    content: str = ""
    
@dataclass
class NewsMetadata:
    """ë‰´ìŠ¤ ë©”íƒ€ë°ì´í„° êµ¬ì¡°"""
    relevance_score: int
    topics: List[str]
    keywords: List[str]
    summary: str
    sentiment: str
    importance: int
    company_mentions: List[str]
    date: str
    source: str

@dataclass
class NewsChunk:
    """ë‰´ìŠ¤ ì²­í¬ êµ¬ì¡°"""
    chunk_id: int
    content: str
    topics: List[str]
    keywords: List[str]
    chunk_type: str

class EnhancedNaverNewsAPI:
    """í–¥ìƒëœ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, client_id: str, client_secret: str):
        self.client_id = client_id
        self.client_secret = client_secret
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        
        if not client_id or client_id == "YOUR_NAVER_CLIENT_ID":
            logger.warning("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            self.test_mode = True
        else:
            self.test_mode = False
    
    def search_news_with_keywords(self, company_name: str, additional_keywords: List[str] = None, 
                                display: int = 10, start: int = 1, sort: str = "date") -> List[NewsArticle]:
        """íšŒì‚¬ëª…ê³¼ ì¶”ê°€ í‚¤ì›Œë“œë¥¼ ì¡°í•©í•œ í–¥ìƒëœ ë‰´ìŠ¤ ê²€ìƒ‰"""
        all_articles = []
        
        # ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ë“¤ ìƒì„±
        search_queries = [company_name]
        
        # ì¶”ê°€ í‚¤ì›Œë“œì™€ ì¡°í•©
        if additional_keywords:
            for keyword in additional_keywords:
                search_queries.append(f"{company_name} {keyword}")
        
        # ê¸°ë³¸ ì¡°í•© ì¿¼ë¦¬ ì¶”ê°€
        search_queries.extend([
            f"{company_name} ì‹ ì œí’ˆ",
            f"{company_name} ë°œí‘œ",
            f"{company_name} ê¸°ìˆ ",
            f"{company_name} ì‹¤ì "
        ])
        
        # ì¤‘ë³µ ì œê±°
        search_queries = list(set(search_queries))
        logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±: {search_queries}")
        
        for query in search_queries[:5]:  # ìµœëŒ€ 5ê°œ ì¿¼ë¦¬ë§Œ ì‹¤í–‰
            try:
                articles = self.search_news(query, display=min(display, 10), start=start, sort=sort)
                all_articles.extend(articles)
                
                # API í˜¸ì¶œ ì œí•œ
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"ê²€ìƒ‰ ì¿¼ë¦¬ '{query}' ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        # ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        
        for article in all_articles:
            if article.title not in seen_titles:
                unique_articles.append(article)
                seen_titles.add(article.title)
        
        logger.info(f"ì¤‘ë³µ ì œê±° í›„ unique ê¸°ì‚¬: {len(unique_articles)}ê°œ")
        return unique_articles[:display]  # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
    
    def search_news(self, query: str, display: int = 10, start: int = 1, 
                   sort: str = "date") -> List[NewsArticle]:
        """ê¸°ë³¸ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
        if self.test_mode:
            return self._get_dummy_news(query, display)
        
        try:
            encoded_query = urllib.parse.quote(query)
            url = f"{self.base_url}?query={encoded_query}&display={display}&start={start}&sort={sort}"
            
            headers = {
                "X-Naver-Client-Id": self.client_id,
                "X-Naver-Client-Secret": self.client_secret
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            articles = []
            
            for item in data.get('items', []):
                article = NewsArticle(
                    title=self._clean_html(item['title']),
                    link=item['link'],
                    description=self._clean_html(item['description']),
                    pub_date=item['pubDate']
                )
                
                # ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
                article.content = self._fetch_article_content(article.link)
                articles.append(article)
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ë”œë ˆì´
                time.sleep(0.1)
            
            logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
            return articles
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self._get_dummy_news(query, display)
    
    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        return re.sub(r'<[^>]+>', '', text)
    
    def _fetch_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ (ê°œì„ ë¨)"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì„ íƒìë“¤ (í™•ì¥ë¨)
            selectors = [
                '#dic_area',  # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸
                '#articleBodyContents',
                '.news_text',
                '.article_body',
                '.view_text',
                '.news_view',
                '#newsContentDiv',
                '.article-view-content'
            ]
            
            content = ""
            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    # ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸ ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for unwanted in element.find_all(['script', 'style', 'ins', 'iframe']):
                        unwanted.decompose()
                    
                    content = element.get_text(separator='\n').strip()
                    break
            
            # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            if not content or len(content) < 100:
                content = soup.get_text(separator='\n')
                # ë¶ˆí•„ìš”í•œ ì¤„ ì œê±°
                lines = [line.strip() for line in content.split('\n') if line.strip()]
                content = '\n'.join(lines)
            
            return content[:3000]  # ê¸¸ì´ ì œí•œ ì¦ê°€
            
        except Exception as e:
            logger.warning(f"ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
            return ""
    
    def _get_dummy_news(self, query: str, display: int = 10) -> List[NewsArticle]:
        """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë‰´ìŠ¤ (í™•ì¥ë¨)"""
        dummy_articles = []
        
        base_articles = [
            {
                "title_template": "{query} ê´€ë ¨ ìµœì‹  ê¸°ìˆ  ë™í–¥ ë°œí‘œ",
                "description_template": "{query}ì˜ ìƒˆë¡œìš´ í˜ì‹ ì´ ì—…ê³„ì— ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.",
                "content_template": "{query}ê°€ ìƒˆë¡œìš´ ê¸°ìˆ  í˜ì‹ ì„ í†µí•´ ì‹œì¥ì—ì„œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤. ì „ë¬¸ê°€ë“¤ì€ ì´ë²ˆ ë°œí‘œê°€ ì—…ê³„ ì „ë°˜ì— í° ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•œë‹¤ê³  ë°í˜”ìŠµë‹ˆë‹¤. íŠ¹íˆ ì„±ëŠ¥ í–¥ìƒê³¼ ì•ˆì •ì„± ê°œì„ ì— ì¤‘ì ì„ ë‘” ì´ë²ˆ ì—…ë°ì´íŠ¸ëŠ” ê³ ê°ë“¤ë¡œë¶€í„° ê¸ì •ì ì¸ ë°˜ì‘ì„ ì–»ê³  ìˆìŠµë‹ˆë‹¤."
            },
            {
                "title_template": "{query} ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€ ì†Œì‹",
                "description_template": "{query}ì˜ ì‹œì¥ ì˜í–¥ë ¥ì´ ê³„ì† í™•ëŒ€ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "content_template": "{query}ì˜ ì‹œì¥ ì ìœ ìœ¨ì´ ì§€ì†ì ìœ¼ë¡œ í™•ëŒ€ë˜ê³  ìˆì–´ ì—…ê³„ì˜ ê´€ì‹¬ì´ ì§‘ì¤‘ë˜ê³  ìˆìŠµë‹ˆë‹¤. ë¶„ì„ê°€ë“¤ì€ í–¥í›„ ì„±ì¥ ì „ë§ì„ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ìˆìœ¼ë©°, ì‹ ì œí’ˆ ì¶œì‹œì™€ í•¨ê»˜ ë”ìš± ê°•ë ¥í•œ ì‹œì¥ ì§€ë°°ë ¥ì„ ë³´ì¼ ê²ƒìœ¼ë¡œ ì „ë§ë©ë‹ˆë‹¤."
            },
            {
                "title_template": "{query} ì‹ ê·œ íŒŒíŠ¸ë„ˆì‹­ ì²´ê²°",
                "description_template": "{query}ê°€ ê¸€ë¡œë²Œ ê¸°ì—…ê³¼ì˜ ì „ëµì  íŒŒíŠ¸ë„ˆì‹­ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.",
                "content_template": "{query}ê°€ ê¸€ë¡œë²Œ ì„ ë„ ê¸°ì—…ê³¼ì˜ ì „ëµì  íŒŒíŠ¸ë„ˆì‹­ì„ ì²´ê²°í–ˆë‹¤ê³  ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ì´ë²ˆ í˜‘ì—…ì„ í†µí•´ ê¸°ìˆ ë ¥ í–¥ìƒê³¼ ì‹œì¥ í™•ëŒ€ì— íƒ„ë ¥ì„ ë°›ì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë˜ë©°, ì–‘ì‚¬ëŠ” ìƒí˜¸ ë³´ì™„ì ì¸ ê¸°ìˆ ê³¼ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ì°½ì¶œí•  ê³„íšì…ë‹ˆë‹¤."
            }
        ]
        
        for i in range(min(display, len(base_articles) * 2)):
            template = base_articles[i % len(base_articles)]
            
            dummy_articles.append(NewsArticle(
                title=template["title_template"].format(query=query),
                link=f"http://test.com/news{i+1}",
                description=template["description_template"].format(query=query),
                pub_date=f"Mon, {7 + i} Jul 2025 {10 + i}:00:00 +0900",
                content=template["content_template"].format(query=query)
            ))
        
        return dummy_articles

class EnhancedPromptManager:
    """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def get_news_analysis_prompt(news_content: str, company_name: str) -> str:
        return f"""ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ë¶„ì„í•  ë‰´ìŠ¤ ê¸°ì‚¬:**
{news_content}

**ëŒ€ìƒ íšŒì‚¬:** {company_name}

**ìš”êµ¬ì‚¬í•­:**
1. ì´ ë‰´ìŠ¤ê°€ ëŒ€ìƒ íšŒì‚¬ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ê´€ë ¨ë„ë¥¼ 1-10ì ìœ¼ë¡œ í‰ê°€
2. ë‰´ìŠ¤ì˜ ì£¼ìš” í† í”½ ë¶„ë¥˜ (ìµœëŒ€ 3ê°œ)
3. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœëŒ€ 10ê°œ)
4. ë‰´ìŠ¤ ìš”ì•½ (2-3ë¬¸ì¥)
5. ê°ì • ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
6. ì¤‘ìš”ë„ í‰ê°€ (1-10ì )

**ì¶œë ¥ í˜•ì‹:**
```json
{{
  "relevance_score": 0,
  "topics": ["topic1", "topic2", "topic3"],
  "keywords": ["keyword1", "keyword2", ...],
  "summary": "ë‰´ìŠ¤ ìš”ì•½ ë‚´ìš©",
  "sentiment": "ê¸ì •/ë¶€ì •/ì¤‘ë¦½",
  "importance": 0,
  "company_mentions": ["íšŒì‚¬ëª…1", "íšŒì‚¬ëª…2"],
  "date": "YYYY-MM-DD",
  "source": "ë‰´ìŠ¤ ì¶œì²˜"
}}
```

ê´€ë ¨ë„ê°€ 5ì  ì´ìƒì¸ ê²½ìš°ë§Œ ë²¡í„° DBì— ì €ì¥í•˜ì„¸ìš”."""

    @staticmethod
    def get_news_chunking_prompt(news_content: str) -> str:
        return f"""ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì˜ë¯¸ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• í•´ì£¼ì„¸ìš”.

**ì›ë³¸ ë‰´ìŠ¤:**
{news_content}

**ì²­í‚¹ ê·œì¹™:**
1. ê° ì²­í¬ëŠ” 200-400ì ì‚¬ì´
2. ë¬¸ì¥ì˜ ì™„ì „ì„± ìœ ì§€
3. ì£¼ì œë³„ë¡œ ë…¼ë¦¬ì  ë¶„í• 
4. ê° ì²­í¬ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•´ì•¼ í•¨

**ì¶œë ¥ í˜•ì‹:**
```json
{{
  "chunks": [
    {{
      "chunk_id": 1,
      "content": "ì²­í¬ ë‚´ìš©",
      "topics": ["ê´€ë ¨ í† í”½"],
      "keywords": ["ê´€ë ¨ í‚¤ì›Œë“œ"],
      "chunk_type": "ì œëª©/ë³¸ë¬¸/ì¸ìš©/í†µê³„"
    }}
  ]
}}
```"""

    @staticmethod
    def get_enhanced_news_generation_prompt(topic: str, keywords: List[str], 
                                          user_facts: str, reference_materials: str,
                                          length_specification: str = "") -> str:
        keywords_str = ", ".join(keywords)
        return f"""ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë‰´ìŠ¤ ê¸°ìì…ë‹ˆë‹¤. ë‹¤ìŒ ì¡°ê±´ì— ë§ì¶° ê³ í’ˆì§ˆ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ì…ë ¥ ì •ë³´:**
- í† í”½: {topic}
- í‚¤ì›Œë“œ: {keywords_str}
- ì‚¬ìš©ì ì œê³µ ì‚¬ì‹¤: {user_facts}
- ê¸¸ì´ ìš”êµ¬ì‚¬í•­: {length_specification}

**ì°¸ê³  ìë£Œ (RAG):**
{reference_materials}

**ì‘ì„± ê°€ì´ë“œë¼ì¸:**
1. ê°ê´€ì ì´ê³  ì‚¬ì‹¤ì ì¸ ë³´ë„ ìŠ¤íƒ€ì¼
2. 5W1H (Who, What, When, Where, Why, How) ì›ì¹™ ì¤€ìˆ˜
3. ì œëª©, ë¦¬ë“œ, ë³¸ë¬¸, ê²°ë¡ ì˜ ëª…í™•í•œ êµ¬ì¡°
4. ì°¸ê³  ìë£Œì˜ ì •ë³´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©
5. ì „ë¬¸ì ì´ê³  ì‹ ë¢°ì„± ìˆëŠ” í‘œí˜„ ì‚¬ìš©

**ë‰´ìŠ¤ êµ¬ì¡°:**
1. **ì œëª©** (25-35ì, ì„íŒ©íŠ¸ ìˆê³  ëª…í™•í•˜ê²Œ)
2. **ë¦¬ë“œ** (í•µì‹¬ ë‚´ìš© ìš”ì•½, 2-3ë¬¸ì¥, ìœ¡í•˜ì›ì¹™ í¬í•¨)
3. **ë³¸ë¬¸** (ìƒì„¸ ë‚´ìš©, ë…¼ë¦¬ì  ë‹¨ë½ êµ¬ì„±)
   - ë°°ê²½ ì •ë³´
   - ì£¼ìš” ë‚´ìš© ìƒì„¸ ì„¤ëª…
   - ê´€ë ¨ì ë°œì–¸ ë˜ëŠ” ì „ë¬¸ê°€ ì˜ê²¬
   - ì‹œì¥ ë°˜ì‘ ë˜ëŠ” ì˜í–¥ ë¶„ì„
4. **ê²°ë¡ ** (ì „ë§, ì˜ë¯¸, í›„ì† ê³„íš)

**í’ˆì§ˆ ê¸°ì¤€:**
- ì •í™•í•œ ì •ë³´ë§Œ ì‚¬ìš©
- ê· í˜•ì¡íŒ ì‹œê° ì œì‹œ
- ì°¸ê³  ìë£Œ ë‚´ìš©ì„ ì ì ˆíˆ ì¸ìš©
- ì „ë¬¸ì ì¸ ë‰´ìŠ¤ ë¬¸ì²´ ìœ ì§€
- {length_specification}

**ì¶œë ¥ í˜•ì‹:**
```
ì œëª©: [ë‰´ìŠ¤ ì œëª©]

ë¦¬ë“œ: [í•µì‹¬ ë‚´ìš© ìš”ì•½]

ë³¸ë¬¸:
[ë°°ê²½ ì •ë³´ ë‹¨ë½]

[ì£¼ìš” ë‚´ìš© ë‹¨ë½]

[ì „ë¬¸ê°€ ì˜ê²¬/ê´€ë ¨ì ë°œì–¸ ë‹¨ë½]

[ì‹œì¥ ë°˜ì‘/ì˜í–¥ ë¶„ì„ ë‹¨ë½]

ê²°ë¡ : [ì „ë§ ë˜ëŠ” ì˜ë¯¸, í›„ì† ê³„íš]

í‚¤ì›Œë“œ: [ê´€ë ¨ í‚¤ì›Œë“œ ë‚˜ì—´]
```

ì°¸ê³  ìë£Œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ì‹¤ì ì´ê³  ì‹ ë¢°ì„± ìˆëŠ” ì „ë¬¸ ë‰´ìŠ¤ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    @staticmethod
    def get_quality_check_prompt(news_content: str) -> str:
        return f"""ì‘ì„±ëœ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í’ˆì§ˆì„ ì „ë¬¸ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

**ë‰´ìŠ¤ ê¸°ì‚¬:**
{news_content}

**í‰ê°€ ê¸°ì¤€:**
1. ì‚¬ì‹¤ì„± (Facts): ì •í™•í•œ ì •ë³´ í¬í•¨ ì—¬ë¶€ (1-10ì )
2. ì™„ì„±ë„ (Completeness): 5W1H ì¶©ì¡± ë° êµ¬ì¡° ì™„ì„±ë„ (1-10ì )
3. ê°ê´€ì„± (Objectivity): í¸í–¥ì„± ì—†ëŠ” ê· í˜•ì¡íŒ ë³´ë„ (1-10ì )
4. ê°€ë…ì„± (Readability): ì´í•´í•˜ê¸° ì‰¬ìš´ êµ¬ì¡°ì™€ ë¬¸ì²´ (1-10ì )
5. ì‹ ë¢°ì„± (Credibility): ì¶œì²˜ ëª…í™•ì„±ê³¼ ì „ë¬¸ì„± (1-10ì )

**í‰ê°€ ê²°ê³¼:**
```json
{{
  "overall_score": 0,
  "detailed_scores": {{
    "facts": 0,
    "completeness": 0,
    "objectivity": 0,
    "readability": 0,
    "credibility": 0
  }},
  "improvements": ["ê°œì„ ì‚¬í•­ ëª©ë¡"],
  "strengths": ["ê°•ì  ëª©ë¡"],
  "approval": true
}}
```

ì „ì²´ ì ìˆ˜ 7ì  ì´ìƒì¼ ë•Œ ìµœì¢… ìŠ¹ì¸í•˜ì„¸ìš”."""

class EnhancedChromaDBManager:
    """í–¥ìƒëœ ChromaDB ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, db_path: str = "./chroma_db"):
        try:
            self.client = chromadb.PersistentClient(path=db_path)
            self.collection = self.client.get_or_create_collection(
                name="enhanced_news_collection",
                metadata={"description": "Enhanced AI News Writer ë‰´ìŠ¤ ì»¬ë ‰ì…˜"}
            )
            logger.info(f"ChromaDB ì´ˆê¸°í™” ì™„ë£Œ: {db_path}")
        except Exception as e:
            logger.error(f"ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def store_news_chunk(self, chunk: NewsChunk, metadata: NewsMetadata, 
                        embedding: List[float]) -> None:
        """ë‰´ìŠ¤ ì²­í¬ë¥¼ ë²¡í„° DBì— ì €ì¥ (ê°œì„ ë¨)"""
        try:
            # ì„ë² ë”© ì°¨ì›ì„ 768ë¡œ í†µì¼
            if len(embedding) != 768:
                if len(embedding) < 768:
                    embedding = embedding + [0.0] * (768 - len(embedding))
                else:
                    embedding = embedding[:768]
            
            # ê³ ìœ  ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)
            import hashlib
            content_hash = hashlib.md5(chunk.content.encode()).hexdigest()
            unique_id = f"chunk_{content_hash}_{chunk.chunk_id}_{int(time.time())}"
            
            # ë©”íƒ€ë°ì´í„° í™•ì¥
            chunk_metadata = {
                "topics": json.dumps(chunk.topics, ensure_ascii=False),
                "keywords": json.dumps(chunk.keywords, ensure_ascii=False),
                "chunk_type": chunk.chunk_type,
                "sentiment": metadata.sentiment,
                "importance": metadata.importance,
                "relevance_score": metadata.relevance_score,
                "company_mentions": json.dumps(metadata.company_mentions, ensure_ascii=False),
                "date": metadata.date,
                "source": metadata.source,
                "summary": metadata.summary,
                "created_at": datetime.now().isoformat()
            }
            
            self.collection.add(
                documents=[chunk.content],
                metadatas=[chunk_metadata],
                embeddings=[embedding],
                ids=[unique_id]
            )
            logger.info(f"ì²­í¬ ì €ì¥ ì™„ë£Œ: {chunk.chunk_id} (ID: {unique_id})")
        except Exception as e:
            logger.error(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def search_relevant_news(self, query: str, n_results: int = 10, 
                           min_relevance: int = 5) -> Dict:
        """ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ (ê°œì„ ë¨)"""
        try:
            # ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            collection_count = self.collection.count()
            if collection_count == 0:
                logger.warning("ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
            
            # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ê²°ê³¼ ìˆ˜ ì œí•œ
            actual_n_results = min(n_results, collection_count)
            
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤í–‰
            results = self.collection.query(
                query_texts=[query],
                n_results=actual_n_results,
                include=['documents', 'metadatas', 'distances']
            )
            
            # ê´€ë ¨ë„ í•„í„°ë§ (ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
            if results.get('metadatas') and results['metadatas'][0]:
                filtered_results = {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
                
                for i, metadata in enumerate(results['metadatas'][0]):
                    relevance = metadata.get('relevance_score', 10)  # ê¸°ë³¸ê°’ 10
                    if relevance >= min_relevance:
                        filtered_results['documents'][0].append(results['documents'][0][i])
                        filtered_results['metadatas'][0].append(metadata)
                        filtered_results['distances'][0].append(results['distances'][0][i])
                
                logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(filtered_results['documents'][0])}ê°œ ê²°ê³¼ (ê´€ë ¨ë„ {min_relevance}+ í•„í„°ë§)")
                return filtered_results
            else:
                logger.info(f"ê²€ìƒ‰ ì™„ë£Œ: {len(results['documents'][0])}ê°œ ê²°ê³¼")
                return results
                
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return {'documents': [[]], 'metadatas': [[]], 'distances': [[]]}
    
    def get_collection_stats(self) -> Dict:
        """ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ"""
        try:
            count = self.collection.count()
            return {
                "total_chunks": count,
                "collection_name": self.collection.name
            }
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"total_chunks": 0, "collection_name": "unknown"}

class EnhancedClaudeClient:
    """í–¥ìƒëœ Claude API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key
        if api_key and api_key != "YOUR_CLAUDE_API_KEY":
            self.client = anthropic.Anthropic(api_key=api_key)
        else:
            self.client = None
            logger.warning("Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ì„¤ì •
        self.last_call_time = 0
        self.min_interval = 2  # ìµœì†Œ 2ì´ˆ ê°„ê²©
        self.request_count = 0
        self.max_requests_per_hour = 50  # ì‹œê°„ë‹¹ ìµœëŒ€ ìš”ì²­ ìˆ˜
    
    async def generate_response(self, prompt: str, max_tokens: int = 4000) -> str:
        """Claude APIë¥¼ í†µí•´ ì‘ë‹µ ìƒì„± (í–¥ìƒëœ ì†ë„ ì œí•œ ë° ì˜¤ë¥˜ ì²˜ë¦¬)"""
        # API í˜¸ì¶œ ê°„ê²© ì œí•œ
        current_time = time.time()
        time_since_last_call = current_time - self.last_call_time
        if time_since_last_call < self.min_interval:
            sleep_time = self.min_interval - time_since_last_call
            logger.info(f"API í˜¸ì¶œ ì œí•œì„ ìœ„í•´ {sleep_time:.1f}ì´ˆ ëŒ€ê¸°ì¤‘...")
            await asyncio.sleep(sleep_time)
        
        self.last_call_time = time.time()
        self.request_count += 1
        
        if not self.client:
            # API í‚¤ê°€ ì—†ì„ ë•Œ í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì‘ë‹µ
            return self._get_enhanced_dummy_response(prompt)
        
        try:
            # ì¬ì‹œë„ ë¡œì§ ê°•í™”
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    response = self.client.messages.create(
                        model="claude-3-5-sonnet-20241022",
                        max_tokens=max_tokens,
                        temperature=0.1,
                        messages=[
                            {"role": "user", "content": prompt}
                        ]
                    )
                    
                    logger.info(f"Claude API í˜¸ì¶œ ì„±ê³µ (ìš”ì²­ #{self.request_count})")
                    return response.content[0].text
                    
                except Exception as e:
                    error_msg = str(e).lower()
                    if "529" in error_msg or "overloaded" in error_msg or "rate limit" in error_msg:
                        if attempt < max_retries - 1:
                            wait_time = (attempt + 1) * 10  # 10, 20, 30ì´ˆ ëŒ€ê¸°
                            logger.warning(f"Claude API ê³¼ë¶€í•˜/ì œí•œ (ì‹œë„ {attempt + 1}/{max_retries}). {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                            await asyncio.sleep(wait_time)
                            continue
                    raise e
                    
        except Exception as e:
            logger.error(f"Claude API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return self._get_enhanced_dummy_response(prompt)
    
    def _get_enhanced_dummy_response(self, prompt: str) -> str:
        """í–¥ìƒëœ í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì‘ë‹µ ìƒì„±"""
        if "ë‰´ìŠ¤ ë¶„ì„" in prompt or "ë¶„ì„í•  ë‰´ìŠ¤" in prompt:
            return """{
  "relevance_score": 8,
  "topics": ["ê¸°ì—…ê¸°ìˆ ", "ì œí’ˆì¶œì‹œ", "ì‹œì¥ë™í–¥"],
  "keywords": ["ì•Œí‹°ë² ì´ìŠ¤", "HyperDB", "ì¸ë©”ëª¨ë¦¬DB", "ì„±ëŠ¥í–¥ìƒ", "ì‹¤ì‹œê°„ë¶„ì„", "ë°ì´í„°ë² ì´ìŠ¤", "ê¸°ìˆ í˜ì‹ "],
  "summary": "ì•Œí‹°ë² ì´ìŠ¤ê°€ ìƒˆë¡œìš´ ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ HyperDB 3.0ì„ ì¶œì‹œí•˜ì—¬ 30% ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ ë¶„ì„ ê¸°ëŠ¥ì´ ê°•í™”ë˜ì–´ ê¸ˆìœµê¶Œì˜ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤.",
  "sentiment": "ê¸ì •",
  "importance": 8,
  "company_mentions": ["ì•Œí‹°ë² ì´ìŠ¤"],
  "date": "2025-07-07",
  "source": "í…ŒìŠ¤íŠ¸ë‰´ìŠ¤"
}"""
        elif "ì²­í¬ë¡œ ë¶„í• " in prompt:
            return """{
  "chunks": [
    {
      "chunk_id": 1,
      "content": "ì•Œí‹°ë² ì´ìŠ¤ê°€ ìƒˆë¡œìš´ ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ 'HyperDB 3.0'ì„ ê³µì‹ ì¶œì‹œí–ˆë‹¤ê³  7ì¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.",
      "topics": ["ì œí’ˆì¶œì‹œ", "ê¸°ì—…ë°œí‘œ"],
      "keywords": ["ì•Œí‹°ë² ì´ìŠ¤", "HyperDB", "ì¶œì‹œ", "ë°œí‘œ"],
      "chunk_type": "ì œëª©"
    },
    {
      "chunk_id": 2,
      "content": "ì´ë²ˆ ë²„ì „ì€ ê¸°ì¡´ ëŒ€ë¹„ 30% í–¥ìƒëœ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ì‹¤ì‹œê°„ ë¶„ì„ ê¸°ëŠ¥ì´ ëŒ€í­ ê°•í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ê³¼ ë³‘ë ¬ ì²˜ë¦¬ ê¸°ìˆ ì´ ì ìš©ë˜ì–´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
      "topics": ["ì„±ëŠ¥ê°œì„ ", "ê¸°ìˆ í˜ì‹ "],
      "keywords": ["ì„±ëŠ¥í–¥ìƒ", "ì‹¤ì‹œê°„ë¶„ì„", "ë©”ëª¨ë¦¬ìµœì í™”", "ë³‘ë ¬ì²˜ë¦¬"],
      "chunk_type": "ë³¸ë¬¸"
    },
    {
      "chunk_id": 3,
      "content": "ì•Œí‹°ë² ì´ìŠ¤ ê´€ê³„ìëŠ” 'HyperDB 3.0ì€ ë””ì§€í„¸ ì „í™˜ ì‹œëŒ€ì˜ ìš”êµ¬ì— ë¶€í•©í•˜ëŠ” í˜ì‹ ì ì¸ ì œí’ˆ'ì´ë¼ë©° 'ê¸ˆìœµ, í†µì‹ , ì œì¡°ì—… ë“±ì—ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ì œê³µí•  ê²ƒ'ì´ë¼ê³  ë°í˜”ìŠµë‹ˆë‹¤.",
      "topics": ["ê´€ê³„ìë°œì–¸", "ì‚°ì—…ì ìš©"],
      "keywords": ["ë””ì§€í„¸ì „í™˜", "ê¸ˆìœµ", "í†µì‹ ", "ì œì¡°ì—…", "ì•ˆì •ì„±"],
      "chunk_type": "ì¸ìš©"
    }
  ]
}"""
        elif "í’ˆì§ˆì„ í‰ê°€" in prompt:
            return """{
  "overall_score": 8,
  "detailed_scores": {
    "facts": 8,
    "completeness": 9,
    "objectivity": 8,
    "readability": 9,
    "credibility": 7
  },
  "improvements": ["ì¶œì²˜ ì •ë³´ ë³´ê°•", "ì „ë¬¸ê°€ ì˜ê²¬ ì¶”ê°€"],
  "strengths": ["ëª…í™•í•œ êµ¬ì¡°", "ê· í˜•ì¡íŒ ë‚´ìš©", "ì „ë¬¸ì  ë¬¸ì²´"],
  "approval": true
}"""
        else:
            # ë‰´ìŠ¤ ìƒì„±ìš© ë”ë¯¸ ì‘ë‹µ (í–¥ìƒë¨)
            return """ì œëª©: ì•Œí‹°ë² ì´ìŠ¤, ì°¨ì„¸ëŒ€ ì¸ë©”ëª¨ë¦¬ DB ì—”ì§„ 'HyperDB 3.0' ì¶œì‹œë¡œ ì‹œì¥ ì„ ë„

ë¦¬ë“œ: êµ­ë‚´ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê¸°ì—… ì•Œí‹°ë² ì´ìŠ¤ê°€ ê¸°ì¡´ ëŒ€ë¹„ 30% í–¥ìƒëœ ì„±ëŠ¥ì˜ ì°¨ì„¸ëŒ€ ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ì—”ì§„ 'HyperDB 3.0'ì„ ê³µì‹ ì¶œì‹œí–ˆë‹¤ê³  7ì¼ ë°œí‘œí–ˆë‹¤. ì‹¤ì‹œê°„ ë¶„ì„ ê¸°ëŠ¥ ê°•í™”ì™€ ë©”ëª¨ë¦¬ ìµœì í™”ë¡œ ëŒ€ê¸°ì—…ê³¼ ê¸ˆìœµê¶Œì˜ ì£¼ëª©ì„ ë°›ê³  ìˆë‹¤.

ë³¸ë¬¸:
ì•Œí‹°ë² ì´ìŠ¤(ëŒ€í‘œ ì´í™ì„±)ëŠ” ì´ë‚  ì„œìš¸ ê°•ë‚¨êµ¬ ë³¸ì‚¬ì—ì„œ ê¸°ìê°„ë‹´íšŒë¥¼ ì—´ê³  "HyperDB 3.0ì´ ê¸°ì¡´ ë²„ì „ ëŒ€ë¹„ ì²˜ë¦¬ ì†ë„ 30% í–¥ìƒ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 20% ì ˆê°ì„ ë‹¬ì„±í–ˆë‹¤"ê³  ë°í˜”ë‹¤.

ìƒˆë¡œìš´ ì—”ì§„ì€ ë…ì ê°œë°œí•œ ë©”ëª¨ë¦¬ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ê³¼ ë©€í‹°ì½”ì–´ ë³‘ë ¬ ì²˜ë¦¬ ê¸°ìˆ ì„ ì ìš©í•´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ì„ í¬ê²Œ ê°œì„ í–ˆë‹¤. íŠ¹íˆ ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œê°„ì„ ê¸°ì¡´ ëŒ€ë¹„ ì ˆë°˜ìœ¼ë¡œ ë‹¨ì¶•ì‹œì¼°ë‹¤ëŠ” ê²ƒì´ íšŒì‚¬ ì¸¡ ì„¤ëª…ì´ë‹¤.

ì´í™ì„± ëŒ€í‘œëŠ” "ë””ì§€í„¸ ì „í™˜ ê°€ì†í™”ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ìˆ˜ìš”ê°€ ê¸‰ì¦í•˜ëŠ” ìƒí™©ì—ì„œ HyperDB 3.0ì€ ì°¨ë³„í™”ëœ ê¸°ìˆ ë ¥ìœ¼ë¡œ ì‹œì¥ì„ ì„ ë„í•  ê²ƒ"ì´ë¼ë©° "íŠ¹íˆ ê¸ˆìœµ, í†µì‹ , ì œì¡°ì—… ë“± ë¯¸ì…˜ í¬ë¦¬í‹°ì»¬í•œ í™˜ê²½ì—ì„œ ì•ˆì •ì ì´ê³  ë¹ ë¥¸ ì„±ëŠ¥ì„ ë³´ì¥í•œë‹¤"ê³  ê°•ì¡°í–ˆë‹¤.

ì—…ê³„ì—ì„œëŠ” ì´ë²ˆ ì œí’ˆ ì¶œì‹œê°€ êµ­ë‚´ ë°ì´í„°ë² ì´ìŠ¤ ì‹œì¥ì˜ ê²½ìŸ êµ¬ë„ë¥¼ ë°”ê¿€ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê´€ì¸¡í•˜ê³  ìˆë‹¤. í•œ DB ì „ë¬¸ê°€ëŠ” "ì•Œí‹°ë² ì´ìŠ¤ì˜ ê¸°ìˆ ë ¥ì´ ê¸€ë¡œë²Œ ìˆ˜ì¤€ì— ê·¼ì ‘í–ˆë‹¤"ë©° "ì˜¤ë¼í´, IBM ë“± í•´ì™¸ ì—…ì²´ë“¤ê³¼ì˜ ê²½ìŸì—ì„œë„ ì¶©ë¶„íˆ ìŠ¹ë¶€í•  ìˆ˜ ìˆì„ ê²ƒ"ì´ë¼ê³  í‰ê°€í–ˆë‹¤.

ê²°ë¡ : ì•Œí‹°ë² ì´ìŠ¤ëŠ” HyperDB 3.0ì„ í†µí•´ êµ­ë‚´ì™¸ ì‹œì¥ í™•ëŒ€ë¥¼ ë³¸ê²©í™”í•  ê³„íšì´ë©°, ì˜¬ í•˜ë°˜ê¸° ê¸€ë¡œë²Œ ì§„ì¶œì„ ìœ„í•œ ë§ˆì¼€íŒ…ì„ ê°•í™”í•  ì˜ˆì •ì´ë‹¤. íšŒì‚¬ëŠ” 2025ë…„ ë§¤ì¶œ 300ì–µì› ë‹¬ì„±ì„ ëª©í‘œë¡œ í•˜ê³  ìˆë‹¤.

í‚¤ì›Œë“œ: ì•Œí‹°ë² ì´ìŠ¤, HyperDB, ì¸ë©”ëª¨ë¦¬ë°ì´í„°ë² ì´ìŠ¤, ì„±ëŠ¥í–¥ìƒ, ì‹¤ì‹œê°„ë¶„ì„, ë””ì§€í„¸ì „í™˜, ë°ì´í„°ë² ì´ìŠ¤ì‹œì¥"""

class EnhancedNewsCollector:
    """í–¥ìƒëœ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, claude_client: EnhancedClaudeClient, db_manager: EnhancedChromaDBManager, 
                 naver_api: EnhancedNaverNewsAPI):
        self.claude_client = claude_client
        self.db_manager = db_manager
        self.naver_api = naver_api
        
    async def collect_company_news_enhanced(self, company_name: str, additional_keywords: List[str] = None,
                                          days_back: int = 365, max_articles: int = 50) -> int:
        """í–¥ìƒëœ íšŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘"""
        logger.info(f"{company_name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (í‚¤ì›Œë“œ: {additional_keywords}, ìµœê·¼ {days_back}ì¼, ìµœëŒ€ {max_articles}ê°œ)")
        
        collected_count = 0
        
        try:
            # í–¥ìƒëœ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‚¬ìš©
            articles = self.naver_api.search_news_with_keywords(
                company_name, 
                additional_keywords, 
                display=max_articles
            )
            
            for article in articles[:max_articles]:
                # ë‚ ì§œ í•„í„°ë§
                if self._is_recent_article(article.pub_date, days_back):
                    success = await self.collect_and_store_news(company_name, article)
                    if success:
                        collected_count += 1
                
                # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ë”œë ˆì´
                await asyncio.sleep(2)
                
                # ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜ ë„ë‹¬ì‹œ ì¤‘ë‹¨
                if collected_count >= max_articles:
                    break
                
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                
        logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {collected_count}ê°œ ê¸°ì‚¬ ì €ì¥")
        return collected_count
    
    async def collect_and_store_news(self, company_name: str, article: NewsArticle) -> bool:
        """ê°œë³„ ë‰´ìŠ¤ ë¶„ì„ ë° ì €ì¥ (ê°œì„ ë¨)"""
        try:
            # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì œëª©+ì„¤ëª… ì‚¬ìš©
            news_content = article.content if article.content else f"{article.title}\n{article.description}"
            
            if len(news_content.strip()) < 50:  # ë„ˆë¬´ ì§§ì€ ë‚´ìš© ì œì™¸
                logger.warning(f"ë‰´ìŠ¤ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ: {article.title}")
                return False
            
            # 1. ë‰´ìŠ¤ ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            analysis_prompt = EnhancedPromptManager.get_news_analysis_prompt(news_content, company_name)
            analysis_result = await self.claude_client.generate_response(analysis_prompt)
            
            # JSON íŒŒì‹± ê°œì„ 
            try:
                metadata_dict = self._extract_json_from_response(analysis_result)
                metadata = NewsMetadata(**metadata_dict)
            except Exception as e:
                logger.error(f"ë©”íƒ€ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
                return False
            
            # ê´€ë ¨ë„ ê²€ì‚¬
            if metadata.relevance_score < 5:
                logger.info(f"ê´€ë ¨ë„ ë¶€ì¡± ({metadata.relevance_score}): {article.title}")
                return False
            
            # 2. ë‰´ìŠ¤ ì²­í‚¹
            chunking_prompt = EnhancedPromptManager.get_news_chunking_prompt(news_content)
            chunking_result = await self.claude_client.generate_response(chunking_prompt)
            
            try:
                chunks_dict = self._extract_json_from_response(chunking_result)
                chunks = [NewsChunk(**chunk) for chunk in chunks_dict['chunks']]
            except Exception as e:
                logger.error(f"ì²­í‚¹ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
                return False
            
            # 3. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata.source = article.link
            metadata.date = self._convert_pub_date(article.pub_date)
            
            # 4. ë²¡í„° DBì— ì €ì¥
            for chunk in chunks:
                # 768ì°¨ì› ë”ë¯¸ ì„ë² ë”© ìƒì„± (ì‹¤ì œ êµ¬í˜„ì‹œ Sentence Transformers ì‚¬ìš© ê¶Œì¥)
                embedding = [0.1] * 768
                self.db_manager.store_news_chunk(chunk, metadata, embedding)
            
            logger.info(f"ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ: {article.title[:50]}... ({len(chunks)}ê°œ ì²­í¬, ê´€ë ¨ë„: {metadata.relevance_score})")
            return True
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return False
    
    def _extract_json_from_response(self, response: str) -> dict:
        """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ (ê°œì„ ë¨)"""
        # JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
            else:
                raise ValueError("JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # JSON ì •ë¦¬
        json_str = json_str.strip()
        
        # ì²« ë²ˆì§¸ ì™„ì „í•œ JSON ê°ì²´ë§Œ ì¶”ì¶œ
        if json_str.count('}') > 1:
            brace_count = 0
            json_end_pos = 0
            for i, char in enumerate(json_str):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_end_pos = i + 1
                        break
            json_str = json_str[:json_end_pos]
        
        return json.loads(json_str)
    
    def _is_recent_article(self, pub_date: str, days_back: int) -> bool:
        """ìµœê·¼ ê¸°ì‚¬ì¸ì§€ í™•ì¸"""
        try:
            from datetime import datetime
            article_date = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %z")
            cutoff_date = datetime.now().astimezone() - timedelta(days=days_back)
            return article_date >= cutoff_date
        except:
            return True  # íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨
    
    def _convert_pub_date(self, pub_date: str) -> str:
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜"""
        try:
            from datetime import datetime
            dt = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d")
        except:
            return datetime.now().strftime("%Y-%m-%d")

class EnhancedNewsWriter:
    """í–¥ìƒëœ ë‰´ìŠ¤ ì‘ì„±ê¸°"""
    
    def __init__(self, claude_client: EnhancedClaudeClient, db_manager: EnhancedChromaDBManager):
        self.claude_client = claude_client
        self.db_manager = db_manager
    
    async def generate_enhanced_news(self, topic: str, keywords: List[str], 
                                   user_facts: str, style: str = "ê¸°ì—… ë³´ë„í˜•",
                                   length_specification: str = "",
                                   use_rag: bool = True, rag_count: int = 10) -> Optional[str]:
        """í–¥ìƒëœ ë‰´ìŠ¤ ìƒì„± (RAG ê°œì„ )"""
        try:
            reference_materials = ""
            
            if use_rag:
                # 1. ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ (ê°œì„ ëœ RAG)
                search_query = f"{topic} {' '.join(keywords)}"
                logger.info(f"RAG ê²€ìƒ‰: '{search_query}' (ìƒìœ„ {rag_count}ê°œ)")
                
                search_results = self.db_manager.search_relevant_news(
                    search_query, 
                    n_results=rag_count,
                    min_relevance=6  # ê´€ë ¨ë„ 6ì  ì´ìƒë§Œ
                )
                
                # 2. ì°¸ê³  ìë£Œ êµ¬ì„± (ì „ì²´ ë‚´ìš© í¬í•¨)
                reference_materials = self._build_comprehensive_reference_materials(search_results)
                logger.info(f"RAG ì°¸ê³  ìë£Œ êµ¬ì„± ì™„ë£Œ: {len(search_results.get('documents', [[]])[0])}ê°œ ë¬¸ì„œ")
            
            # 3. ë‰´ìŠ¤ ì‘ì„±
            generation_prompt = EnhancedPromptManager.get_enhanced_news_generation_prompt(
                topic, keywords, user_facts, reference_materials, length_specification
            )
            
            news_draft = await self.claude_client.generate_response(generation_prompt, max_tokens=6000)
            
            # 4. í’ˆì§ˆ ê²€ì¦
            quality_check_prompt = EnhancedPromptManager.get_quality_check_prompt(news_draft)
            quality_result = await self.claude_client.generate_response(quality_check_prompt)
            
            try:
                quality_dict = self._extract_json_from_response(quality_result)
                logger.info(f"ë‰´ìŠ¤ í’ˆì§ˆ í‰ê°€: {quality_dict.get('overall_score', 0)}ì ")
                
                if quality_dict.get('approval', False):
                    logger.info("ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ í†µê³¼")
                    return news_draft
                else:
                    logger.warning("ë‰´ìŠ¤ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨í•˜ì§€ë§Œ ê²°ê³¼ ë°˜í™˜")
                    return news_draft  # ì‹¤íŒ¨í•´ë„ ë‰´ìŠ¤ëŠ” ë°˜í™˜
            except:
                logger.warning("í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨")
                return news_draft  # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _build_comprehensive_reference_materials(self, search_results: Dict) -> str:
        """í¬ê´„ì ì¸ ì°¸ê³  ìë£Œ êµ¬ì„± (ì „ì²´ ë‚´ìš© í¬í•¨)"""
        if not search_results or not search_results.get('documents') or not search_results['documents'][0]:
            return "ê´€ë ¨ ì°¸ê³  ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        materials = []
        documents = search_results['documents'][0]
        metadatas = search_results.get('metadatas', [[]])[0] if search_results.get('metadatas') else []
        distances = search_results.get('distances', [[]])[0] if search_results.get('distances') else []
        
        for i, doc in enumerate(documents[:10]):  # ìµœëŒ€ 10ê°œ
            metadata = metadatas[i] if i < len(metadatas) else {}
            distance = distances[i] if i < len(distances) else 0
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
            try:
                topics = json.loads(metadata.get('topics', '[]'))
                keywords = json.loads(metadata.get('keywords', '[]'))
                company_mentions = json.loads(metadata.get('company_mentions', '[]'))
            except:
                topics = []
                keywords = []
                company_mentions = []
            
            source = metadata.get('source', f'ì°¸ê³ ìë£Œ {i+1}')
            date = metadata.get('date', 'N/A')
            importance = metadata.get('importance', 'N/A')
            relevance_score = metadata.get('relevance_score', 'N/A')
            sentiment = metadata.get('sentiment', 'N/A')
            summary = metadata.get('summary', '')
            
            # ì¢…í•©ì ì¸ ì°¸ê³  ìë£Œ ì •ë³´ êµ¬ì„±
            material = f"""=== ì°¸ê³ ìë£Œ {i+1} ===
ì¶œì²˜: {source}
ë‚ ì§œ: {date}
ê´€ë ¨ë„: {relevance_score}/10 (ìœ ì‚¬ë„: {1-distance:.3f})
ì¤‘ìš”ë„: {importance}/10
ê°ì •: {sentiment}
ì£¼ìš” í† í”½: {', '.join(topics[:3])}
í‚¤ì›Œë“œ: {', '.join(keywords[:5])}
ì–¸ê¸‰ ê¸°ì—…: {', '.join(company_mentions)}

ìš”ì•½: {summary}

ì „ì²´ ë‚´ìš©:
{doc}

----------------------------------------

"""
            materials.append(material)
        
        reference_text = "\n".join(materials) if materials else "ê´€ë ¨ ì°¸ê³  ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # í†µê³„ ì •ë³´ ì¶”ê°€
        stats_info = f"""
[ì°¸ê³  ìë£Œ í†µê³„]
- ì´ {len(materials)}ê°œ ë¬¸ì„œ ì°¸ì¡°
- í‰ê·  ê´€ë ¨ë„: {sum([float(m.get('relevance_score', 0)) for m in metadatas[:len(materials)]]) / len(materials) if materials else 0:.1f}/10
- ë‚ ì§œ ë²”ìœ„: {min([m.get('date', '') for m in metadatas[:len(materials)]])} ~ {max([m.get('date', '') for m in metadatas[:len(materials)]])}

"""
        
        return stats_info + reference_text
    
    def _extract_json_from_response(self, response: str) -> dict:
        """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
        json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
            else:
                raise ValueError("JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return json.loads(json_str.strip())

class EnhancedAINewsWriterSystem:
    """í–¥ìƒëœ AI News Writer ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, claude_api_key: str = None, naver_client_id: str = None, 
                 naver_client_secret: str = None, db_path: str = "./chroma_db"):
        self.claude_client = EnhancedClaudeClient(claude_api_key)
        self.db_manager = EnhancedChromaDBManager(db_path)
        self.naver_api = EnhancedNaverNewsAPI(naver_client_id, naver_client_secret)
        self.news_collector = EnhancedNewsCollector(self.claude_client, self.db_manager, self.naver_api)
        self.news_writer = EnhancedNewsWriter(self.claude_client, self.db_manager)
        
        logger.info("Enhanced AI News Writer ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def collect_news_background(self, company_name: str, additional_keywords: List[str] = None,
                                    days_back: int = 365, max_articles: int = 50) -> int:
        """í–¥ìƒëœ ë°±ê·¸ë¼ìš´ë“œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        return await self.news_collector.collect_company_news_enhanced(
            company_name, additional_keywords, days_back, max_articles
        )
    
    async def collect_manual_news(self, company_name: str, news_content: str) -> bool:
        """ìˆ˜ë™ ë‰´ìŠ¤ ì…ë ¥"""
        from datetime import datetime
        
        article = NewsArticle(
            title="ìˆ˜ë™ ì…ë ¥ ë‰´ìŠ¤",
            link="manual_input",
            description="ì‚¬ìš©ìê°€ ì§ì ‘ ì…ë ¥í•œ ë‰´ìŠ¤",
            pub_date=datetime.now().strftime("%a, %d %b %Y %H:%M:%S %z"),
            content=news_content
        )
        
        return await self.news_collector.collect_and_store_news(company_name, article)
    
    async def write_news(self, topic: str, keywords: List[str], user_facts: str, 
                        style: str = "ê¸°ì—… ë³´ë„í˜•", length_specification: str = "",
                        use_rag: bool = True, rag_count: int = 10) -> Optional[str]:
        """í–¥ìƒëœ ë‰´ìŠ¤ ì‘ì„±"""
        return await self.news_writer.generate_enhanced_news(
            topic, keywords, user_facts, style, length_specification, use_rag, rag_count
        )
    
    def get_system_stats(self) -> Dict:
        """ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ"""
        try:
            db_stats = self.db_manager.get_collection_stats()
            return {
                "database": db_stats,
                "api_requests": self.claude_client.request_count,
                "naver_test_mode": self.naver_api.test_mode
            }
        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def schedule_news_collection(self, company_name: str, additional_keywords: List[str] = None,
                                     interval_hours: int = 6, max_articles: int = 20):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬"""
        logger.info(f"{company_name} ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ê°„ê²©: {interval_hours}ì‹œê°„)")
        
        while True:
            try:
                collected = await self.collect_news_background(
                    company_name, additional_keywords, days_back=1, max_articles=max_articles
                )
                logger.info(f"ìŠ¤ì¼€ì¤„ ìˆ˜ì§‘ ì™„ë£Œ: {collected}ê°œ ê¸°ì‚¬")
                
                # ë‹¤ìŒ ìˆ˜ì§‘ê¹Œì§€ ëŒ€ê¸°
                await asyncio.sleep(interval_hours * 3600)
                
            except Exception as e:
                logger.error(f"ìŠ¤ì¼€ì¤„ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)  # 1ë¶„ í›„ ì¬ì‹œë„

# ì´ì „ ë²„ì „ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
AINewsWriterSystem = EnhancedAINewsWriterSystem

# ì‚¬ìš© ì˜ˆì‹œ
async def enhanced_main():
    """í–¥ìƒëœ ì‹œìŠ¤í…œ ë°ëª¨"""
    print("=== Enhanced AI News Writer ì‹œìŠ¤í…œ ===\n")
    
    # .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
    claude_api_key = os.getenv('CLAUDE_API_KEY')
    naver_client_id = os.getenv('NAVER_CLIENT_ID') 
    naver_client_secret = os.getenv('NAVER_CLIENT_SECRET')
    
    # API í‚¤ ìƒíƒœ í™•ì¸
    print("=== ğŸ”‘ API í‚¤ ìƒíƒœ í™•ì¸ ===")
    print(f"Claude API Key: {'âœ… ì„¤ì •ë¨' if claude_api_key else 'âŒ ì—†ìŒ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)'}")
    print(f"ë„¤ì´ë²„ Client ID: {'âœ… ì„¤ì •ë¨' if naver_client_id else 'âŒ ì—†ìŒ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)'}")
    print(f"ë„¤ì´ë²„ Client Secret: {'âœ… ì„¤ì •ë¨' if naver_client_secret else 'âŒ ì—†ìŒ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)'}")
    print()
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = EnhancedAINewsWriterSystem(
        claude_api_key=claude_api_key,
        naver_client_id=naver_client_id,
        naver_client_secret=naver_client_secret
    )
    
    company_name = "ì•Œí‹°ë² ì´ìŠ¤"
    additional_keywords = ["ë°ì´í„°ë² ì´ìŠ¤", "DBMS", "ì˜¤ë¼í´", "ì¸ë©”ëª¨ë¦¬"]
    
    # 1. í–¥ìƒëœ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
    print("1. í–¥ìƒëœ í‚¤ì›Œë“œ ì¡°í•© ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    collected_count = await system.collect_news_background(
        company_name, 
        additional_keywords, 
        days_back=365,  # 12ê°œì›”
        max_articles=20
    )
    print(f"ìˆ˜ì§‘ ê²°ê³¼: {collected_count}ê°œ ê¸°ì‚¬\n")
    
    # 2. ìˆ˜ë™ ë‰´ìŠ¤ ì¶”ê°€
    print("2. ìˆ˜ë™ ë‰´ìŠ¤ ì¶”ê°€ ì¤‘...")
    manual_news = """
    ì•Œí‹°ë² ì´ìŠ¤ê°€ ì°¨ì„¸ëŒ€ ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ 'HyperDB 3.0'ì„ ê³µì‹ ì¶œì‹œí–ˆë‹¤.
    ì´ë²ˆ ì‹ ì œí’ˆì€ ê¸°ì¡´ ë²„ì „ ëŒ€ë¹„ 30% í–¥ìƒëœ ì²˜ë¦¬ ì„±ëŠ¥ê³¼ 20% ì ˆì•½ëœ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìë‘í•œë‹¤.
    ì‹¤ì‹œê°„ ë¶„ì„ ê¸°ëŠ¥ì´ ëŒ€í­ ê°•í™”ë˜ì–´ ê¸ˆìœµê¶Œê³¼ ëŒ€ê¸°ì—…ì˜ ì£¼ëª©ì„ ë°›ê³  ìˆìœ¼ë©°,
    ë…ì ê°œë°œí•œ ë©”ëª¨ë¦¬ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ê³¼ ë©€í‹°ì½”ì–´ ë³‘ë ¬ ì²˜ë¦¬ ê¸°ìˆ ì´ ì ìš©ë˜ì—ˆë‹¤.
    íšŒì‚¬ ê´€ê³„ìëŠ” "ë””ì§€í„¸ ì „í™˜ ì‹œëŒ€ì˜ ìš”êµ¬ì— ë¶€í•©í•˜ëŠ” í˜ì‹ ì ì¸ ì œí’ˆ"ì´ë¼ê³  ë°í˜”ë‹¤.
    """
    
    manual_success = await system.collect_manual_news(company_name, manual_news)
    print(f"ìˆ˜ë™ ë‰´ìŠ¤ ì¶”ê°€: {'âœ… ì„±ê³µ' if manual_success else 'âŒ ì‹¤íŒ¨'}\n")
    
    # 3. í–¥ìƒëœ RAG ë‰´ìŠ¤ ì‘ì„±
    print("3. í–¥ìƒëœ RAG AI ë‰´ìŠ¤ ì‘ì„± ì¤‘...")
    topic = "ë°ì´í„°ë² ì´ìŠ¤ ì‹ ê¸°ìˆ  ì¶œì‹œ"
    keywords = ["ì•Œí‹°ë² ì´ìŠ¤", "HyperDB", "ì¸ë©”ëª¨ë¦¬", "ì„±ëŠ¥í–¥ìƒ", "ì‹¤ì‹œê°„ë¶„ì„"]
    user_facts = """ì•Œí‹°ë² ì´ìŠ¤ê°€ HyperDB 3.0ì´ë¼ëŠ” í˜ì‹ ì ì¸ ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¶œì‹œí–ˆë‹¤.
ì£¼ìš” íŠ¹ì§•:
- ê¸°ì¡´ ëŒ€ë¹„ 30% ì„±ëŠ¥ í–¥ìƒ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 20% ì ˆì•½
- ì‹¤ì‹œê°„ ë¶„ì„ ê¸°ëŠ¥ ê°•í™”
- ë©€í‹°ì½”ì–´ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
- ê¸ˆìœµê¶Œê³¼ ëŒ€ê¸°ì—… íƒ€ê²Ÿ"""
    
    generated_news = await system.write_news(
        topic=topic,
        keywords=keywords,
        user_facts=user_facts,
        style="ê¸°ì—… ë³´ë„í˜•",
        length_specification="100ì¤„ ë¶„ëŸ‰ì˜ ìƒì„¸í•œ ë‰´ìŠ¤",
        use_rag=True,
        rag_count=10
    )
    
    if generated_news:
        print("=== ğŸ—ï¸ ìƒì„±ëœ Enhanced AI ë‰´ìŠ¤ ===")
        print(generated_news)
        print("\n" + "="*70)
    else:
        print("âŒ ë‰´ìŠ¤ ìƒì„± ì‹¤íŒ¨")
    
    # 4. ì‹œìŠ¤í…œ í†µê³„
    print("\n=== ğŸ“Š ì‹œìŠ¤í…œ í†µê³„ ===")
    stats = system.get_system_stats()
    print(f"DB ì €ì¥ëœ ì²­í¬ ìˆ˜: {stats.get('database', {}).get('total_chunks', 0)}")
    print(f"API í˜¸ì¶œ íšŸìˆ˜: {stats.get('api_requests', 0)}")
    print(f"ë„¤ì´ë²„ API ëª¨ë“œ: {'í…ŒìŠ¤íŠ¸' if stats.get('naver_test_mode', True) else 'ì‹¤ì œ'}")
    
    print("\n=== âœ¨ í–¥ìƒëœ ê¸°ëŠ¥ ===")
    print("âœ… íšŒì‚¬ëª… + ì¶”ê°€ í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰")
    print("âœ… 12ê°œì›”(365ì¼) ê¸°ë³¸ ìˆ˜ì§‘ ê¸°ê°„")
    print("âœ… ë¡œì»¬ íŒŒì¼ ìë™ ì €ì¥")
    print("âœ… ê°œì„ ëœ RAG (10ê°œ ë‰´ìŠ¤ ì „ì²´ ë‚´ìš© ì°¸ì¡°)")
    print("âœ… í–¥ìƒëœ ë‰´ìŠ¤ í’ˆì§ˆ í‰ê°€")
    print("âœ… ë‰´ìŠ¤ ê¸¸ì´ ì¡°ì ˆ ì§€ì›")
    print("âœ… ì¢…í•©ì ì¸ ì°¸ê³  ìë£Œ ë©”íƒ€ë°ì´í„°")

# ë™ê¸° í•¨ìˆ˜ë¡œ ë˜í•‘
def run_enhanced_main():
    """í–¥ìƒëœ ë©”ì¸ í•¨ìˆ˜ë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰"""
    asyncio.run(enhanced_main())

if __name__ == "__main__":
    run_enhanced_main()